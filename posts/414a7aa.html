<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><script>(()=>{let r=navigator.serviceWorker,e=()=>console.error("Service Worker 注册失败！可能是由于您的浏览器不支持该功能！");r?.register("/sw.js")?.catch(e)||e()})()</script><div id="myscoll"></div><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Pytorch基础知识（二） | MuXiaoChen🍊</title><meta name="keywords" content="Pytorch,Deep Learning,Machine Learning"><meta name="author" content="MuXiaoChen🍊"><meta name="copyright" content="MuXiaoChen🍊"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文主要介绍有关Pytorch的基础知识（二）"><meta property="og:type" content="article"><meta property="og:title" content="Pytorch基础知识（二）"><meta property="og:url" content="https://www.aimiliy.top/posts/414a7aa.html"><meta property="og:site_name" content="MuXiaoChen🍊"><meta property="og:description" content="本文主要介绍有关Pytorch的基础知识（二）"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.aimiliy.top/pic/pytorch2.webp"><meta property="article:published_time" content="2024-07-04T09:29:30.000Z"><meta property="article:modified_time" content="2024-09-11T10:46:55.197Z"><meta property="article:author" content="MuXiaoChen🍊"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Machine Learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.aimiliy.top/pic/pytorch2.webp"><link rel="shortcut icon" href="/"><link rel="canonical" href="https://www.aimiliy.top/posts/414a7aa"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//sdk.51.la"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="baidu-site-verification" content="codeva-JLcaXpJvSI"><meta name="360-site-verification" content="b30a5ef244c11182af61a7ad77a2b235"><meta name="sogou_site_verification" content="qR8JiGGdBn"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.aimiliy.top/npm/css/fancybox.min.css" media="print" onload='this.media="all"'><script>!function(t){"use strict";!function(){var s=window,e=document,c=t,i="".concat("https:"===e.location.protocol?"https://":"http://","sdk.51.la/js-sdk-pro.min.js"),o=e.createElement("script"),n=e.getElementsByTagName("script")[0];o.type="text/javascript",o.setAttribute("charset","UTF-8"),o.async=!0,o.src=i,o.id="LA_COLLECT",c.d=o;var r=function(){s.LA.ids.push(c)};s.LA?s.LA.ids&&r():(s.LA=t,s.LA.ids=[],r()),n.parentNode.insertBefore(o,n)}()}({id:"3IlXoM0PL8mq0gTh",ck:"3IlXoM0PL8mq0gTh",hashMode:!0})</script><script>!function(e,t){var n=t.createElement("script"),i=t.getElementsByTagName("script")[0];n.type="text/javascript",n.crossorigin=!0,n.onload=function(){(new e.LingQue.Monitor).init({id:"3JaTbTP1VNsqZWGP"})},i.parentNode.insertBefore(n,i),n.src="https://sdk.51.la/perf/js-sdk-perf.min.js"}(window,document)</script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"TMAQULEA61",apiKey:"cbdc736a26353ade14da768f54cde1fe",indexName:"MuXiaoChen",hits:{per_page:5},languages:{input_placeholder:"搜索文章",hits_empty:"找不到您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，用时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,noticeOutdate:{limitDay:365,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:230},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{justifiedGallery:{js:"https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.js",css:"https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.css"}},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Pytorch基础知识（二）",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-09-11 18:46:55"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const a=864e5*o,n={value:t,expiry:(new Date).getTime()+a};localStorage.setItem(e,JSON.stringify(n))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise(((t,o)=>{const a=document.createElement("script");a.src=e,a.async=!0,a.onerror=o,a.onload=a.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)})),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=18?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="https://cdn.aimiliy.top/npm/css/index.css"><style id="themeColor"></style><style id="rightSide"></style><style id="transPercent"></style><style id="bgFilterParam"></style><style id="settingStyle"></style><span id="fps"></span><style id="defineBg"></style><style id="menu_shadow"></style><style id="nav-display"></style><style id="aside-pos"></style><style id="aside-show"></style><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><svg aria-hidden="true" style="position:absolute;overflow:hidden;width:0;height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248 626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><script defer src="/sw-dom.js"></script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload='this.media="screen"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload='this.media="all"'><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.aimiliy.top/npm/css/swiperstyle.css" media="print" onload='this.media="all"'><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="MuXiaoChen🍊" type="application/atom+xml"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><div id="web_bg"></div><div id="an_music_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://qiniu.aimiliy.top/webAvatar/Linghua11711165511555501.webp" onerror='onerror=null,src="/assets/r1.jpg"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-home"></use></svg> <span class="menu_word" style="font-size:17px">首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon--article"></use></svg> <span class="menu_word" style="font-size:17px">文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-guidang1"></use></svg> <span class="menu_word" style="font-size:17px">归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-sekuaibiaoqian"></use></svg> <span class="menu_word" style="font-size:17px">标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-fenlei"></use></svg> <span class="menu_word" style="font-size:17px">分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-pinweishenghuo"></use></svg> <span class="menu_word" style="font-size:17px">休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-yinle"></use></svg> <span class="menu_word" style="font-size:17px">音乐盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-dianying1"></use></svg> <span class="menu_word" style="font-size:17px">影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-youxishoubing"></use></svg> <span class="menu_word" style="font-size:17px">游戏</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/bangumi/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-wodezhuifan"></use></svg> <span class="menu_word" style="font-size:17px">追番</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-xiangzi"></use></svg> <span class="menu_word" style="font-size:17px">百宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-tubiaozhizuomoban"></use></svg> <span class="menu_word" style="font-size:17px">画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-nvwumao"></use></svg> <span class="menu_word" style="font-size:17px">动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-zhifengche"></use></svg> <span class="menu_word" style="font-size:17px">收藏夹</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shejiaoxinxi"></use></svg> <span class="menu_word" style="font-size:17px">社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-pengyouquan"></use></svg> <span class="menu_word" style="font-size:17px">朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-liuyan"></use></svg> <span class="menu_word" style="font-size:17px">留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-lianjie"></use></svg> <span class="menu_word" style="font-size:17px">友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-wangye"></use></svg> <span class="menu_word" style="font-size:17px">网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon--tongjibiao"></use></svg> <span class="menu_word" style="font-size:17px">网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shujutongji1"></use></svg> <span class="menu_word" style="font-size:17px">文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-xianxingshalou"></use></svg> <span class="menu_word" style="font-size:17px">时光册</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-maoliang"></use></svg> <span class="menu_word" style="font-size:17px">个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-qunliaotian"></use></svg> <span class="menu_word" style="font-size:17px">日常分享</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-love-sign"></use></svg> <span class="menu_word" style="font-size:17px">恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-paperplane"></use></svg> <span class="menu_word" style="font-size:17px">关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">MuXiaoChen🍊</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-home"></use></svg> <span class="menu_word" style="font-size:17px">首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon--article"></use></svg> <span class="menu_word" style="font-size:17px">文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-guidang1"></use></svg> <span class="menu_word" style="font-size:17px">归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-sekuaibiaoqian"></use></svg> <span class="menu_word" style="font-size:17px">标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-fenlei"></use></svg> <span class="menu_word" style="font-size:17px">分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-pinweishenghuo"></use></svg> <span class="menu_word" style="font-size:17px">休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-yinle"></use></svg> <span class="menu_word" style="font-size:17px">音乐盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-dianying1"></use></svg> <span class="menu_word" style="font-size:17px">影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-youxishoubing"></use></svg> <span class="menu_word" style="font-size:17px">游戏</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/bangumi/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-wodezhuifan"></use></svg> <span class="menu_word" style="font-size:17px">追番</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-xiangzi"></use></svg> <span class="menu_word" style="font-size:17px">百宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-tubiaozhizuomoban"></use></svg> <span class="menu_word" style="font-size:17px">画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-nvwumao"></use></svg> <span class="menu_word" style="font-size:17px">动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-zhifengche"></use></svg> <span class="menu_word" style="font-size:17px">收藏夹</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shejiaoxinxi"></use></svg> <span class="menu_word" style="font-size:17px">社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-pengyouquan"></use></svg> <span class="menu_word" style="font-size:17px">朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-liuyan"></use></svg> <span class="menu_word" style="font-size:17px">留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-lianjie"></use></svg> <span class="menu_word" style="font-size:17px">友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-wangye"></use></svg> <span class="menu_word" style="font-size:17px">网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon--tongjibiao"></use></svg> <span class="menu_word" style="font-size:17px">网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shujutongji1"></use></svg> <span class="menu_word" style="font-size:17px">文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-xianxingshalou"></use></svg> <span class="menu_word" style="font-size:17px">时光册</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-maoliang"></use></svg> <span class="menu_word" style="font-size:17px">个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-qunliaotian"></use></svg> <span class="menu_word" style="font-size:17px">日常分享</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-love-sign"></use></svg> <span class="menu_word" style="font-size:17px">恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-paperplane"></use></svg> <span class="menu_word" style="font-size:17px">关于</span></a></li></ul></div></div><center id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></center><div id="nav-right"><div id="search-button"><a class="search faa-parent animated-hover" title="检索站内任何你想要的信息"><svg class="faa-tada icon" style="height:24px;width:24px;fill:currentColor;position:relative;top:6px" aria-hidden="true"><use xlink:href="#icon-valentine_-search-love-find-heart"></use></svg> <span>搜索</span></a></div><a class="meihua faa-parent animated-hover" onclick="toggleWinbox()" title="美化设置-自定义你的风格" id="meihua-button"><svg class="faa-tada icon" style="height:26px;width:26px;fill:currentColor;position:relative;top:8px" aria-hidden="true"><use xlink:href="#icon-picture"></use></svg></a><a class="sun_moon faa-parent animated-hover" onclick="switchNightMode()" title="浅色和深色模式转换" id="nightmode-button"><svg class="faa-tada" style="height:25px;width:25px;fill:currentColor;position:relative;top:7px" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><div id="toggle-menu"><a><i class="fas fa-bars fa-fw"></i></a></div></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch基础知识（二）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><svg class="meta_icon post-meta-icon" style="width:30px;height:30px;position:relative;top:10px"><use xlink:href="#icon-rili"></use></svg><span class="post-meta-label">发表于</span> <time class="post-meta-date-created" datetime="2024-07-04T09:29:30.000Z" title="发表于 2024-07-04 17:29:30">2024-07-04</time><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-gengxin1"></use></svg><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-11T10:46:55.197Z" title="更新于 2024-09-11 18:46:55">2024-09-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-biaoqian"></use></svg><a class="post-meta-categories" href="/categories/%E6%95%99%E7%A8%8B/">教程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:8px"><use xlink:href="#icon-charuword"></use></svg><span class="post-meta-label">字数总计:</span><span class="word-count">5377</span><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:20px;height:20px;position:relative;top:5px"><use xlink:href="#icon-shizhong"></use></svg><span class="post-meta-label">阅读时长:</span><span>26分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Pytorch基础知识（二）"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:5px"><use xlink:href="#icon-eye"></use></svg><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s58 18 88 18 58-18 88-18 58 18 88 18v44h-352Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>1.Tensor</h1><p><strong>torch.Tensor</strong>是一种包含单一数据类型元素的多维矩阵<br>Torch定义了10种CPU tensor类型和GPU tensor类型：</p><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td>torch.float32 or torch.float</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64-bit floating point</td><td>torch.float64 or torch.double</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr><tr><td>16-bit floating point [1]</td><td>torch.float16 or torch.half</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>16-bit floating point [2]</td><td>torch.bfloat16</td><td>torch.BFloat16Tensor</td><td>torch.cuda.BFloat16Tensor</td></tr><tr><td>32-bit complex</td><td>torch.complex32 or torch.chalf</td><td></td><td></td></tr><tr><td>64-bit complex</td><td>torch.complex64 or torch.cfloat</td><td></td><td></td></tr><tr><td>128-bit complex</td><td>torch.complex128 or torch.cdouble</td><td></td><td></td></tr><tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr><tr><td>8-bit integer (signed)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer (signed)</td><td>torch.int16 or torch.short</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32-bit integer (signed)</td><td>torch.int32 or torch.int</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64-bit integer (signed)</td><td>torch.int64 or torch.long</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr><tr><td>Boolean</td><td>torch.bool</td><td>torch.BoolTensor</td><td>torch.cuda.BoolTensor</td></tr><tr><td>quantized 8-bit integer (unsigned)</td><td>torch.quint8</td><td>torch.ByteTensor</td><td>/</td></tr><tr><td>quantized 8-bit integer (signed)</td><td>torch.qint8</td><td>torch.CharTensor</td><td>/</td></tr><tr><td>quantized 32-bit integer (signed)</td><td>torch.qint32</td><td>torch.IntTensor</td><td>/</td></tr><tr><td>quantized 4-bit integer (unsigned) [3]</td><td>torch.quint4x2</td><td>torch.ByteTensor</td><td>/</td></tr></tbody></table><div class="note info flat"><p>创建</p></div><p>一个张量tensor可以从Python的<strong>list</strong>或序列构建：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.FloatTensor([[1, 2, 3], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line">Out[0]: </span><br><span class="line">tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br></pre></td></tr></table></figure><p>根据可选择的大小和数据新建一个tensor。 如果没有提供参数，将会返回一个空的零维张量。如果提供了<strong>numpy.ndarray</strong>,<strong>torch.Tensor</strong>或<strong>torch.Storage</strong>，将会返回一个有同样参数的tensor.如果提供了python序列，将会从序列的副本创建一个tensor</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">接口 一个空张量tensor可以通过规定其大小来构建</span></span><br><span class="line">class torch.Tensor</span><br><span class="line">class torch.Tensor(*sizes)</span><br><span class="line">class torch.Tensor(size)</span><br><span class="line">class torch.Tensor(sequence)</span><br><span class="line">class torch.Tensor(ndarray)</span><br><span class="line">class torch.Tensor(tensor)</span><br><span class="line">class torch.Tensor(storage)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">实例化</span></span><br><span class="line">torch.IntTensor(2, 4).zero_()</span><br></pre></td></tr></table></figure><p>可以用python的索引和切片来获取和修改一个张量tensor中的内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">x[1][2]</span><br><span class="line">Out[0]: tensor(6.)</span><br><span class="line"></span><br><span class="line">x[0][1] = 8</span><br><span class="line">x</span><br><span class="line">Out[1]: </span><br><span class="line">tensor([[1., 8., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br></pre></td></tr></table></figure><p>每一个张量tensor都有一个相应的<strong>torch.Storage</strong>用来保存其数据。类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算<strong>会改变tensor的函数操作会用一个下划线后缀来标示</strong>。比如，**torch.FloatTensor.abs_()<strong>会在原地计算绝对值，并返回改变后的tensor，而</strong>tensor.FloatTensor.abs()**将会在一个新的tensor中计算结果</p><h1>2.storage</h1><div class="note info flat"><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/ebd7f6395bf4">tensor的数据结构、storage()、stride()、storage_offset()</a></p></div><p>pytorch中一个tensor对象分为头信息区（Tensor）和存储区（Storage）两部分<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/articlePic/img_2.png" alt="img.png"><br>头信息区主要保存tensor的形状（size）、步长（stride）、数据类型（type）等信息；而真正的data（数据)则以连续一维数组的形式放在存储区，由torch.Storage实例管理着<br><strong>注意：storage永远是一维数组，任何维度的tensor的实际数据都存储在一维的storage中</strong></p><div class="note info flat"><p>获取tensor的storage</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[1.0, 4.0],[2.0, 1.0],[3.0, 5.0]])</span><br><span class="line">a.storage()</span><br><span class="line">Out[0]: </span><br><span class="line"> 1.0</span><br><span class="line"> 4.0</span><br><span class="line"> 2.0</span><br><span class="line"> 1.0</span><br><span class="line"> 3.0</span><br><span class="line"> 5.0</span><br><span class="line">[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]</span><br><span class="line"></span><br><span class="line">a.storage()[2] = 9</span><br><span class="line"></span><br><span class="line">id(a.storage())</span><br><span class="line">Out[1]: 1343354913168</span><br></pre></td></tr></table></figure><h1>3.Pytorch加载数据</h1><p>Pytorch中加载数据需要Dataset、Dataloader。</p><ul><li>Dataset提供一种方式去获取每个数据及其对应的label，告诉我们总共有多少个数据。</li><li>Dataloader为后面的网络提供不同的数据形式，它将一批一批数据进行一个打包。</li></ul><h1>4.Tensorboard</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor())               </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回</span>      </span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=64,shuffle=True,num_workers=0,drop_last=False)      </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">用<span class="keyword">for</span>循环取出DataLoader打包好的四个数据</span></span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line">step = 0</span><br><span class="line">for data in test_loader:</span><br><span class="line">    imgs, targets = data # 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成             </span><br><span class="line">    writer.add_images(&quot;test_data&quot;,imgs,step)</span><br><span class="line">    step = step + 1</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1>4.transforms</h1><p>① Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。<br>② Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from torchvision import transforms</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">img_path = &quot;Data/FirstTypeData/val/bees/10870992_eebeeb3a12.jpg&quot;</span><br><span class="line">img = Image.open(img_path)  </span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()  # 创建 transforms.ToTensor类 的实例化对象</span><br><span class="line">tensor_img = tensor_trans(img)  # 调用 transforms.ToTensor类 的__call__的魔术方法   </span><br><span class="line">print(tensor_img)</span><br></pre></td></tr></table></figure><h1>5.torchvision数据集</h1><p>① torchvision中有很多数据集，当我们写代码时指定相应的数据集指定一些参数，它就可以自行下载。<br>② CIFAR-10数据集包含60000张32×32的彩色图片，一共10个类别，其中50000张训练图片，10000张测试图片。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=True,download=True) # root为存放数据集的相对路线</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=False,download=True) # train=True是训练集，train=False是测试集  </span><br><span class="line"></span><br><span class="line">print(test_set[0])       # 输出的3是target </span><br><span class="line">print(test_set.classes)  # 测试数据集中有多少种</span><br><span class="line"></span><br><span class="line">img, target = test_set[0] # 分别获得图片、target</span><br><span class="line">print(img)</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line">print(test_set.classes[target]) # 3号target对应的种类</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure><h1>6.损失函数</h1><p>① Loss损失函数一方面计算实际输出和目标之间的差距。<br>② Loss损失函数另一方面为我们更新输出提供一定的依据</p><div class="note info flat"><p>L1loss损失函数</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">inputs = torch.tensor([1,2,3],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([1,2,5],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(1,1,1,3))</span><br><span class="line">targets = torch.reshape(targets,(1,1,1,3))</span><br><span class="line">loss = L1Loss()  # 默认为 maen</span><br><span class="line">result = loss(inputs,targets)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>MSE损失函数</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">from torch import nn</span><br><span class="line">inputs = torch.tensor([1,2,3],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([1,2,5],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(1,1,1,3))</span><br><span class="line">targets = torch.reshape(targets,(1,1,1,3))</span><br><span class="line">loss_mse = nn.MSELoss()</span><br><span class="line">result_mse = loss_mse(inputs,targets)</span><br><span class="line">print(result_mse)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>交叉熵损失函数</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">x = torch.tensor([0.1,0.2,0.3])</span><br><span class="line">y = torch.tensor([1])</span><br><span class="line">x = torch.reshape(x,(1,3)) # 1的 batch_size，有三类</span><br><span class="line">loss_cross = nn.CrossEntropyLoss()</span><br><span class="line">result_cross = loss_cross(x,y)</span><br><span class="line">print(result_cross)</span><br></pre></td></tr></table></figure><h1>7.优化器</h1><p>① 损失函数调用backward方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。<br>② 梯度要清零，如果梯度不清零会导致梯度累加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss() # 交叉熵    </span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器</span><br><span class="line">for data in dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = tudui(imgs)</span><br><span class="line">    result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距</span><br><span class="line">    optim.zero_grad()  # 梯度清零</span><br><span class="line">    result_loss.backward() # 反向传播，计算损失函数的梯度</span><br><span class="line">    optim.step()   # 根据梯度，对网络的参数进行调优</span><br><span class="line">    print(result_loss) # 对数据只看了一遍，只看了一轮，所以loss下降不大</span><br></pre></td></tr></table></figure><div class="note info flat"><p>神经网络学习率优化</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn </span><br><span class="line">from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=64,drop_last=True)</span><br><span class="line"></span><br><span class="line">class Tudui(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(3,32,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32,32,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32,64,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(1024,64),</span><br><span class="line">            Linear(64,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss() # 交叉熵    </span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1) # 每过 step_size 更新一次优化器，更新是学习率为原来的学习率的 0.1 倍    </span><br><span class="line">for epoch in range(20):</span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for data in dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距</span><br><span class="line">        optim.zero_grad()  # 梯度清零</span><br><span class="line">        result_loss.backward() # 反向传播，计算损失函数的梯度</span><br><span class="line">        optim.step()   # 根据梯度，对网络的参数进行调优</span><br><span class="line">        scheduler.step() # 学习率太小了，所以20个轮次后，相当于没走多少</span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line">    print(running_loss) # 对这一轮所有误差的总和</span><br></pre></td></tr></table></figure><h1>8.网络模型使用及修改</h1><div class="note info flat"><p>网络模型添加</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=True) # 下载卷积层对应的参数是多少、池化层对应的参数时多少，这些参数时ImageNet训练好了的</span><br><span class="line">vgg16_true.add_module(&#x27;add_linear&#x27;,nn.Linear(1000,10)) # 在VGG16后面添加一个线性层，使得输出为适应CIFAR10的输出，CIFAR10需要输出10个种类</span><br><span class="line"></span><br><span class="line">print(vgg16_true)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>网络模型修改</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=False) # 没有预训练的参数     </span><br><span class="line">print(vgg16_false)</span><br><span class="line">vgg16_false.classifier[6] = nn.Linear(4096,10)</span><br><span class="line">print(vgg16_false)</span><br></pre></td></tr></table></figure><h1>9.网络模型保存与读取</h1><div class="note info flat"><p>模型结构 + 模型参数</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=False)</span><br><span class="line">torch.save(vgg16,&quot;./model/vgg16_method1.pth&quot;) # 保存方式一：模型结构 + 模型参数      </span><br><span class="line">print(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(&quot;./model/vgg16_method1.pth&quot;) # 保存方式一对应的加载模型    </span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>模型参数（官方推荐），不保存网络模型结构</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=False)</span><br><span class="line">torch.save(vgg16.state_dict(),&quot;./model/vgg16_method2.pth&quot;) # 保存方式二：模型参数（官方推荐）,不再保存网络模型结构  </span><br><span class="line">print(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(&quot;./model/vgg16_method2.pth&quot;) # 导入模型参数   </span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><h1>10.固定模型参数</h1><p>在训练过程中可能需要固定一部分模型的参数，只更新另一部分参数，有两种思路实现这个目标</p><ul><li>1.一个是设置不要更新参数的<strong>网络层</strong>为false</li><li>2.另一个就是在定义优化器时只传入要更新的参数<br>当然最优的做法是，优化器中只传入requires_grad=True的参数，这样占用的内存会更小一点，效率也会更高</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义一个简单的网络</span></span><br><span class="line">class net(nn.Module):</span><br><span class="line">    def __init__(self, num_class=3):</span><br><span class="line">        super(net, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(8, 4)</span><br><span class="line">        self.fc2 = nn.Linear(4, num_class)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.fc2(self.fc1(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = net()</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">冻结fc1层的参数</span></span><br><span class="line">for name, param in model.named_parameters():</span><br><span class="line">    if &quot;fc1&quot; in name:</span><br><span class="line">        param.requires_grad = False</span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">只传入requires_grad = True的参数</span></span><br><span class="line">optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters(), lr=1e-2)</span><br><span class="line">print(&quot;model.fc1.weight&quot;, model.fc1.weight)</span><br><span class="line">print(&quot;model.fc2.weight&quot;, model.fc2.weight)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line">for epoch in range(10):</span><br><span class="line">    x = torch.randn((3, 8))</span><br><span class="line">    label = torch.randint(0, 3, [3]).long()</span><br><span class="line">    output = model(x)</span><br><span class="line"></span><br><span class="line">    loss = loss_fn(output, label)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">print(&quot;model.fc1.weight&quot;, model.fc1.weight)</span><br><span class="line">print(&quot;model.fc2.weight&quot;, model.fc2.weight)</span><br></pre></td></tr></table></figure><h1>11.训练流程</h1><div class="note info flat"><p>DataLoader加载数据集</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">length 长度</span></span><br><span class="line">train_data_size = len(train_data)</span><br><span class="line">test_data_size = len(test_data)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line">print(&quot;训练数据集的长度：&#123;&#125;&quot;.format(train_data_size))</span><br><span class="line">print(&quot;测试数据集的长度：&#123;&#125;&quot;.format(test_data_size))</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data_size, batch_size=64)        </span><br><span class="line">test_dataloader = DataLoader(test_data_size, batch_size=64)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>测试网络正确</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">搭建神经网络</span></span><br><span class="line">class Tudui(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,32,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,64,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Flatten(),  # 展平后变成 64*4*4 了</span><br><span class="line">            nn.Linear(64*4*4,64),</span><br><span class="line">            nn.Linear(64,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    tudui = Tudui()</span><br><span class="line">    input = torch.ones((64,3,32,32))</span><br><span class="line">    output = tudui(input)</span><br><span class="line">    print(output.shape)  # 测试输出的尺寸是不是我们想要的</span><br></pre></td></tr></table></figure><div class="note info flat"><p>网络训练数据</p></div><p>① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。<br>② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。<br>③ 不启用 Batch Normalization 和 Dropout。 如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。<br>④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的性质。<br>⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里</span></span><br><span class="line">class Tudui(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,32,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,64,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Flatten(),  # 展平后变成 64*4*4 了</span><br><span class="line">            nn.Linear(64*4*4,64),</span><br><span class="line">            nn.Linear(64,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        return x</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">length 长度</span></span><br><span class="line">train_data_size = len(train_data)</span><br><span class="line">test_data_size = len(test_data)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line">print(&quot;训练数据集的长度：&#123;&#125;&quot;.format(train_data_size))</span><br><span class="line">print(&quot;测试数据集的长度：&#123;&#125;&quot;.format(test_data_size))</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=64)        </span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建网络模型</span></span><br><span class="line">tudui = Tudui() </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">优化器</span></span><br><span class="line">learning = 0.01  # 1e-2 就是 0.01 的意思</span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置网络的一些参数</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记录训练的次数</span></span><br><span class="line">total_train_step = 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记录测试的次数</span></span><br><span class="line">total_test_step = 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">训练的轮次</span></span><br><span class="line">epoch = 10</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加 tensorboard</span></span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line"></span><br><span class="line">for i in range(epoch):</span><br><span class="line">    print(&quot;-----第 &#123;&#125; 轮训练开始-----&quot;.format(i+1))</span><br><span class="line"></span><br><span class="line">    # 训练步骤开始</span><br><span class="line">    tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用</span><br><span class="line">    for data in train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距</span><br><span class="line"></span><br><span class="line">        # 优化器对模型调优</span><br><span class="line">        optimizer.zero_grad()  # 梯度清零</span><br><span class="line">        loss.backward() # 反向传播，计算损失函数的梯度</span><br><span class="line">        optimizer.step()   # 根据梯度，对网络的参数进行调优</span><br><span class="line"></span><br><span class="line">        total_train_step = total_train_step + 1</span><br><span class="line">        if total_train_step % 100 == 0:</span><br><span class="line">            print(&quot;训练次数：&#123;&#125;，Loss：&#123;&#125;&quot;.format(total_train_step,loss.item()))  # 方式二：获得loss值</span><br><span class="line">            writer.add_scalar(&quot;train_loss&quot;,loss.item(),total_train_step)</span><br><span class="line"></span><br><span class="line">    # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）</span><br><span class="line">    tudui.eval()  # 当网络中有dropout层、batchnorm层时，这些层不能起作用</span><br><span class="line">    total_test_loss = 0</span><br><span class="line">    total_accuracy = 0</span><br><span class="line">    with torch.no_grad():  # 没有梯度了</span><br><span class="line">        for data in test_dataloader: # 测试数据集提取数据</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            outputs = tudui(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失</span><br><span class="line">            total_test_loss = total_test_loss + loss.item() # 所有loss</span><br><span class="line">            accuracy = (outputs.argmax(1) == targets).sum()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line"></span><br><span class="line">    print(&quot;整体测试集上的Loss：&#123;&#125;&quot;.format(total_test_loss))</span><br><span class="line">    print(&quot;整体测试集上的正确率：&#123;&#125;&quot;.format(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(&quot;test_loss&quot;,total_test_loss,total_test_step)</span><br><span class="line">    writer.add_scalar(&quot;test_accuracy&quot;,total_accuracy/test_data_size,total_test_step)  </span><br><span class="line">    total_test_step = total_test_step + 1</span><br><span class="line"></span><br><span class="line">    torch.save(tudui, &quot;./model/tudui_&#123;&#125;.pth&quot;.format(i)) # 保存每一轮训练后的结果</span><br><span class="line">    #torch.save(tudui.state_dict(),&quot;tudui_&#123;&#125;.path&quot;.format(i)) # 保存方式二         </span><br><span class="line">    print(&quot;模型已保存&quot;)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1>12.模型|参数查看</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyModel(torch.nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.layer1 = torch.nn.Sequential(</span><br><span class="line">            torch.nn.Linear(3, 4),</span><br><span class="line">            torch.nn.Linear(4, 3),</span><br><span class="line">        )</span><br><span class="line">        self.layer2 = torch.nn.Linear(3, 6)</span><br><span class="line">        self.layer3 = torch.nn.Sequential(</span><br><span class="line">            torch.nn.Linear(6, 7),</span><br><span class="line">            torch.nn.Linear(7, 5),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = MyModel()</span><br><span class="line">print(net)</span><br><span class="line">MyModel(</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): Linear(in_features=3, out_features=4, bias=True)</span><br><span class="line">    (1): Linear(in_features=4, out_features=3, bias=True)</span><br><span class="line">  )</span><br><span class="line">  (layer2): Linear(in_features=3, out_features=6, bias=True)</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (0): Linear(in_features=6, out_features=7, bias=True)</span><br><span class="line">    (1): Linear(in_features=7, out_features=5, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>查看参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">for layer in net.modules():</span><br><span class="line">    print(type(layer))  # 查看每一层的类型</span><br><span class="line">    # print(layer)</span><br><span class="line">&lt;class &#x27;__main__.MyModel&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.container.Sequential&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.container.Sequential&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for param in net.parameters():</span><br><span class="line">    print(param.shape)  # 打印每一层的参数</span><br><span class="line">torch.Size([4, 3])</span><br><span class="line">torch.Size([4])</span><br><span class="line">torch.Size([3, 4])</span><br><span class="line">torch.Size([3])</span><br><span class="line">torch.Size([6, 3])</span><br><span class="line">torch.Size([6])</span><br><span class="line">torch.Size([7, 6])</span><br><span class="line">torch.Size([7])</span><br><span class="line">torch.Size([5, 7])</span><br><span class="line">torch.Size([5])</span><br><span class="line"></span><br><span class="line">for name, param in net.named_parameters():</span><br><span class="line">    print(name, param.shape)  # 看的更细</span><br><span class="line">layer1.0.weight torch.Size([4, 3])</span><br><span class="line">layer1.0.bias torch.Size([4])</span><br><span class="line">layer1.1.weight torch.Size([3, 4])</span><br><span class="line">layer1.1.bias torch.Size([3])</span><br><span class="line">layer2.weight torch.Size([6, 3])</span><br><span class="line">layer2.bias torch.Size([6])</span><br><span class="line">layer3.0.weight torch.Size([7, 6])</span><br><span class="line">layer3.0.bias torch.Size([7])</span><br><span class="line">layer3.1.weight torch.Size([5, 7])</span><br><span class="line">layer3.1.bias torch.Size([5])</span><br><span class="line"></span><br><span class="line">for key, value in net.state_dict().items():  # 参数名以及参数</span><br><span class="line">    print(key, value.shape)</span><br><span class="line">layer1.0.weight torch.Size([4, 3])</span><br><span class="line">layer1.0.bias torch.Size([4])</span><br><span class="line">layer1.1.weight torch.Size([3, 4])</span><br><span class="line">layer1.1.bias torch.Size([3])</span><br><span class="line">layer2.weight torch.Size([6, 3])</span><br><span class="line">layer2.bias torch.Size([6])</span><br><span class="line">layer3.0.weight torch.Size([7, 6])</span><br><span class="line">layer3.0.bias torch.Size([7])</span><br><span class="line">layer3.1.weight torch.Size([5, 7])</span><br><span class="line">layer3.1.bias torch.Size([5])</span><br></pre></td></tr></table></figure><h1>13.模型保存|加载</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1、加载模型+参数</span></span><br><span class="line">net = torch.load(&quot;resnet50.pth&quot;)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2、已有模型,加载预训练参数</span></span><br><span class="line">resnet50 = models.resnet50(weights=None)  </span><br><span class="line">resnet50.load_state_dict(torch.load(&quot;resnet58_weight.pth&quot;))</span><br></pre></td></tr></table></figure><h1>14.网络的修改</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">from torchvision import models</span><br><span class="line"></span><br><span class="line">alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (4): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">    (5): ReLU(inplace=True)</span><br><span class="line">    (6): Linear(in_features=4096, out_features=1000, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>修改网络结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1、-----删除网络的最后一层-----</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">del alexnet.classifier</span></span><br><span class="line">del alexnet.classifier[6]</span><br><span class="line">print(alexnet)</span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (4): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">    (5): ReLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2、-----删除网络的最后多层-----</span></span><br><span class="line">alexnet.classifier = alexnet.classifier[:-2]</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3、-----修改网络的某一层-----</span></span><br><span class="line">alexnet.classifier[6] = nn.Linear(in_features=4096, out_features=1024)</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Linear(in_features=4096, out_features=1024, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">4、-----网络添加层，每次添加一层-----</span></span><br><span class="line">alexnet.classifier.add_module(&#x27;7&#x27;, nn.ReLU(inplace=True))</span><br><span class="line">alexnet.classifier.add_module(&#x27;8&#x27;, nn.Linear(in_features=1024, out_features=20))</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Linear(in_features=4096, out_features=1024, bias=True)</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): Linear(in_features=1024, out_features=20, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1>15.参数冻结</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">任务一∶</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1、将模型A作为backbone，修改为模型B</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2、模型A的预训练参数加载到模型B上</span></span><br><span class="line"></span><br><span class="line">resnet_modified = resnet50()</span><br><span class="line">new_weights_dict = resnet_modified.state_dict()</span><br><span class="line"></span><br><span class="line">resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)</span><br><span class="line">weights_dict = resnet.state_dict()</span><br><span class="line"></span><br><span class="line">for k in weights_dict.keys():</span><br><span class="line">    if k in new_weights_dict.keys() and not k.startswith(&#x27;fc&#x27;):</span><br><span class="line">        new_weights_dict[k] = weights_dict[k]</span><br><span class="line">resnet_modified.load_state_dict(new_weights_dict)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">resnet_modified.load_state_dict(new_weights_dict,strict=False)</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">任务二:</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">冻结与训练好的参数</span></span><br><span class="line">params = []</span><br><span class="line">train_layer = [&#x27;layer5&#x27;, &#x27;conv_end&#x27;, &#x27;bn_end&#x27;]</span><br><span class="line">for name, param in resnet_modified.named_parameters():</span><br><span class="line">    if any(name.startswith(prefix) for prefix in train_layer):</span><br><span class="line">        print(name)</span><br><span class="line">        params.append(param)</span><br><span class="line">    else:</span><br><span class="line">        param.requires_grad = False</span><br><span class="line">optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=5e-4)</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>Pytorch基础知识（二）</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://www.aimiliy.top/posts/414a7aa.html">https://www.aimiliy.top/posts/414a7aa.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>MuXiaoChen🍊</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2024-07-04</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2024-09-11</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/"><div class="tags-punctuation"><svg class="faa-tada icon" style="height:1.1em;width:1.1em;fill:currentColor;position:relative;top:2px;margin-right:3px" aria-hidden="true"><use xlink:href="#icon-sekuaibiaoqian"></use></svg></div>Pytorch</a><a class="post-meta__tags" href="/tags/Deep-Learning/"><div class="tags-punctuation"><svg class="faa-tada icon" style="height:1.1em;width:1.1em;fill:currentColor;position:relative;top:2px;margin-right:3px" aria-hidden="true"><use xlink:href="#icon-sekuaibiaoqian"></use></svg></div>Deep Learning</a><a class="post-meta__tags" href="/tags/Machine-Learning/"><div class="tags-punctuation"><svg class="faa-tada icon" style="height:1.1em;width:1.1em;fill:currentColor;position:relative;top:2px;margin-right:3px" aria-hidden="true"><use xlink:href="#icon-sekuaibiaoqian"></use></svg></div>Machine Learning</a></div></div><link rel="stylesheet" href="/css/coin.css" media="defer" onload='this.media="all"'><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">投喂作者</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://tuchuang.voooe.cn/images/2024/07/03/wechat.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2024/07/03/wechat.png" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://tuchuang.voooe.cn/images/2024/07/03/alipay.png" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2024/07/03/alipay.png" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></button></div><audio id="coinAudio" src="https://npm.elemecdn.com/akilar-candyassets@1.0.36/audio/aowu.m4a"></audio><script defer src="/js/coin.js"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/f6122879.html"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/pic/XMP.webp" onerror='onerror=null,src="/assets/r2.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">内存开启XMP</div></div></a></div><div class="next-post pull-right"><a href="/posts/5a986f0.html"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/articlePic/pytorch.jpg" onerror='onerror=null,src="/assets/r2.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Pytorch基础知识（一）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/5a986f0.html" title="Pytorch基础知识（一）"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/articlePic/pytorch.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2024-09-11</div><div class="title">Pytorch基础知识（一）</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><svg class="meta_icon" style="width:22px;height:22px;position:relative;top:5px"><use xlink:href="#icon-mulu1"></use></svg><span style="font-weight:700">目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">1.Tensor</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">2.storage</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">3.Pytorch加载数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">4.Tensorboard</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">4.transforms</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">5.torchvision数据集</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">6.损失函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">7.优化器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">8.网络模型使用及修改</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">9.网络模型保存与读取</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">10.固定模型参数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">11.训练流程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">12.模型|参数查看</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">13.模型保存|加载</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">14.网络的修改</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">15.参数冻结</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-color:transparent"><div id="footer-wrap"><div id="ft"><div class="ft-item-1"><div class="t-top"><div class="t-t-l"><p class="ft-t t-l-t">格言🌼</p><div class="bg-ad"><div>人生就是这样，得失无常，常存安静之心，常存宽容之心，心里放不下，自然就成了负担，负担越多，人生就越不快乐。优雅的人生，就是用平静的心，平和的心态，别再为错过了什么而懊悔。💫</div><div class="btn-xz-box"><a class="btn-xz" target="_blank" rel="noopener" href="https://stellarium.org/">点击开启星辰之旅</a></div></div></div><div class="t-t-r"><p class="ft-t t-l-t">猜你想看💡</p><ul class="ft-links"><li><a href="/">返回主页</a><a href="/box/nav/">网址导航</a></li><li><a href="/social/link/">我的朋友</a><a href="/comments/">留点什么</a></li><li><a href="/personal/about/">关于作者</a><a href="/archives/">文章归档</a></li><li><a href="/categories/">文章分类</a><a href="/tags/">文章标签</a></li><li><a href="/box/Gallery/">我的画廊</a><a href="/personal/bb/">我的唠叨</a></li><li><a href="/site/time/">建设进程</a><a href="/site/census/">网站统计</a></li></ul></div></div></div><div class="ft-item-2"><p class="ft-t">推荐友链⌛</p><div class="ft-img-group"><div class="img-group-item"><a href="https://www.aimiliy.top/" title="MuXiaoChen🍊"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://qiniu.aimiliy.top/webAvatar/Linghua11711165511555501.webp" alt=""></a></div><div class="img-group-item"><a target="_blank" rel="noopener" href="https://starsei.com/" title="猫不吃鱼"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://starsei.com/favicon.ico" alt=""></a></div><div class="img-group-item"><a target="_blank" rel="noopener" href="https://stvue.com/" title="芸熙の小屋"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://store-tu.sdn.stvue.com/2022/04/02/6247410a43f6a.jpg" alt=""></a></div><div class="img-group-item"><a target="_blank" rel="noopener" href="https://www.hoshiroko.com" title="薄荷の小屋"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://api.hoshiroko.com/img/avatar.jpg" alt=""></a></div><div class="img-group-item"><a target="_blank" rel="noopener" href="https://poetize.cn/" title="POETIZE"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://qiniu.aimiliy.top/globalResource/z9E7X4.webp" alt=""></a></div><div class="img-group-item"><a target="_blank" rel="noopener" href="https://blog.iamsjy.com/" title="Tony’s Blog"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://qiniu.aimiliy.top/globalResource/favicon.webp" alt=""></a></div><div class="img-group-item"><a target="_blank" rel="noopener" href="https://blog.ltya.top" title="岚天小窝"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/pic/5af06b461740fb2bd7467b8561399703.webp" alt=""></a></div><div class="img-group-item"><a target="_blank" rel="noopener" href="https://jipa.moe" title="JIPA233の小窝"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/articlePic/avatar.webp" alt=""></a></div></div></div></div><div class="copyright"><span><b>&copy;2023-2024</b></span><span><b>&nbsp;&nbsp;By MuXiaoChen🍊</b></span></div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" title="博客框架为Hexo_v6.3.0"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/aurora/shields/Frame-Hexo-blue.svg" alt=""></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" title="主题版本Butterfly_v4.3.1"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/aurora/shields/Theme-Butterfly-6513df.svg" alt=""></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" title="本网站托管与阿里云服务器"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/aurora/shields/Hosted-Aliyun-brightgreen.svg" alt=""></a><a class="github-badge" target="_blank" href="https://oss.console.aliyun.com/bucket/" style="margin-inline:5px" title="本网站使用阿里云提供对象存储服务"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/aurora/shields/Bucket-aliyun-9c62da.svg" alt=""></a><a class="github-badge" target="_blank" href="https://cdn.console.aliyun.com/" style="margin-inline:5px" title="本网站使用阿里云提供CDN加速与防护"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/aurora/shields/CDN-aliyun-yellow.svg" alt=""></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" title="本网站源码由Github提供存储仓库"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/aurora/shields/Source-Github-d021d6.svg" alt=""></a></p></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="share" type="button" title="右键模式" onclick="changeMouseMode()"><i class="fas fa-mouse"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog right_side"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button class="share" type="button" title="分享链接" onclick="share()"><i class="fas fa-share-nodes"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i><span id="percent">0<span>%</span></span></button><button id="go-down" type="button" title="直达底部" onclick="btf.scrollToDest(document.body.scrollHeight,500)"><i class="fas fa-arrow-down"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fa fa-refresh"></i></a><a class="rightMenu-item" href="javascript:rmf.scrollToTop();"><i class="fa fa-arrow-up"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();"><i class="fa fa-search"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-too"><a class="rightMenu-item" href="javascript:window.open(window.getSelection().toString());window.location.reload();"><i class="fa fa-link"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-post"><a class="rightMenu-item" href="#post-comment"><i class="fas fa-comment"></i><span>空降评论</span></a><a class="rightMenu-item" href="javascript:rmf.copyWordsLink()"><i class="fa fa-link"></i><span>复制本文地址</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-to"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>新窗口打开</span></a><a class="rightMenu-item" id="menu-too" href="javascript:rmf.open()"><i class="fa fa-link"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:randomPost()"><i class="fa fa-paper-plane"></i><span>随便逛逛</span></a><a class="rightMenu-item" href="javascript:switchNightMode();"><i class="fa fa-moon"></i><span>昼夜切换</span></a><a class="rightMenu-item" href="/personal/about/"><i class="fa fa-info-circle"></i><span>关于博客</span></a><a class="rightMenu-item" href="javascript:toggleWinbox();"><i class="fas fa-cog"></i><span>美化设置</span></a><a class="rightMenu-item" href="javascript:rmf.fullScreen();"><i class="fas fa-expand"></i><span>切换全屏</span></a><a class="rightMenu-item" href="javascript:window.print();"><i class="fa-solid fa-print"></i><span>打印页面</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script defer src="https://cdn.aimiliy.top/npm/js/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module" defer></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="https://cdn.aimiliy.top/npm/js/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.aimiliy.top/npm/js/instantsearch.production.min.js"></script><script defer src="/js/search/algolia.js"></script><script async>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading()),setTimeout((function(){preloader.endLoading()}),5e3),document.getElementById("loading-box").addEventListener("click",(()=>{preloader.endLoading()}))</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.2},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""],insertScript:[200,()=>{document.querySelectorAll("mjx-container:not([display])").forEach((t=>{const e=t.parentNode;"li"===e.nodeName.toLowerCase()?e.parentNode.classList.add("has-jax"):e.classList.add("has-jax")}))},"",!1]}}};const t=document.createElement("script");t.src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script><script>(()=>{const t=()=>{twikoo.init(Object.assign({el:"#twikoo-wrap",envId:"https://twikoo.aimiliy.top",region:"",onCommentLoaded:function(){btf.loadLightbox(document.querySelectorAll("#twikoo .tk-content img:not(.tk-owo-emotion)"))}},null))},o=()=>{"object"!=typeof twikoo?getScript("https://cdn.aimiliy.top/npm/js/twikoo.all.min.js").then(t):setTimeout(t,0)};btf.loadComment(document.getElementById("twikoo-wrap"),o)})()</script></div><script src="https://cdn.aimiliy.top/npm/js/jquery.min.js"></script><script async src="https://cdn.aimiliy.top/npm/js/vue.min.js"></script><script async src="https://cdn.aimiliy.top/npm/js/index.js"></script><script async src="https://cdn.aimiliy.top/npm/js/clipboard.min.js"></script><script defer src="https://cdn.aimiliy.top/npm/js/sweetalert2.all.js"></script><script async src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><script defer src="https://cdn.aimiliy.top/npm/js/winbox.bundle.min.js"></script><script async src="https://cdn.aimiliy.top/npm/js/font_3586335_hsivh70x0fm.js"></script><script async src="https://cdn.aimiliy.top/npm/js/font_3636804_glcijzcpx7f.js"></script><script async src="https://cdn.aimiliy.top/npm/js/font_3612150_gkavascgf3n.js"></script><script async src="https://cdn.aimiliy.top/npm/js/font_4662577_i568c9tmtea.js"></script><script defer src="https://cdn.aimiliy.top/npm/js/countup.js"></script><script defer src="/js/bibi.js"></script><script defer src="https://cdn.aimiliy.top/npm/js/fclite.min.js"></script><canvas id="universe"></canvas><canvas id="snow"></canvas><script defer src="/js/fomal.js"></script><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.css" media="print" onload='this.media="all"'><script src="https://cdn.aimiliy.top/npm/js/APlayer.min.js"></script><script src="https://cdn.aimiliy.top/npm/js/Meting.min.js"></script><script src="https://lib.baomitu.com/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors=["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax","#bibi","body > title","#app","#tag-echarts","#posts-echart","#categories-echarts"];var pjax=new Pjax({elements:'a:not([target="_blank"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!1,scrollRestoration:!1});document.addEventListener("pjax:send",(function(){if(window.tocScrollFn&&window.removeEventListener("scroll",window.tocScrollFn),window.scrollCollect&&window.removeEventListener("scroll",scrollCollect),"object"==typeof preloader&&preloader.initLoading(),document.getElementById("rightside").style.cssText="opacity: ''; transform: ''",window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();const e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode"),"object"==typeof disqusjs&&disqusjs.destroy()})),document.addEventListener("pjax:complete",(function(){window.refreshFn(),document.querySelectorAll("script[data-pjax]").forEach((e=>{const t=document.createElement("script"),o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach((e=>t.setAttribute(e.name,e.value))),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)})),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof chatBtnFn&&chatBtnFn(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","",{page_path:window.location.pathname}),"object"==typeof _hmt&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll(),"object"==typeof preloader&&preloader.endLoading()})),document.addEventListener("pjax:error",(e=>{404===e.request.status&&pjax.loadUrl("/404.html")}))</script><script async data-pjax src="//cdn.aimiliy.top/npm/js/busuanzi.pure.mini.js"></script></div><script data-pjax>if(document.getElementById("recent-posts")&&"/"===location.pathname){var parent=document.getElementById("recent-posts"),child='<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://www.aimiliy.top/categories/技术/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🍋 MXCの技术教程 (12)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.aimiliy.top/categories/演示/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🍎 MXCの案例演示 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.aimiliy.top/categories/教程/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🍉 MXCの学习教程 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.aimiliy.top/categories/游戏/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🥝 MXCの游戏生涯 (6)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.aimiliy.top/categories/生活/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🥑 MXCの生活日常 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.aimiliy.top/categories/资源/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🍍 MXCの资源分享 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://www.aimiliy.top/categories/" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';console.log("已挂载magnet"),parent.insertAdjacentHTML("afterbegin",child)}</script><style>#catalog_magnet{flex-wrap:wrap;display:flex;width:100%;justify-content:space-between;padding:10px 10px 0 10px;align-content:flex-start}.magnet_item{flex-basis:calc(33.333333333333336% - 5px);background:#e9e9e9;margin-bottom:10px;border-radius:8px;transition:all .2s ease-in-out}.magnet_item:hover{background:var(--text-bg-hover)}.magnet_link_more{color:#555}.magnet_link{color:#000}.magnet_link:hover{color:#fff}@media screen and (max-width:600px){.magnet_item{flex-basis:100%}}.magnet_link_context{display:flex;padding:10px;font-size:16px;transition:all .2s ease-in-out}.magnet_link_context:hover{padding:10px 20px}</style><style></style><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","2s"),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset","30"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","2s"),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset","30"),arr[i].setAttribute("data-wow-iteration","1")</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax>function butterfly_swiper_injector_config(){var a=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),a.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/3275e913.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/backImg/5fefcc8126a14661bb6a016978c94aee.webp" alt="" onerror="this.src=https://cdn.aimiliy.top/backImg/loading2.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-10-30</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/3275e913.html&quot;);" href="javascript:void(0);" alt="">【日常分享】页面美化</a><div class="blog-slider__text">本文主要记录【日常分享】页面的美化过程</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/3275e913.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/5c19c6c5.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/backImg/725dadd4883111ebb6edd017c2d2eca2.webp" alt="" onerror="this.src=https://cdn.aimiliy.top/backImg/loading2.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-10-21</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/5c19c6c5.html&quot;);" href="javascript:void(0);" alt="">【关于】、【音乐盒】页面美化记录</a><div class="blog-slider__text">主要介绍关于页、音乐盒页面的美化过程记录</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/5c19c6c5.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/ecd77df4.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/backImg/4f6bb6dd573d422ea9e798ecf62d4e1a.webp" alt="" onerror="this.src=https://cdn.aimiliy.top/backImg/loading2.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-10-08</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/ecd77df4.html&quot;);" href="javascript:void(0);" alt="">自建MetingJs后端API</a><div class="blog-slider__text">本文主要介绍MetingJs如何自建后端API</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/ecd77df4.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/2cba4350.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/backImg/1c76fa64e7fe431e83fe16205f4c1529.webp" alt="" onerror="this.src=https://cdn.aimiliy.top/backImg/loading2.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-09-24</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/2cba4350.html&quot;);" href="javascript:void(0);" alt="">Alibaba Cloud Linux中CronTab的用法详解</a><div class="blog-slider__text">本文主要介绍在Alibaba Cloud Linux上CronTab的详细用法</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/2cba4350.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/f3355759.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/backImg/5e8adf6ae8674d51bb71611072e84298.webp" alt="" onerror="this.src=https://cdn.aimiliy.top/backImg/loading2.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-09-19</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/f3355759.html&quot;);" href="javascript:void(0);" alt="">前端性能优化：从加载速度到用户体验</a><div class="blog-slider__text">本文主要介绍有关前端性能优化的方法和技巧</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/f3355759.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/8f8dd2c9.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/backImg/874ab01eb7f5465689389a8efa03306b.webp" alt="" onerror="this.src=https://cdn.aimiliy.top/backImg/loading2.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-09-11</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/8f8dd2c9.html&quot;);" href="javascript:void(0);" alt="">中秋月圆，情怀渐远</a><div class="blog-slider__text">中秋佳节，昔日的团圆情怀渐行渐远。</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/8f8dd2c9.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/f8b6e52d.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/backImg/cb752d0ac6d646c483d4c04b0552e136.webp" alt="" onerror="this.src=https://cdn.aimiliy.top/backImg/loading2.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-09-10</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/f8b6e52d.html&quot;);" href="javascript:void(0);" alt="">二次元随机壁纸API</a><div class="blog-slider__text">整理一下二次元随机壁纸API</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/f8b6e52d.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="/",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script></body></html>