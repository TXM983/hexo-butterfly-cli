<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Pytorch基础知识（二） | MuXiaoChen🍊</title><meta name="keywords" content="Pytorch,Machine Learning,Deep Learning"><meta name="author" content="MuXiaoChen🍊,1554267532@qq.com"><meta name="copyright" content="MuXiaoChen🍊"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="本文主要介绍有关Pytorch的基础知识（二）"><meta property="og:type" content="article"><meta property="og:title" content="Pytorch基础知识（二）"><meta property="og:url" content="https://miraii.cn/posts/414a7aa/"><meta property="og:site_name" content="MuXiaoChen🍊"><meta property="og:description" content="本文主要介绍有关Pytorch的基础知识（二）"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.aimiliy.top/articleBackground/article7.webp"><meta property="article:published_time" content="2024-07-04T09:29:30.000Z"><meta property="article:modified_time" content="2025-05-29T15:23:47.806Z"><meta property="article:author" content="MuXiaoChen🍊"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.aimiliy.top/articleBackground/article7.webp"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="https://miraii.cn/posts/414a7aa/"><link rel="preconnect" href="//cdn.aimiliy.top"><link rel="preconnect" href="//umami.aimiliy.top"><link rel="preconnect" href="//busuanzi.aimiliy.top"><meta name="baidu-site-verification" content="codeva-JLcaXpJvSI"><meta name="baidu-site-verification" content="codeva-XXU4K1uicS"><link rel="stylesheet" href="/css/index.css?v=1.7.59"><link rel="stylesheet" href="https://cdn.aimiliy.top/npm/font-awesome/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.aimiliy.top/npm/swiper@11.2.6-c/swiper-bundle.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="/css/swiperstyle.css?v=1.7.59" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.aimiliy.top/npm/hexo-butterfly-tag-plugins-plus/font-awesome-animation.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.aimiliy.top/npm/aplayer@1.10.1/APlayer.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.aimiliy.top/npm/toastify@1.12.0/toastify.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.aimiliy.top/npm/fancybox@5.0.36/fancybox.min.css" media="print" onload='this.media="all"'><script async src="https://umami.aimiliy.top/script.js" data-website-id="3348d0ad-813b-4d17-98d2-d5404445f786"></script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"TMAQULEA61",apiKey:"cbdc736a26353ade14da768f54cde1fe",indexName:"MuXiaoChen",hitsPerPage:6,languages:{input_placeholder:"搜索文章",hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,noticeOutdate:{limitDay:365,position:"top",messagePrev:"It has been",messageNext:"days since the last update, the content of the article may be outdated."},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:300,highlightFullpage:!0,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.aimiliy.top/npm/infinitegrid@4.12.0/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!0,isAnchor:!1,percent:{toc:!0}}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Pytorch基础知识（二）",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2025-05-29 23:23:47"}</script><script>(()=>{window.saveToLocal={set:(e,t,o)=>{if(!o)return;const n=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:n}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:n}=JSON.parse(t);if(!(Date.now()>n))return o;localStorage.removeItem(e)}},window._scriptCache={},window.getScript=(e,t={})=>{if(window._scriptCache[e])return window._scriptCache[e];const o=new Promise(((o,n)=>{if(document.querySelector(`script[src="${e}"]`)){if("complete"===document.readyState)o();else{const e=()=>{"complete"===document.readyState&&(o(),document.removeEventListener("readystatechange",e))};document.addEventListener("readystatechange",e)}return}const a=document.createElement("script");a.src=e,a.async=!0,Object.entries(t).forEach((([e,t])=>a.setAttribute(e,t))),a.onload=()=>o(),a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||o()},a.onerror=()=>n(new Error(`Script load error: ${e}`)),document.head.appendChild(a)}));return window._scriptCache[e]=o,o},window.btf={getCSS:(e,t)=>new Promise(((o,n)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=e,t&&(a.id=t),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||o()},a.onerror=n,document.head.appendChild(a)})),addGlobalFn:(e,t,o=!1,n=window)=>{const a=n.globalFn||{};a[e]=a[e]||{},a[e][o||Object.keys(a[e]).length]=t,n.globalFn=a}};const e=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},t=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","ffffff")};window.activateDarkMode=e,window.activateLightMode=t;const o=saveToLocal.get("theme"),n=(new Date).getHours();void 0===o?n<=6||n>=18?e():t():"light"===o?t():e();/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><style id="themeColor"></style><style id="transPercent"></style><style id="bgFilterParam"></style><style id="defineBg"></style><style id="menu_shadow"></style><style id="aside-pos"></style><style id="global-font"></style><script defer data-pjax src="/lib/hbe.js?v=1.7.59"></script><link href="/css/hbe.style.css?v=1.7.59" rel="stylesheet"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="MuXiaoChen🍊" type="application/atom+xml"></head><body data-type=""><div id="loading-box" onclick='document.getElementById("loading-box").classList.add("loaded")'><div class="mbg-top"></div><img class="mbg-loading-img nolazyload" alt="加载头像" src="https://cdn.aimiliy.top/avatar/202504256.webp"><div class="mbg-bottom"></div></div><div id="web_bg"></div><div id="an_music_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/avatar/202504256.webp" onerror='onerror=null,src="/assets/r1.webp"' alt="avatar"></div><div class="author-info__name">MuXiaoChen🍊</div><div class="author-info__description">No pains, no gains. 🌱🔥🚀</div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">64</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn-bar" target="_blank" rel="noopener" href="https://github.com/TXM983" title="前往作者的Github"><i class="fab fa-github" style="margin-left:40px"></i><span>Follow Me</span><i class="faa-passing animated" style="padding-left:20px;display:inline-block;vertical-align:middle"><svg class="icon" style="height:28px;width:28px;fill:currentColor;position:relative;top:5px"><use xlink:href="#icon-xiaoqiche"></use></svg></i></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="/assets/QRCode.webp" target="_blank" title="微信"><svg class="social_icon faa-tada" aria-hidden="true"><use xlink:href="#icon-weixin"></use></svg></a><a class="social-icon faa-parent animated-hover" href="https://res.abeim.cn/api/qq/?qq=1554267532" target="_blank" title="QQ"><svg class="social_icon faa-tada" aria-hidden="true"><use xlink:href="#icon-QQ"></use></svg></a><a class="social-icon faa-parent animated-hover" href="mailto:1554267532@qq.com" target="_blank" title="QQ邮箱"><svg class="social_icon faa-tada" aria-hidden="true"><use xlink:href="#icon-youxiang"></use></svg></a><a class="social-icon faa-parent animated-hover" href="/atom.xml" target="_blank" title="RSS"><svg class="social_icon faa-tada" aria-hidden="true"><use xlink:href="#icon-RSS"></use></svg></a></div><div class="menus_items"><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-home-19"></use></svg> <span class="menu_word" style="font-size:17px">导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-home"></use></svg> <span class="menu_word" style="font-size:17px">首页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-guidang1"></use></svg> <span class="menu_word" style="font-size:17px">归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-sekuaibiaoqian"></use></svg> <span class="menu_word" style="font-size:17px">标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-fenlei"></use></svg> <span class="menu_word" style="font-size:17px">分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-kafei"></use></svg> <span class="menu_word" style="font-size:17px">休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-yinle"></use></svg> <span class="menu_word" style="font-size:17px">音乐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-dianying1"></use></svg> <span class="menu_word" style="font-size:17px">影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-youxishoubing"></use></svg> <span class="menu_word" style="font-size:17px">游戏</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/bangumi/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-wodezhuifan"></use></svg> <span class="menu_word" style="font-size:17px">追番</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-liwu-18"></use></svg> <span class="menu_word" style="font-size:17px">百宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-tubiaozhizuomoban"></use></svg> <span class="menu_word" style="font-size:17px">画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-nvwumao"></use></svg> <span class="menu_word" style="font-size:17px">动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-zhifengche"></use></svg> <span class="menu_word" style="font-size:17px">收藏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-shejiaoxinxi-18"></use></svg> <span class="menu_word" style="font-size:17px">社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-pengyouquan"></use></svg> <span class="menu_word" style="font-size:17px">朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-liuyan"></use></svg> <span class="menu_word" style="font-size:17px">留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-lianjie"></use></svg> <span class="menu_word" style="font-size:17px">友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-wangye-12"></use></svg> <span class="menu_word" style="font-size:17px">网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon--tongjibiao"></use></svg> <span class="menu_word" style="font-size:17px">网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-shujutongji1"></use></svg> <span class="menu_word" style="font-size:17px">文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingshalou"></use></svg> <span class="menu_word" style="font-size:17px">流光拾遗</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-maoliang"></use></svg> <span class="menu_word" style="font-size:17px">个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-qunliaotian"></use></svg> <span class="menu_word" style="font-size:17px">生活点滴</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/secret/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-youhua-fengshu"></use></svg> <span class="menu_word" style="font-size:17px">浮光小筑</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-paperplane"></use></svg> <span class="menu_word" style="font-size:17px">自述与我</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">MuXiaoChen🍊</a></span><div class="menus_items"><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-home-19"></use></svg> <span class="menu_word" style="font-size:17px">导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-home"></use></svg> <span class="menu_word" style="font-size:17px">首页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-guidang1"></use></svg> <span class="menu_word" style="font-size:17px">归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-sekuaibiaoqian"></use></svg> <span class="menu_word" style="font-size:17px">标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-fenlei"></use></svg> <span class="menu_word" style="font-size:17px">分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-kafei"></use></svg> <span class="menu_word" style="font-size:17px">休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-yinle"></use></svg> <span class="menu_word" style="font-size:17px">音乐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-dianying1"></use></svg> <span class="menu_word" style="font-size:17px">影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-youxishoubing"></use></svg> <span class="menu_word" style="font-size:17px">游戏</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/bangumi/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-wodezhuifan"></use></svg> <span class="menu_word" style="font-size:17px">追番</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-liwu-18"></use></svg> <span class="menu_word" style="font-size:17px">百宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-tubiaozhizuomoban"></use></svg> <span class="menu_word" style="font-size:17px">画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-nvwumao"></use></svg> <span class="menu_word" style="font-size:17px">动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-zhifengche"></use></svg> <span class="menu_word" style="font-size:17px">收藏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-shejiaoxinxi-18"></use></svg> <span class="menu_word" style="font-size:17px">社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-pengyouquan"></use></svg> <span class="menu_word" style="font-size:17px">朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-liuyan"></use></svg> <span class="menu_word" style="font-size:17px">留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-lianjie"></use></svg> <span class="menu_word" style="font-size:17px">友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-wangye-12"></use></svg> <span class="menu_word" style="font-size:17px">网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon--tongjibiao"></use></svg> <span class="menu_word" style="font-size:17px">网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-shujutongji1"></use></svg> <span class="menu_word" style="font-size:17px">文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingshalou"></use></svg> <span class="menu_word" style="font-size:17px">流光拾遗</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-maoliang"></use></svg> <span class="menu_word" style="font-size:17px">个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-qunliaotian"></use></svg> <span class="menu_word" style="font-size:17px">生活点滴</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/secret/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-youhua-fengshu"></use></svg> <span class="menu_word" style="font-size:17px">浮光小筑</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menus_svg_header faa-tada" aria-hidden="true"><use xlink:href="#icon-paperplane"></use></svg> <span class="menu_word" style="font-size:17px">自述与我</span></a></li></ul></div></div><div id="name-container-mask"><div class="visible" id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></div></div><div id="nav-right"><div id="search-button"><span class="search site-page faa-parent animated-hover" title="检索站内任何你想要的信息"><svg class="faa-tada svg-header" aria-hidden="true"><use xlink:href="#icon-sousuo5"></use></svg></span></div><div id="beautify-button"><span class="site-page meihua faa-parent animated-hover" onclick="toggleWinbox()" title="美化设置-自定义你的风格"><svg class="faa-tada svg-header" aria-hidden="true"><use xlink:href="#icon-meihua"></use></svg></span></div><div id="tenYears"><a class="site-page faa-parent animated-hover" target="_blank" rel="noopener" href="https://foreverblog.cn/go.html" title="穿梭虫洞-十年之约"><svg class="svg-header faa-tada" aria-hidden="true"><use xlink:href="#icon-huochezhan"></use></svg></a></div><div id="toggle-menu"><span class="site-page"><a><i class="fas fa-bars fa-fw" style="color:#3e86f1"></i></a></span></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch基础知识（二）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><svg class="meta_icon post-meta-icon" style="width:30px;height:30px;position:relative;top:10px"><use xlink:href="#icon-rili"></use></svg><span class="post-meta-label">发表于</span> <time class="post-meta-date-created" datetime="2024-07-04T09:29:30.000Z" title="发表于 2024-07-04 17:29:30">2024-07-04</time><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-gengxin1"></use></svg><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-29T15:23:47.806Z" title="更新于 2025-05-29 23:23:47">2025-05-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-biaoqian"></use></svg><a class="post-meta-categories" href="/categories/%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0/">教程笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:8px"><use xlink:href="#icon-charuword"></use></svg><span class="post-meta-label">字数总计:</span><span class="word-count">5354</span><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:20px;height:20px;position:relative;top:5px"><use xlink:href="#icon-shizhong"></use></svg><span class="post-meta-label">阅读时长:</span><span>26分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Pytorch基础知识（二）"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:5px"><use xlink:href="#icon-eye"></use></svg><span class="post-meta-label">阅读量:</span><span id="busuanzi_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s58 18 88 18 58-18 88-18 58 18 88 18v44h-352Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div class="post-outdate-notice" id="post-outdate-notice" data="{&quot;limitDay&quot;:365,&quot;messagePrev&quot;:&quot;It has been&quot;,&quot;messageNext&quot;:&quot;days since the last update, the content of the article may be outdated.&quot;,&quot;postUpdate&quot;:&quot;2025-05-29 23:23:47&quot;}" hidden></div><h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><p><strong>torch.Tensor</strong>是一种包含单一数据类型元素的多维矩阵<br>Torch定义了10种CPU tensor类型和GPU tensor类型：</p><div class="table-container"><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td>torch.float32 or torch.float</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64-bit floating point</td><td>torch.float64 or torch.double</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr><tr><td>16-bit floating point [1]</td><td>torch.float16 or torch.half</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>16-bit floating point [2]</td><td>torch.bfloat16</td><td>torch.BFloat16Tensor</td><td>torch.cuda.BFloat16Tensor</td></tr><tr><td>32-bit complex</td><td>torch.complex32 or torch.chalf</td><td></td><td></td></tr><tr><td>64-bit complex</td><td>torch.complex64 or torch.cfloat</td><td></td><td></td></tr><tr><td>128-bit complex</td><td>torch.complex128 or torch.cdouble</td><td></td><td></td></tr><tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr><tr><td>8-bit integer (signed)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer (signed)</td><td>torch.int16 or torch.short</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32-bit integer (signed)</td><td>torch.int32 or torch.int</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64-bit integer (signed)</td><td>torch.int64 or torch.long</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr><tr><td>Boolean</td><td>torch.bool</td><td>torch.BoolTensor</td><td>torch.cuda.BoolTensor</td></tr><tr><td>quantized 8-bit integer (unsigned)</td><td>torch.quint8</td><td>torch.ByteTensor</td><td>/</td></tr><tr><td>quantized 8-bit integer (signed)</td><td>torch.qint8</td><td>torch.CharTensor</td><td>/</td></tr><tr><td>quantized 32-bit integer (signed)</td><td>torch.qint32</td><td>torch.IntTensor</td><td>/</td></tr><tr><td>quantized 4-bit integer (unsigned) [3]</td><td>torch.quint4x2</td><td>torch.ByteTensor</td><td>/</td></tr></tbody></table></div><div class="note info flat"><p>创建</p></div><p>一个张量tensor可以从Python的<strong>list</strong>或序列构建：<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.FloatTensor([[1, 2, 3], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line">Out[0]: </span><br><span class="line">tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br></pre></td></tr></table></figure><p></p><p>根据可选择的大小和数据新建一个tensor。 如果没有提供参数，将会返回一个空的零维张量。如果提供了<strong>numpy.ndarray</strong>,<strong>torch.Tensor</strong>或<strong>torch.Storage</strong>，将会返回一个有同样参数的tensor.如果提供了python序列，将会从序列的副本创建一个tensor<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">接口 一个空张量tensor可以通过规定其大小来构建</span></span><br><span class="line">class torch.Tensor</span><br><span class="line">class torch.Tensor(*sizes)</span><br><span class="line">class torch.Tensor(size)</span><br><span class="line">class torch.Tensor(sequence)</span><br><span class="line">class torch.Tensor(ndarray)</span><br><span class="line">class torch.Tensor(tensor)</span><br><span class="line">class torch.Tensor(storage)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">实例化</span></span><br><span class="line">torch.IntTensor(2, 4).zero_()</span><br></pre></td></tr></table></figure><p></p><p>可以用python的索引和切片来获取和修改一个张量tensor中的内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">x[1][2]</span><br><span class="line">Out[0]: tensor(6.)</span><br><span class="line"></span><br><span class="line">x[0][1] = 8</span><br><span class="line">x</span><br><span class="line">Out[1]: </span><br><span class="line">tensor([[1., 8., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br></pre></td></tr></table></figure><p>每一个张量tensor都有一个相应的<strong>torch.Storage</strong>用来保存其数据。类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算<strong>会改变tensor的函数操作会用一个下划线后缀来标示</strong>。比如，<strong>torch.FloatTensor.abs_()</strong>会在原地计算绝对值，并返回改变后的tensor，而<strong>tensor.FloatTensor.abs()</strong>将会在一个新的tensor中计算结果</p><h1 id="storage"><a href="#storage" class="headerlink" title="storage"></a>storage</h1><div class="note info flat"><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/ebd7f6395bf4">tensor的数据结构、storage()、stride()、storage_offset()</a></p></div><p>pytorch中一个tensor对象分为头信息区（Tensor）和存储区（Storage）两部分<br><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/articlePic/img_2.png" alt="img.png"><br>头信息区主要保存tensor的形状（size）、步长（stride）、数据类型（type）等信息；而真正的data（数据)则以连续一维数组的形式放在存储区，由torch.Storage实例管理着<br><strong>注意：storage永远是一维数组，任何维度的tensor的实际数据都存储在一维的storage中</strong></p><div class="note info flat"><p>获取tensor的storage</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[1.0, 4.0],[2.0, 1.0],[3.0, 5.0]])</span><br><span class="line">a.storage()</span><br><span class="line">Out[0]: </span><br><span class="line"> 1.0</span><br><span class="line"> 4.0</span><br><span class="line"> 2.0</span><br><span class="line"> 1.0</span><br><span class="line"> 3.0</span><br><span class="line"> 5.0</span><br><span class="line">[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]</span><br><span class="line"></span><br><span class="line">a.storage()[2] = 9</span><br><span class="line"></span><br><span class="line">id(a.storage())</span><br><span class="line">Out[1]: 1343354913168</span><br></pre></td></tr></table></figure><h1 id="Pytorch加载数据"><a href="#Pytorch加载数据" class="headerlink" title="Pytorch加载数据"></a>Pytorch加载数据</h1><p>Pytorch中加载数据需要Dataset、Dataloader。</p><ul><li>Dataset提供一种方式去获取每个数据及其对应的label，告诉我们总共有多少个数据。</li><li>Dataloader为后面的网络提供不同的数据形式，它将一批一批数据进行一个打包。</li></ul><h1 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor())               </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回</span>      </span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=64,shuffle=True,num_workers=0,drop_last=False)      </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">用<span class="keyword">for</span>循环取出DataLoader打包好的四个数据</span></span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line">step = 0</span><br><span class="line">for data in test_loader:</span><br><span class="line">    imgs, targets = data # 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成             </span><br><span class="line">    writer.add_images(&quot;test_data&quot;,imgs,step)</span><br><span class="line">    step = step + 1</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h1><p>① Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。<br>② Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果。<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from torchvision import transforms</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">img_path = &quot;Data/FirstTypeData/val/bees/10870992_eebeeb3a12.jpg&quot;</span><br><span class="line">img = Image.open(img_path)  </span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()  # 创建 transforms.ToTensor类 的实例化对象</span><br><span class="line">tensor_img = tensor_trans(img)  # 调用 transforms.ToTensor类 的__call__的魔术方法   </span><br><span class="line">print(tensor_img)</span><br></pre></td></tr></table></figure><p></p><h1 id="torchvision数据集"><a href="#torchvision数据集" class="headerlink" title="torchvision数据集"></a>torchvision数据集</h1><p>① torchvision中有很多数据集，当我们写代码时指定相应的数据集指定一些参数，它就可以自行下载。<br>② CIFAR-10数据集包含60000张32×32的彩色图片，一共10个类别，其中50000张训练图片，10000张测试图片。<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=True,download=True) # root为存放数据集的相对路线</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=False,download=True) # train=True是训练集，train=False是测试集  </span><br><span class="line"></span><br><span class="line">print(test_set[0])       # 输出的3是target </span><br><span class="line">print(test_set.classes)  # 测试数据集中有多少种</span><br><span class="line"></span><br><span class="line">img, target = test_set[0] # 分别获得图片、target</span><br><span class="line">print(img)</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line">print(test_set.classes[target]) # 3号target对应的种类</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure><p></p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>① Loss损失函数一方面计算实际输出和目标之间的差距。<br>② Loss损失函数另一方面为我们更新输出提供一定的依据<br></p><div class="note info flat"><p>L1loss损失函数</p></div><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">inputs = torch.tensor([1,2,3],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([1,2,5],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(1,1,1,3))</span><br><span class="line">targets = torch.reshape(targets,(1,1,1,3))</span><br><span class="line">loss = L1Loss()  # 默认为 maen</span><br><span class="line">result = loss(inputs,targets)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><br><div class="note info flat"><p>MSE损失函数</p></div><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">from torch import nn</span><br><span class="line">inputs = torch.tensor([1,2,3],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([1,2,5],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(1,1,1,3))</span><br><span class="line">targets = torch.reshape(targets,(1,1,1,3))</span><br><span class="line">loss_mse = nn.MSELoss()</span><br><span class="line">result_mse = loss_mse(inputs,targets)</span><br><span class="line">print(result_mse)</span><br></pre></td></tr></table></figure><br><div class="note info flat"><p>交叉熵损失函数</p></div><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">x = torch.tensor([0.1,0.2,0.3])</span><br><span class="line">y = torch.tensor([1])</span><br><span class="line">x = torch.reshape(x,(1,3)) # 1的 batch_size，有三类</span><br><span class="line">loss_cross = nn.CrossEntropyLoss()</span><br><span class="line">result_cross = loss_cross(x,y)</span><br><span class="line">print(result_cross)</span><br></pre></td></tr></table></figure><p></p><h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>① 损失函数调用backward方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。<br>② 梯度要清零，如果梯度不清零会导致梯度累加<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss() # 交叉熵    </span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器</span><br><span class="line">for data in dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = tudui(imgs)</span><br><span class="line">    result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距</span><br><span class="line">    optim.zero_grad()  # 梯度清零</span><br><span class="line">    result_loss.backward() # 反向传播，计算损失函数的梯度</span><br><span class="line">    optim.step()   # 根据梯度，对网络的参数进行调优</span><br><span class="line">    print(result_loss) # 对数据只看了一遍，只看了一轮，所以loss下降不大</span><br></pre></td></tr></table></figure><br><div class="note info flat"><p>神经网络学习率优化</p></div><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn </span><br><span class="line">from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=64,drop_last=True)</span><br><span class="line"></span><br><span class="line">class Tudui(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(3,32,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32,32,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32,64,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(1024,64),</span><br><span class="line">            Linear(64,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss() # 交叉熵    </span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1) # 每过 step_size 更新一次优化器，更新是学习率为原来的学习率的 0.1 倍    </span><br><span class="line">for epoch in range(20):</span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for data in dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距</span><br><span class="line">        optim.zero_grad()  # 梯度清零</span><br><span class="line">        result_loss.backward() # 反向传播，计算损失函数的梯度</span><br><span class="line">        optim.step()   # 根据梯度，对网络的参数进行调优</span><br><span class="line">        scheduler.step() # 学习率太小了，所以20个轮次后，相当于没走多少</span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line">    print(running_loss) # 对这一轮所有误差的总和</span><br></pre></td></tr></table></figure><p></p><h1 id="网络模型使用及修改"><a href="#网络模型使用及修改" class="headerlink" title="网络模型使用及修改"></a>网络模型使用及修改</h1><div class="note info flat"><p>网络模型添加</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=True) # 下载卷积层对应的参数是多少、池化层对应的参数时多少，这些参数时ImageNet训练好了的</span><br><span class="line">vgg16_true.add_module(&#x27;add_linear&#x27;,nn.Linear(1000,10)) # 在VGG16后面添加一个线性层，使得输出为适应CIFAR10的输出，CIFAR10需要输出10个种类</span><br><span class="line"></span><br><span class="line">print(vgg16_true)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>网络模型修改</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=False) # 没有预训练的参数     </span><br><span class="line">print(vgg16_false)</span><br><span class="line">vgg16_false.classifier[6] = nn.Linear(4096,10)</span><br><span class="line">print(vgg16_false)</span><br></pre></td></tr></table></figure><h1 id="网络模型保存与读取"><a href="#网络模型保存与读取" class="headerlink" title="网络模型保存与读取"></a>网络模型保存与读取</h1><div class="note info flat"><p>模型结构 + 模型参数</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=False)</span><br><span class="line">torch.save(vgg16,&quot;./model/vgg16_method1.pth&quot;) # 保存方式一：模型结构 + 模型参数      </span><br><span class="line">print(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(&quot;./model/vgg16_method1.pth&quot;) # 保存方式一对应的加载模型    </span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>模型参数（官方推荐），不保存网络模型结构</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=False)</span><br><span class="line">torch.save(vgg16.state_dict(),&quot;./model/vgg16_method2.pth&quot;) # 保存方式二：模型参数（官方推荐）,不再保存网络模型结构  </span><br><span class="line">print(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(&quot;./model/vgg16_method2.pth&quot;) # 导入模型参数   </span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><h1 id="固定模型参数"><a href="#固定模型参数" class="headerlink" title="固定模型参数"></a>固定模型参数</h1><p>在训练过程中可能需要固定一部分模型的参数，只更新另一部分参数，有两种思路实现这个目标</p><ul><li>1.一个是设置不要更新参数的<strong>网络层</strong>为false</li><li>2.另一个就是在定义优化器时只传入要更新的参数<br>当然最优的做法是，优化器中只传入requires_grad=True的参数，这样占用的内存会更小一点，效率也会更高<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义一个简单的网络</span></span><br><span class="line">class net(nn.Module):</span><br><span class="line">    def __init__(self, num_class=3):</span><br><span class="line">        super(net, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(8, 4)</span><br><span class="line">        self.fc2 = nn.Linear(4, num_class)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.fc2(self.fc1(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = net()</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">冻结fc1层的参数</span></span><br><span class="line">for name, param in model.named_parameters():</span><br><span class="line">    if &quot;fc1&quot; in name:</span><br><span class="line">        param.requires_grad = False</span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">只传入requires_grad = True的参数</span></span><br><span class="line">optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters(), lr=1e-2)</span><br><span class="line">print(&quot;model.fc1.weight&quot;, model.fc1.weight)</span><br><span class="line">print(&quot;model.fc2.weight&quot;, model.fc2.weight)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line">for epoch in range(10):</span><br><span class="line">    x = torch.randn((3, 8))</span><br><span class="line">    label = torch.randint(0, 3, [3]).long()</span><br><span class="line">    output = model(x)</span><br><span class="line"></span><br><span class="line">    loss = loss_fn(output, label)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">print(&quot;model.fc1.weight&quot;, model.fc1.weight)</span><br><span class="line">print(&quot;model.fc2.weight&quot;, model.fc2.weight)</span><br></pre></td></tr></table></figure><h1 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h1></li></ul><div class="note info flat"><p>DataLoader加载数据集</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">length 长度</span></span><br><span class="line">train_data_size = len(train_data)</span><br><span class="line">test_data_size = len(test_data)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line">print(&quot;训练数据集的长度：&#123;&#125;&quot;.format(train_data_size))</span><br><span class="line">print(&quot;测试数据集的长度：&#123;&#125;&quot;.format(test_data_size))</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data_size, batch_size=64)        </span><br><span class="line">test_dataloader = DataLoader(test_data_size, batch_size=64)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>测试网络正确</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">搭建神经网络</span></span><br><span class="line">class Tudui(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,32,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,64,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Flatten(),  # 展平后变成 64*4*4 了</span><br><span class="line">            nn.Linear(64*4*4,64),</span><br><span class="line">            nn.Linear(64,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    tudui = Tudui()</span><br><span class="line">    input = torch.ones((64,3,32,32))</span><br><span class="line">    output = tudui(input)</span><br><span class="line">    print(output.shape)  # 测试输出的尺寸是不是我们想要的</span><br></pre></td></tr></table></figure><div class="note info flat"><p>网络训练数据</p></div><p>① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。<br>② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。<br>③ 不启用 Batch Normalization 和 Dropout。 如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。<br>④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的性质。<br>⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里</span></span><br><span class="line">class Tudui(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,32,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,64,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Flatten(),  # 展平后变成 64*4*4 了</span><br><span class="line">            nn.Linear(64*4*4,64),</span><br><span class="line">            nn.Linear(64,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        return x</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">length 长度</span></span><br><span class="line">train_data_size = len(train_data)</span><br><span class="line">test_data_size = len(test_data)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line">print(&quot;训练数据集的长度：&#123;&#125;&quot;.format(train_data_size))</span><br><span class="line">print(&quot;测试数据集的长度：&#123;&#125;&quot;.format(test_data_size))</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=64)        </span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建网络模型</span></span><br><span class="line">tudui = Tudui() </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">优化器</span></span><br><span class="line">learning = 0.01  # 1e-2 就是 0.01 的意思</span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置网络的一些参数</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记录训练的次数</span></span><br><span class="line">total_train_step = 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记录测试的次数</span></span><br><span class="line">total_test_step = 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">训练的轮次</span></span><br><span class="line">epoch = 10</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加 tensorboard</span></span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line"></span><br><span class="line">for i in range(epoch):</span><br><span class="line">    print(&quot;-----第 &#123;&#125; 轮训练开始-----&quot;.format(i+1))</span><br><span class="line"></span><br><span class="line">    # 训练步骤开始</span><br><span class="line">    tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用</span><br><span class="line">    for data in train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距</span><br><span class="line"></span><br><span class="line">        # 优化器对模型调优</span><br><span class="line">        optimizer.zero_grad()  # 梯度清零</span><br><span class="line">        loss.backward() # 反向传播，计算损失函数的梯度</span><br><span class="line">        optimizer.step()   # 根据梯度，对网络的参数进行调优</span><br><span class="line"></span><br><span class="line">        total_train_step = total_train_step + 1</span><br><span class="line">        if total_train_step % 100 == 0:</span><br><span class="line">            print(&quot;训练次数：&#123;&#125;，Loss：&#123;&#125;&quot;.format(total_train_step,loss.item()))  # 方式二：获得loss值</span><br><span class="line">            writer.add_scalar(&quot;train_loss&quot;,loss.item(),total_train_step)</span><br><span class="line"></span><br><span class="line">    # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）</span><br><span class="line">    tudui.eval()  # 当网络中有dropout层、batchnorm层时，这些层不能起作用</span><br><span class="line">    total_test_loss = 0</span><br><span class="line">    total_accuracy = 0</span><br><span class="line">    with torch.no_grad():  # 没有梯度了</span><br><span class="line">        for data in test_dataloader: # 测试数据集提取数据</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            outputs = tudui(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失</span><br><span class="line">            total_test_loss = total_test_loss + loss.item() # 所有loss</span><br><span class="line">            accuracy = (outputs.argmax(1) == targets).sum()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line"></span><br><span class="line">    print(&quot;整体测试集上的Loss：&#123;&#125;&quot;.format(total_test_loss))</span><br><span class="line">    print(&quot;整体测试集上的正确率：&#123;&#125;&quot;.format(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(&quot;test_loss&quot;,total_test_loss,total_test_step)</span><br><span class="line">    writer.add_scalar(&quot;test_accuracy&quot;,total_accuracy/test_data_size,total_test_step)  </span><br><span class="line">    total_test_step = total_test_step + 1</span><br><span class="line"></span><br><span class="line">    torch.save(tudui, &quot;./model/tudui_&#123;&#125;.pth&quot;.format(i)) # 保存每一轮训练后的结果</span><br><span class="line">    #torch.save(tudui.state_dict(),&quot;tudui_&#123;&#125;.path&quot;.format(i)) # 保存方式二         </span><br><span class="line">    print(&quot;模型已保存&quot;)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p></p><h1 id="模型-参数查看"><a href="#模型-参数查看" class="headerlink" title="模型|参数查看"></a>模型|参数查看</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyModel(torch.nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.layer1 = torch.nn.Sequential(</span><br><span class="line">            torch.nn.Linear(3, 4),</span><br><span class="line">            torch.nn.Linear(4, 3),</span><br><span class="line">        )</span><br><span class="line">        self.layer2 = torch.nn.Linear(3, 6)</span><br><span class="line">        self.layer3 = torch.nn.Sequential(</span><br><span class="line">            torch.nn.Linear(6, 7),</span><br><span class="line">            torch.nn.Linear(7, 5),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = MyModel()</span><br><span class="line">print(net)</span><br><span class="line">MyModel(</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): Linear(in_features=3, out_features=4, bias=True)</span><br><span class="line">    (1): Linear(in_features=4, out_features=3, bias=True)</span><br><span class="line">  )</span><br><span class="line">  (layer2): Linear(in_features=3, out_features=6, bias=True)</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (0): Linear(in_features=6, out_features=7, bias=True)</span><br><span class="line">    (1): Linear(in_features=7, out_features=5, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>查看参数<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">for layer in net.modules():</span><br><span class="line">    print(type(layer))  # 查看每一层的类型</span><br><span class="line">    # print(layer)</span><br><span class="line">&lt;class &#x27;__main__.MyModel&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.container.Sequential&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.container.Sequential&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for param in net.parameters():</span><br><span class="line">    print(param.shape)  # 打印每一层的参数</span><br><span class="line">torch.Size([4, 3])</span><br><span class="line">torch.Size([4])</span><br><span class="line">torch.Size([3, 4])</span><br><span class="line">torch.Size([3])</span><br><span class="line">torch.Size([6, 3])</span><br><span class="line">torch.Size([6])</span><br><span class="line">torch.Size([7, 6])</span><br><span class="line">torch.Size([7])</span><br><span class="line">torch.Size([5, 7])</span><br><span class="line">torch.Size([5])</span><br><span class="line"></span><br><span class="line">for name, param in net.named_parameters():</span><br><span class="line">    print(name, param.shape)  # 看的更细</span><br><span class="line">layer1.0.weight torch.Size([4, 3])</span><br><span class="line">layer1.0.bias torch.Size([4])</span><br><span class="line">layer1.1.weight torch.Size([3, 4])</span><br><span class="line">layer1.1.bias torch.Size([3])</span><br><span class="line">layer2.weight torch.Size([6, 3])</span><br><span class="line">layer2.bias torch.Size([6])</span><br><span class="line">layer3.0.weight torch.Size([7, 6])</span><br><span class="line">layer3.0.bias torch.Size([7])</span><br><span class="line">layer3.1.weight torch.Size([5, 7])</span><br><span class="line">layer3.1.bias torch.Size([5])</span><br><span class="line"></span><br><span class="line">for key, value in net.state_dict().items():  # 参数名以及参数</span><br><span class="line">    print(key, value.shape)</span><br><span class="line">layer1.0.weight torch.Size([4, 3])</span><br><span class="line">layer1.0.bias torch.Size([4])</span><br><span class="line">layer1.1.weight torch.Size([3, 4])</span><br><span class="line">layer1.1.bias torch.Size([3])</span><br><span class="line">layer2.weight torch.Size([6, 3])</span><br><span class="line">layer2.bias torch.Size([6])</span><br><span class="line">layer3.0.weight torch.Size([7, 6])</span><br><span class="line">layer3.0.bias torch.Size([7])</span><br><span class="line">layer3.1.weight torch.Size([5, 7])</span><br><span class="line">layer3.1.bias torch.Size([5])</span><br></pre></td></tr></table></figure><p></p><h1 id="模型保存-加载"><a href="#模型保存-加载" class="headerlink" title="模型保存|加载"></a>模型保存|加载</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1、加载模型+参数</span></span><br><span class="line">net = torch.load(&quot;resnet50.pth&quot;)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2、已有模型,加载预训练参数</span></span><br><span class="line">resnet50 = models.resnet50(weights=None)  </span><br><span class="line">resnet50.load_state_dict(torch.load(&quot;resnet58_weight.pth&quot;))</span><br></pre></td></tr></table></figure><h1 id="网络的修改"><a href="#网络的修改" class="headerlink" title="网络的修改"></a>网络的修改</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">from torchvision import models</span><br><span class="line"></span><br><span class="line">alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (4): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">    (5): ReLU(inplace=True)</span><br><span class="line">    (6): Linear(in_features=4096, out_features=1000, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>修改网络结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1、-----删除网络的最后一层-----</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">del alexnet.classifier</span></span><br><span class="line">del alexnet.classifier[6]</span><br><span class="line">print(alexnet)</span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (4): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">    (5): ReLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2、-----删除网络的最后多层-----</span></span><br><span class="line">alexnet.classifier = alexnet.classifier[:-2]</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3、-----修改网络的某一层-----</span></span><br><span class="line">alexnet.classifier[6] = nn.Linear(in_features=4096, out_features=1024)</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Linear(in_features=4096, out_features=1024, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">4、-----网络添加层，每次添加一层-----</span></span><br><span class="line">alexnet.classifier.add_module(&#x27;7&#x27;, nn.ReLU(inplace=True))</span><br><span class="line">alexnet.classifier.add_module(&#x27;8&#x27;, nn.Linear(in_features=1024, out_features=20))</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Linear(in_features=4096, out_features=1024, bias=True)</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): Linear(in_features=1024, out_features=20, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="参数冻结"><a href="#参数冻结" class="headerlink" title="参数冻结"></a>参数冻结</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">任务一∶</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1、将模型A作为backbone，修改为模型B</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2、模型A的预训练参数加载到模型B上</span></span><br><span class="line"></span><br><span class="line">resnet_modified = resnet50()</span><br><span class="line">new_weights_dict = resnet_modified.state_dict()</span><br><span class="line"></span><br><span class="line">resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)</span><br><span class="line">weights_dict = resnet.state_dict()</span><br><span class="line"></span><br><span class="line">for k in weights_dict.keys():</span><br><span class="line">    if k in new_weights_dict.keys() and not k.startswith(&#x27;fc&#x27;):</span><br><span class="line">        new_weights_dict[k] = weights_dict[k]</span><br><span class="line">resnet_modified.load_state_dict(new_weights_dict)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">resnet_modified.load_state_dict(new_weights_dict,strict=False)</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">任务二:</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">冻结与训练好的参数</span></span><br><span class="line">params = []</span><br><span class="line">train_layer = [&#x27;layer5&#x27;, &#x27;conv_end&#x27;, &#x27;bn_end&#x27;]</span><br><span class="line">for name, param in resnet_modified.named_parameters():</span><br><span class="line">    if any(name.startswith(prefix) for prefix in train_layer):</span><br><span class="line">        print(name)</span><br><span class="line">        params.append(param)</span><br><span class="line">    else:</span><br><span class="line">        param.requires_grad = False</span><br><span class="line">optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=5e-4)</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>Pytorch基础知识（二）</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://miraii.cn/posts/414a7aa/">https://miraii.cn/posts/414a7aa/</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>MuXiaoChen🍊</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2024-07-04</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2025-05-29</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/"><div class="tags-punctuation"><svg class="faa-tada svg_tag icon" aria-hidden="true"><use xlink:href="#icon-wenzhangbiaoqian-copy"></use></svg></div>Pytorch</a><a class="post-meta__tags" href="/tags/Machine-Learning/"><div class="tags-punctuation"><svg class="faa-tada svg_tag icon" aria-hidden="true"><use xlink:href="#icon-wenzhangbiaoqian-copy"></use></svg></div>Machine Learning</a><a class="post-meta__tags" href="/tags/Deep-Learning/"><div class="tags-punctuation"><svg class="faa-tada svg_tag icon" aria-hidden="true"><use xlink:href="#icon-wenzhangbiaoqian-copy"></use></svg></div>Deep Learning</a></div></div><link rel="stylesheet" href="/css/coin.css?v=1.7.59" media="defer" onload='this.media="all"'><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">投喂作者</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.aimiliy.top/reward/wechat.webp" target="_blank"><img class="post-qr-code-img" src="https://cdn.aimiliy.top/reward/wechat.webp" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://cdn.aimiliy.top/reward/alipay.webp" target="_blank"><img class="post-qr-code-img" src="https://cdn.aimiliy.top/reward/alipay.webp" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></button></div><audio id="coinAudio" src="https://cdn.aimiliy.top/reward/aowu.m4a"></audio><script defer src="/js/coin.js?v=1.7.59"></script><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/5a986f0/" title="Pytorch基础知识（一）"><img class="cover nolazyload" src="https://cdn.aimiliy.top/articleBackground/article6.webp" onerror='onerror=null,src="/assets/r2.webp"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Pytorch基础知识（一）</div></div><div class="info-2"><div class="info-item-1">本文主要是介绍有关Pytorch的基础知识（一）</div></div></div></a><a class="pagination-related" href="/posts/f6122879/" title="内存开启XMP"><img class="cover nolazyload" src="https://cdn.aimiliy.top/articleBackground/article8.webp" onerror='onerror=null,src="/assets/r2.webp"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">内存开启XMP</div></div><div class="info-2"><div class="info-item-1">本文主要记录内存开启XMP时的过程</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/5a986f0/" title="Pytorch基础知识（一）"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/articleBackground/article6.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-05-29</div><div class="info-item-2">Pytorch基础知识（一）</div></div><div class="info-2"><div class="info-item-1">本文主要是介绍有关Pytorch的基础知识（一）</div></div></div></a></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><svg class="meta_icon" style="width:22px;height:22px;position:relative;top:5px"><use xlink:href="#icon-mulu1"></use></svg><span style="font-weight:700">目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Tensor"><span class="toc-number">1.</span> <span class="toc-text">Tensor</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#storage"><span class="toc-number">2.</span> <span class="toc-text">storage</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">3.</span> <span class="toc-text">Pytorch加载数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Tensorboard"><span class="toc-number">4.</span> <span class="toc-text">Tensorboard</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transforms"><span class="toc-number">5.</span> <span class="toc-text">transforms</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torchvision%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.</span> <span class="toc-text">torchvision数据集</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">7.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">8.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E5%8F%8A%E4%BF%AE%E6%94%B9"><span class="toc-number">9.</span> <span class="toc-text">网络模型使用及修改</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96"><span class="toc-number">10.</span> <span class="toc-text">网络模型保存与读取</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9A%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">11.</span> <span class="toc-text">固定模型参数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-number">12.</span> <span class="toc-text">训练流程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B-%E5%8F%82%E6%95%B0%E6%9F%A5%E7%9C%8B"><span class="toc-number">13.</span> <span class="toc-text">模型|参数查看</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98-%E5%8A%A0%E8%BD%BD"><span class="toc-number">14.</span> <span class="toc-text">模型保存|加载</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BF%AE%E6%94%B9"><span class="toc-number">15.</span> <span class="toc-text">网络的修改</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%86%BB%E7%BB%93"><span class="toc-number">16.</span> <span class="toc-text">参数冻结</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-first"></div><div id="footer-wrap"><div class="global-footer"><div class="footer-container"><section class="footer-section motto-container"><h3 class="section-heading">📜 生活箴言</h3><p class="motto-content">人生如逆旅，我亦是行人。保持从容心态，笑看云卷云舒。 不必追赶时间，学会驻足欣赏，心若向阳，处处皆暖阳。</p><a class="nav-item" target="_blank" rel="noopener" href="https://stellarium.org/">探索星空之旅 →</a></section><section class="footer-section nav-container"><h3 class="section-heading">🗺️ 网站地图</h3><nav class="nav-grid"><a class="nav-item" href="/">返回主页</a><a class="nav-item" href="/box/nav/">网址导航</a><a class="nav-item" href="/social/link/">我的朋友</a><a class="nav-item" href="/comments/">留点什么</a><a class="nav-item" href="/personal/about/">关于作者</a><a class="nav-item" href="/archives/">文章归档</a><a class="nav-item" href="/categories/">文章分类</a><a class="nav-item" href="/tags/">文章标签</a><a class="nav-item" href="/box/gallery/">我的画廊</a><a class="nav-item" href="/personal/bb/">日常分享</a><a class="nav-item" href="/site/time/">建设进程</a><a class="nav-item" href="/site/census/">网站统计</a></nav></section><section class="footer-section partner-container"><h3 class="section-heading">⌛ 友情链接</h3><div class="partner-grid"><a class="partner-link" target="_blank" rel="noopener" href="https://poetize.cn/" title="POETIZE"><img class="partner-image" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/avatar/poetize.cn.webp" alt="POETIZE"></a><a class="partner-link" target="_blank" rel="noopener" href="https://www.mxin.moe" title="铭心"><img class="partner-image" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/avatar/www.mxin.moe.webp" alt="铭心"></a><a class="partner-link" href="https://miraii.cn/" title="MuXiaoChen🍊"><img class="partner-image" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/avatar/202504256.webp" alt="MuXiaoChen🍊"></a><a class="partner-link" target="_blank" rel="noopener" href="https://starsei.com/" title="猫不吃鱼"><img class="partner-image" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/avatar/starsei.com.ico" alt="猫不吃鱼"></a><a class="partner-link" target="_blank" rel="noopener" href="https://www.hoshiroko.com" title="薄荷の小屋"><img class="partner-image" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/avatar/www.hoshiroko.com.webp" alt="薄荷の小屋"></a><a class="partner-link" target="_blank" rel="noopener" href="https://blog.ltya.top" title="岚天小窝"><img class="partner-image" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/avatar/blog.ltya.top.webp" alt="岚天小窝"></a><a class="partner-link" target="_blank" rel="noopener" href="https://jipa.moe" title="JIPA233の小窝"><img class="partner-image" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/avatar/jipa.moe.webp" alt="JIPA233の小窝"></a><a class="partner-link" target="_blank" rel="noopener" href="https://blog.iamsjy.com/" title="Tony’s Blog"><img class="partner-image" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.aimiliy.top/avatar/blog.iamsjy.com.webp" alt="Tony’s Blog"></a></div></section></div></div></div><div id="footer-wrap-bottom"><div id="footer-bottom"><div class="footer-bottom-content"><div class="footer-bottom-left"><span class="copyright"><b>&copy;2023-2025</b><b>&nbsp;&nbsp;By</b><b>&nbsp;MuXiaoChen🍊</b></span><div><a class="footer-bottom-link" target="_blank" href="https://icp.gov.moe/?keyword=20240348" rel="noopener external nofollow" title="萌备">萌ICP备20240348号</a><a class="footer-bottom-link" target="_blank" href="https://beian.miit.gov.cn/" rel="noopener external nofollow" title="工信部备案号">鄂ICP备2023025645号-2</a></div></div><div class="footer-bottom-right"><div id="workboard"><div style="font-size:17px;font-weight:700">本站居然运行了&nbsp;<span id="days">000</span>&nbsp;天&nbsp;<span id="hours">00</span>&nbsp;小时&nbsp;<span id="minutes">00</span>&nbsp;分&nbsp;<span id="seconds">00</span>&nbsp;秒<i class="fas fa-heartbeat" id="heartbeat"></i></div></div><div><a class="footer-bottom-link" target="_blank" href="https://www.aliyun.com/" rel="noopener external nofollow" title="本站通过阿里云ESA提供站点加速与保护">阿里云ESA</a><a class="footer-bottom-link" target="_blank" href="https://hexo.io/zh-cn/" rel="noopener external nofollow" title="本站使用Hexo架构搭建而成">Hexo静态框架</a><a class="footer-bottom-link" target="_blank" href="https://butterfly.js.org/" rel="noopener external nofollow" title="本站由Butterfly主题魔改而成">Butterfly主题</a></div></div></div></div></div></footer></div><span id="fps"><span class="fps-num">FPS：0</span> <span class="fps-des" style="color:#12948b">帧率加载中...😊</span></span><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open rightside-icon"></i><span class="rightside-text">阅读模式</span></button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg class="rightside-svg" width="20" height="20" viewBox="0 0 1024 1024"><use id="modeiconT" xlink:href="#icon-moon"></use></svg><span class="rightside-text">亮暗切换</span></a><button class="share" type="button" title="右键模式" onclick="changeMouseMode()"><i class="fas fa-mouse rightside-icon"></i><span class="rightside-text">右键模式</span></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog right_side rightside-icon"></i><span class="rightside-text">更多设置</span></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul rightside-icon"></i><span class="rightside-text">显示目录</span></button><button class="share" type="button" title="分享链接" onclick="share()"><i class="fas fa-share-nodes rightside-icon"></i><span class="rightside-text">分享链接</span></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments rightside-icon"></i><span class="rightside-text">直达评论</span></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up rightside-icon"></i><span id="percent">0<span>%</span></span><span class="rightside-text">回到顶部</span></button><button id="go-down" type="button" title="直达底部"><i class="fas fa-arrow-down rightside-icon"></i><span class="rightside-text">直达底部</span></button></div></div><div><script src="/js/utils.js?v=1.7.59"></script><script src="/js/main.js?v=1.7.59"></script><script defer src="https://cdn.aimiliy.top/npm/toastify@1.12.0/toastify.js"></script><script defer src="/js/aimiliy.js?v=1.7.59"></script><script defer src="https://cdn.aimiliy.top/npm/swiper@11.2.6-c/swiper-bundle.min.js"></script><script defer src="https://cdn.aimiliy.top/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script><script defer src="https://cdn.aimiliy.top/npm/masonry-layout@4/masonry.pkgd.min.js"></script><script defer src="https://cdn.aimiliy.top/npm/marked/marked.min.js"></script><script defer src="https://cdn.aimiliy.top/npm/js/countup.js"></script><script defer src="/js/module.js?v=1.7.59"></script><script async src="https://cdn.aimiliy.top/npm/js/clipboard.min.js"></script><script defer src="https://cdn.aimiliy.top/npm/hexo-butterfly-tag-plugins-plus/carousel-touch.min.js"></script><script async>(()=>{const e=document.body,d=document.getElementById("loading-box"),o=()=>{e.style.overflow="",d.classList.add("loaded")},l=()=>{e.style.overflow="hidden",d.classList.remove("loaded")};l();let t=!1;window.addEventListener("load",(()=>{t||(o(),t=!0)})),setTimeout((function(){t||(o(),t=!0)}),5e3),btf.addGlobalFn("pjaxSend",l,"preloader_init"),btf.addGlobalFn("pjaxComplete",o,"preloader_end")})()</script><script defer src="https://cdn.aimiliy.top/npm/fancybox@5.0.36/fancybox.umd.min.js"></script><script src="https://cdn.aimiliy.top/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.aimiliy.top/npm/vanilla-lazyload@19.1.3/lazyload.iife.min.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"none"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),o=document.createTextNode("");e.parentNode.replaceChild(o,e),a.start={node:o,delim:"",n:0},a.end={node:o,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.aimiliy.top/npm/mathjax@3.2.2/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{const t=()=>{twikoo.init(Object.assign({el:"#twikoo-wrap",envId:"https://twikoo.aimiliy.top",region:"",onCommentLoaded:function(){btf.loadLightbox(document.querySelectorAll("#twikoo .tk-content img:not(.tk-owo-emotion)"))}},null))},o=()=>{"object"!=typeof twikoo?getScript("https://cdn.aimiliy.top/npm/twikoo@1.6.44/twikoo.min.js").then(t):setTimeout(t,0)};btf.loadComment(document.getElementById("twikoo-wrap"),o)})()</script></div><script async src="https://cdn.aimiliy.top/npm/js/font_958693_aj9baopik47.js"></script><script async src="https://cdn.aimiliy.top/npm/js/font_4662577_e3b7w37hk6i.js"></script><canvas id="universe"></canvas><canvas id="snow"></canvas><canvas id="visualizer" height="500" width="1500"></canvas><script defer src="https://cdn.aimiliy.top/npm/aplayer@1.10.1/APlayer.min.js"></script><script defer src="https://cdn.aimiliy.top/npm/MetingJS@2.0.1/Meting.min.js"></script><script src="https://cdn.aimiliy.top/npm/pjax@0.2.8/pjax.min.js"></script><script>(()=>{window.pjax=new Pjax({elements:'a:not([target="_blank"])',selectors:["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#an_music_bg",".js-pjax"],cacheBust:!1,analytics:!1,scrollRestoration:!1});const e=e=>{e&&Object.values(e).forEach((e=>e()))};document.addEventListener("pjax:send",(function(){btf.removeGlobalFnEvent("pjaxSendOnce"),btf.removeGlobalFnEvent("themeChange");const t=document.body.classList;t.contains("read-mode")&&t.remove("read-mode"),e(window.globalFn.pjaxSend)})),document.addEventListener("pjax:complete",(function(){btf.removeGlobalFnEvent("pjaxCompleteOnce"),document.querySelectorAll("script[data-pjax]").forEach((e=>{const t=document.createElement("script"),n=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach((e=>t.setAttribute(e.name,e.value))),t.appendChild(document.createTextNode(n)),e.parentNode.replaceChild(t,e)})),(()=>{const e=document.getElementById("page-type");document.body.dataset.type=e.value})(),e(window.globalFn.pjaxComplete)})),document.addEventListener("pjax:error",(e=>{if(404===e.request.status){!0?pjax.loadUrl("/404/"):window.location.href="/404/"}}))})()</script><script async data-pjax src="//cdn.aimiliy.top/npm/busuanzi@2.8.8/busuanzi.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.aimiliy.top/npm/js/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.aimiliy.top/npm/js/instantsearch.production.min.js"></script><script src="/js/search/algolia.js?v=1.7.59"></script></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fas fa-arrow-left right-icon"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fas fa-arrow-right right-icon"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fas fa-refresh right-icon"></i></a><a class="rightMenu-item" href="javascript:btf.rmf.scrollToTop();"><i class="fas fa-arrow-up right-icon"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:btf.rmf.copySelect();"><i class="fas fa-copy right-icon"></i><span class="right-text">复制</span></a><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();"><i class="fas fa-search right-icon"></i><span class="right-text">百度搜索</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-baiduSearch"><a class="rightMenu-item" href="javascript:window.open(window.getSelection().toString());window.location.reload();"><i class="fas fa-link right-icon"></i><span class="right-text">转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:btf.rmf.paste()"><i class="fas fa-copy right-icon"></i><span class="right-text">粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-post"><a class="rightMenu-item" href="#post-comment"><i class="fas fa-comment right-icon"></i><span class="right-text">空降评论</span></a><a class="rightMenu-item" href="javascript:btf.rmf.copyWordsLink()"><i class="fas fa-link right-icon"></i><span class="right-text">复制本文地址</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-to"><a class="rightMenu-item" href="javascript:btf.rmf.openWithNewTab()"><i class="fas fa-window-restore right-icon"></i><span class="right-text">新窗口打开</span></a><a class="rightMenu-item" id="menu-newOpen" href="javascript:btf.rmf.open()"><i class="fas fa-link right-icon"></i><span class="right-text">转到链接</span></a><a class="rightMenu-item" href="javascript:btf.rmf.copyLink()"><i class="fas fa-copy right-icon"></i><span class="right-text">复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:btf.rmf.saveAs()"><i class="fas fa-download right-icon"></i><span class="right-text">保存图片</span></a><a class="rightMenu-item" href="javascript:btf.rmf.openWithNewTab()"><i class="fas fa-window-restore right-icon"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:btf.rmf.copyLink()"><i class="fas fa-copy right-icon"></i><span class="right-text">复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:randomPost()"><i class="fas fa-paper-plane right-icon"></i><span class="right-text">随便逛逛</span></a><a class="rightMenu-item" href="javascript:switchNightMode();"><i class="fas fa-moon right-icon"></i><span class="right-text">昼夜切换</span></a><a class="rightMenu-item" href="/personal/about/"><i class="fas fa-info-circle right-icon"></i><span class="right-text">关于博客</span></a><a class="rightMenu-item" href="javascript:toggleWinbox();"><i class="fas fa-cog right-icon"></i><span class="right-text">美化设置</span></a><a class="rightMenu-item" href="javascript:btf.rmf.fullScreen();"><i class="fas fa-expand right-icon"></i><span class="right-text">切换全屏</span></a><a class="rightMenu-item" href="javascript:window.print();"><i class="fas fa-solid fa-print right-icon"></i><span class="right-text">打印页面</span></a></div></div><div id="icon-container"></div></div></body></html>