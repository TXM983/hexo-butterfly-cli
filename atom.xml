<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>MuXiaoChen🍊</title>
  
  
  <link href="https://www.aimiliy.top/atom.xml" rel="self"/>
  
  <link href="https://www.aimiliy.top/"/>
  <updated>2024-10-12T12:05:21.838Z</updated>
  <id>https://www.aimiliy.top/</id>
  
  <author>
    <name>MuXiaoChen🍊</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>自建MetingJs后端API</title>
    <link href="https://www.aimiliy.top/posts/ecd77df4.html"/>
    <id>https://www.aimiliy.top/posts/ecd77df4.html</id>
    <published>2024-10-08T06:48:24.000Z</published>
    <updated>2024-10-12T12:05:21.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>国庆假期刚结束，终于有时间更新一下了。最近突然发现MetingJs音乐播放器插件不能获取网易云的音乐，F12一打开就满屏红字报错。抱着试一试的心态，我在本地用 <code>ping</code> 测了一下获取音乐信息的 API，果不其然，请求超时了。<br><img src="https://cdn.aimiliy.top/backImg/img.webp" alt="img.png"><br>看来是没办法了，只能去 GitHub 上翻一下 MetingJs 的源码。仔细检查后发现，这个音乐播放器插件其实是前端 Node 组件和后端 API 组成的。既然有 API，那就意味着可以自己搭建后端服务。问题是，原作者的后端 API 是用 PHP 写的，对我这个没学过 PHP 的人来说有点麻烦啊。</p><p>不过幸好，有大佬用 JavaScript 重写了一版后端，项目地址：<a href="https://github.com/xizeyoupan/Meting-API">https://github.com/xizeyoupan/Meting-API</a>。这下不用折腾 PHP，终于可以放心大胆地用 JS 来搞定。</p><h2 id="折腾"><a href="#折腾" class="headerlink" title="折腾"></a>折腾</h2><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>这个项目需要 Node.js 版本 18 以上，代码下载好后，有两种方式来启动项目：</p><p>1）<strong>第一种方式</strong>：把代码下载到服务器，然后直接用 <code>npm run start:node</code> 启动。默认的端口号在 <code>../src/config.js</code> 文件里可以自行修改。</p><p>2）<strong>第二种方式</strong>：使用 Deno（Deno 是 JavaScript 和 TypeScript 的运行时环境，感兴趣的朋友可以自行了解）。在 Linux 上安装 Deno 很简单，运行以下命令即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://deno.land/x/install/install.sh | sh</span><br></pre></td></tr></table></figure><p>安装好之后，可以查看 Deno 版本：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deno -v</span><br></pre></td></tr></table></figure><br>查看使用方法<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deno --help</span><br></pre></td></tr></table></figure><br>Deno安装位置：/root/.deno<br><img src="https://cdn.aimiliy.top/backImg/img_1.webp" alt="img_1.png"><br>安装完 Deno 后，咱们需要把原始项目打包，使用以下命令打包：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run build:all</span><br></pre></td></tr></table></figure><br>打包完成后，会生成一个 deno.js 文件，路径是 ../src/dist/deno.js，接着运行以下命令启动：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run start:deno</span><br></pre></td></tr></table></figure><br>个人比较推荐第二种方式，易于管理，另外这些命令可以在 package.json 里自行调整。</p><h3 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h3><p>在用 Nginx 代理这个服务的时候，需要设置 X-Forwarded-Host 请求头。注意在 X-Forwarded-Host 后面加上 location 的转发路由，如下示例：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">   listen       8099;</span><br><span class="line">   server_name  localhost;</span><br><span class="line"></span><br><span class="line">   location /music/ &#123;</span><br><span class="line">      proxy_pass http://localhost:3010/;</span><br><span class="line">      proxy_set_header X-Forwarded-Host $scheme://$host:$server_port/music;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>如果你用的是 HTTPS 协议，可以把 X-Forwarded-Host 里的 $server_port 省略掉：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">   listen       443 ssl;</span><br><span class="line">   server_name  xxx.com;</span><br><span class="line">   ......此处省略</span><br><span class="line"></span><br><span class="line">   location /music/ &#123;</span><br><span class="line">      proxy_pass http://localhost:3010/;</span><br><span class="line">      proxy_set_header X-Forwarded-Host $scheme://$host/music;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>当项目启动成功、Nginx 代理配置好后，访问你配置的域名或者链接地址，应该能看到如下页面：<br><img src="https://cdn.aimiliy.top/backImg/img-1.webp" alt="img-1.png"><br>访问测试地址，如出现以下页面并且歌曲加载正常，就说明部署成功了。<br><img src="https://cdn.aimiliy.top/backImg/img_3.webp" alt="img_3.png"></p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>最后，在你需要展示音乐播放器的地方，插入以下代码：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">    <span class="keyword">var</span> meting_api=<span class="string">&#x27;&#123;Nginx代理链接或者域名&#125;/api?server=:server&amp;type=:type&amp;id=:id&amp;auth=:auth&amp;r=:r&#x27;</span>;</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure><br><img src="https://cdn.aimiliy.top/backImg/img_5.webp" alt="img_5.png"><br>注意：原始项目作者同时更换了Meting.min.js插件，本次教程仅仅替换了Meting.min.js调用的后端API，正常来说无需替换插件，请根据具体情况调整。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>总的来说，整个过程不算复杂，但一些细节还是要留心处理。好了，折腾到这儿，咱们下次见！<br><img src="https://cdn.aimiliy.top/GenshinImpactExpression/exp1/35.png" alt="35.png"></p>]]></content>
    
    
    <summary type="html">本文主要介绍MetingJs如何自建后端API</summary>
    
    
    
    <category term="技术" scheme="https://www.aimiliy.top/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="MetingJs" scheme="https://www.aimiliy.top/tags/MetingJs/"/>
    
  </entry>
  
  <entry>
    <title>Alibaba Cloud Linux中CronTab的用法详解</title>
    <link href="https://www.aimiliy.top/posts/2cba4350.html"/>
    <id>https://www.aimiliy.top/posts/2cba4350.html</id>
    <published>2024-09-24T07:35:10.000Z</published>
    <updated>2024-10-12T12:05:21.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近我发现博客上的朋友圈服务挂了，折腾了半天也没搞清楚什么问题，一直返回{“detail”: “not found”}，重启服务器上的Friend-circle服务也没用，还是老样子。日志里找不到任何错误信息，于是我决定换个其他的Friend-circle服务。最终我在GitHub上找到了LiuShen作者的Friend-Circle-Lite项目。这款项目相较于我之前用的，轻量化程度高，占用资源少，部署也非常简单。不过这篇文章主要想介绍的是这个项目中用到的CronTab工具，其他部署的内容就不展开了，有兴趣的话可以去GitHub了解更多，链接在这：<a href="https://github.com/willow-god/Friend-Circle-Lite">https://github.com/willow-god/Friend-Circle-Lite</a></p><h2 id="1、CronTab"><a href="#1、CronTab" class="headerlink" title="1、CronTab"></a>1、CronTab</h2><p>在原始项目中，作者使用的是宝塔面板，可以手动添加任务，但我这次在阿里云的ECS上，只能用命令行来操作。</p><h3 id="1-1、服务基本用法"><a href="#1-1、服务基本用法" class="headerlink" title="1.1、服务基本用法"></a>1.1、服务基本用法</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">service crond start    //启动服务</span><br><span class="line">service crond stop     //关闭服务</span><br><span class="line">service crond restart  //重启服务</span><br><span class="line">service crond reload   //重新载入配置</span><br><span class="line">service crond status   //查看服务状态</span><br></pre></td></tr></table></figure><p><img src="https://cdn.aimiliy.top/articlePic/17271645247473.webp" alt="image1"></p><h3 id="1-2、CronTab基本语法"><a href="#1-2、CronTab基本语法" class="headerlink" title="1.2、CronTab基本语法"></a>1.2、CronTab基本语法</h3><p><strong>crontab [option] [parameter]</strong></p><p>option：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-e：编辑该用户的计时器设置；</span><br><span class="line">-l：列出该用户的计时器设置；</span><br><span class="line">-r：删除该用户的计时器设置；</span><br><span class="line">-u&lt;用户名称&gt;：指定要设定计时器的用户名称。</span><br></pre></td></tr></table></figure></p><h3 id="1-3、CornTab添加定时任务"><a href="#1-3、CornTab添加定时任务" class="headerlink" title="1.3、CornTab添加定时任务"></a>1.3、CornTab添加定时任务</h3><p>1）列出已有定时任务<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -l</span><br></pre></td></tr></table></figure><br>这个命令会列出当前用户的所有定时任务，如果没有任务就会显示为空。<br><img src="https://cdn.aimiliy.top/articlePic/17271653145076.webp" alt="image2"></p><p>2）编辑CronTab<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -e </span><br></pre></td></tr></table></figure><br>这个命令会打开当前用户的CronTab文件，我们可以在里面添加、修改或删除定时任务。定时任务的格式为：Cron表达式 + 命令/执行脚本<br><img src="https://cdn.aimiliy.top/articlePic/17271657011027.webp" alt="image3"></p><p>3）删除CronTab<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab -r</span><br></pre></td></tr></table></figure><br>使用该命令会将当前用户下的所有定时任务彻底删除，所以还请谨慎使用！</p><h3 id="1-4、CronTab的日志调试"><a href="#1-4、CronTab的日志调试" class="headerlink" title="1.4、CronTab的日志调试"></a>1.4、CronTab的日志调试</h3><p>添加定时任务后，我们可能想知道它的执行情况，这时候就需要查看CronTab的日志了。一般来说，日志的路径是：<strong>/var/log/cron</strong><br><img src="https://cdn.aimiliy.top/articlePic/17271664257385.webp" alt="image4"><br>从图中可以看到定时任务的执行时间和脚本，这样我们就能验证它是否生效。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>在Linux系统中，crontab是一个非常方便的工具，可以减少很多手动操作，提高工作效率。不过使用时也得小心，避免占用过多的服务器资源，影响其他服务的运行。</p>]]></content>
    
    
    <summary type="html">本文主要介绍在Alibaba Cloud Linux上CronTab的详细用法</summary>
    
    
    
    <category term="技术" scheme="https://www.aimiliy.top/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="Linux" scheme="https://www.aimiliy.top/tags/Linux/"/>
    
    <category term="CronTab" scheme="https://www.aimiliy.top/tags/CronTab/"/>
    
  </entry>
  
  <entry>
    <title>前端性能优化：从加载速度到用户体验</title>
    <link href="https://www.aimiliy.top/posts/f3355759.html"/>
    <id>https://www.aimiliy.top/posts/f3355759.html</id>
    <published>2024-09-19T07:04:05.000Z</published>
    <updated>2024-10-12T12:05:21.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>总体而言，前端性能优化并不是什么新鲜话题，很多老生常谈的技巧，基本上每个开发者都耳熟能详。但在细节上，每个人的项目和需求不同，优化的侧重点自然也有所不同。有人优化代码，有人盯着渲染性能，还有人专注于用户体验的流畅度。反正，最终目的就是让页面看起来更顺滑、加载更快，让用户有更好的体验。就本站而言由于静态图片资源过多，所以使用CDN加速、压缩图片以及转WebP格式成了重中之重。</p><h2 id="1-减少HTTP请求？没啥感觉，但确实重要"><a href="#1-减少HTTP请求？没啥感觉，但确实重要" class="headerlink" title="1. 减少HTTP请求？没啥感觉，但确实重要"></a>1. 减少HTTP请求？没啥感觉，但确实重要</h2><p>每个资源请求都需要时间，这道理大家都懂，但真的要优化的时候，却总是感觉不紧不慢。合并CSS、JS文件之类的操作，虽然不是什么新鲜招数，但还是得做。毕竟，少一个请求就少一次等待。还有CSS Sprites，把多个小图拼成一张大图，理论上能减少请求数。虽然对很多人来说，这操作没啥视觉冲击，但后台性能确实提高了。</p><h2 id="2-文件压缩和代码精简：细节决定成败"><a href="#2-文件压缩和代码精简：细节决定成败" class="headerlink" title="2. 文件压缩和代码精简：细节决定成败"></a>2. 文件压缩和代码精简：细节决定成败</h2><p>代码压缩和文件优化这件事，乍一看没多大作用，实则能省不少时间。想象一下，用户等着你网站加载，结果还在看空白页面，这时候你可能已经因为几个没压缩的文件浪费了几十毫秒。<br>工具推荐：</p><ul><li><strong>UglifyJS</strong>：用来压缩JS的。</li><li><strong>CSSNano</strong>：压缩CSS没得说。</li><li><strong>HTMLMinifier</strong>：压缩HTML文件，纯靠它来削减多余空格和注释。</li></ul><p>这些工具可能感觉没啥，但都属于细水长流型。</p><h2 id="3-CDN？早该用了吧"><a href="#3-CDN？早该用了吧" class="headerlink" title="3. CDN？早该用了吧"></a>3. CDN？早该用了吧</h2><p>CDN 的好处就是全世界的用户都能从最近的节点拿到资源，尤其对图片、JS、CSS这种静态资源来说，加载速度更快。虽然没啥酷炫的视觉效果，但确实是“润物细无声”式的提升。</p><h2 id="4-图片优化：这个也细致入微"><a href="#4-图片优化：这个也细致入微" class="headerlink" title="4. 图片优化：这个也细致入微"></a>4. 图片优化：这个也细致入微</h2><p>说到图片，大家可能会直接想到视觉效果，但在加载性能优化这方面，它们简直是双刃剑。图片太大，加载慢；太小，效果差。WebP这种格式能在保持不错清晰度的同时减小文件大小，感觉就像是升级版的JPG。而懒加载（Lazy Load）则是让图片在滚动到可视区域时才加载，看似没啥视觉冲击，但是他真的能提升页面的流畅度。</p><h2 id="5-渲染性能优化：这就像调整设计细节"><a href="#5-渲染性能优化：这就像调整设计细节" class="headerlink" title="5. 渲染性能优化：这就像调整设计细节"></a>5. 渲染性能优化：这就像调整设计细节</h2><p>渲染性能优化有点像是macOS通知栏变窄，乍一看不起眼，但能让整个体验更流畅。减少DOM操作和重绘、重排是优化关键。尤其是使用JavaScript操控DOM的时候，频繁更新可能会卡到用户怀疑人生。异步加载JS、CSS让页面渲染时不被阻塞，这点就像是你电脑开机时让不必要的程序稍后再启动一样，有效提升用户体验。</p><h2 id="6-用户交互优化：这个真的戳中痛点"><a href="#6-用户交互优化：这个真的戳中痛点" class="headerlink" title="6. 用户交互优化：这个真的戳中痛点"></a>6. 用户交互优化：这个真的戳中痛点</h2><p>点击一个按钮然后等响应，这个过程一定要流畅，否则用户的心态会被拖垮。100毫秒的延迟可能看似不多，但在实际操作中，这些微秒的延迟会累积成巨大的用户烦躁感。<br>在这点上，节流（Throttle）和防抖（Debounce）技术显得尤为重要，用来限制频繁触发的事件，比如滚动或输入事件。</p><h2 id="7-工具和监控：这些优化是不是有效，数据得说话"><a href="#7-工具和监控：这些优化是不是有效，数据得说话" class="headerlink" title="7. 工具和监控：这些优化是不是有效，数据得说话"></a>7. 工具和监控：这些优化是不是有效，数据得说话</h2><p>我们说了那么多优化手段，如何验证它们是否真的起作用呢？这里推荐几个工具：</p><ul><li><strong>Google Lighthouse</strong>：前端优化的万能工具，可以全面分析页面的各项性能指标。</li><li><strong>WebPageTest</strong>：通过模拟不同的网络环境来检测网页在不同条件下的表现，特别适合全球性的网站。</li><li><strong>Chrome DevTools</strong>：内置的性能分析工具，帮你检查页面的各种性能瓶颈。</li></ul><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>总结一下，前端性能优化就像 macOS 的更新一样，虽然你可能做了很多细节上的调整，用户未必能立刻感受到，但这些看似不起眼的改动能显著提升整体体验。就像某些更新可能没有你预期的重大改动，但细节上的优化却使得日常使用更加顺畅。前端优化的细节，就像 Markdown 预览功能，虽然看起来只是小改动，但实际使用中会显著提高你的效率。</p><p>细节决定成败，前端性能优化亦是如此。每一处小改进都在不断提升用户体验，让整体表现更上一层楼。</p>]]></content>
    
    
    <summary type="html">本文主要介绍有关前端性能优化的方法和技巧</summary>
    
    
    
    <category term="技术" scheme="https://www.aimiliy.top/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="CSS" scheme="https://www.aimiliy.top/tags/CSS/"/>
    
    <category term="JS" scheme="https://www.aimiliy.top/tags/JS/"/>
    
    <category term="HTML" scheme="https://www.aimiliy.top/tags/HTML/"/>
    
  </entry>
  
  <entry>
    <title>中秋月圆，情怀渐远</title>
    <link href="https://www.aimiliy.top/posts/8f8dd2c9.html"/>
    <id>https://www.aimiliy.top/posts/8f8dd2c9.html</id>
    <published>2024-09-11T08:25:24.000Z</published>
    <updated>2024-10-12T12:05:21.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="中秋"><a href="#中秋" class="headerlink" title="中秋"></a>中秋</h2><p>清风拂桂影，明月照人归。</p><p>中秋将至，日历的翻页提醒我，仅剩五日便是月圆之时。曾几何时，中秋节总是让人满怀期待。那时，家家户户提前准备月饼，灯笼挂满屋檐，举家盼望团聚的时刻。月圆之夜，远处灯火依稀，近处的笑声，仿佛能溶解一年的疲惫。那时的中秋，不仅是节日，更是亲情的纽带。</p><p>而今，随着岁月流逝，节日的意义似乎在渐渐被时间冲淡。生活的节奏愈发快了，节日不再是从容等待，而成了忙碌间的匆匆插曲。三天的假期看似宽裕，实则不过是通过调休从其他时间“挤”出来的片刻喘息。再看这调休的安排，本周六依然得去补班，节日的本真似乎早已被这机械般的调度吞噬了。</p><p>记得小时候，父亲总会带着月饼回家，虽然简陋，但那份团聚的喜悦却浓得化不开。而如今，或许更多的人忙于工作，疏于陪伴，甚至连中秋月饼都成了速购速食，少了过去那份细品慢尝的仪式感。灯火阑珊，月光皎洁，却似乎少了当年的温暖。</p><p>时代的脚步飞速向前，而我们却慢慢被裹挟在这洪流中，节日的意义也随之变得模糊。中秋节，渐渐不再是团聚的象征，而是一个打卡节点。忙碌的都市生活让我们错过了那些本该温存的瞬间，节日成了一种形式，一种被不断稀释的传统。</p><p>我常想，若有一天，当我们再回望这些年岁，是否还会记得那些月下的欢笑？亦或是，这些片刻的团聚，终究只是岁月长河中的细小浪花，终将被遗忘在时光的洪流中？或许，真正值得铭记的，并不是节日本身，而是我们曾在其中感受到的那一丝温情。</p><p>愿未来的我们，能够停下脚步，不是为了假期，而是为了让心灵找到那份久违的宁静与团圆。<br><img src="https://cdn.aimiliy.top/backImg/3f30472a880511ebb6edd017c2d2eca2.webp" alt="3f30472a880511ebb6edd017c2d2eca2.webp"></p>]]></content>
    
    
    <summary type="html">中秋佳节，昔日的团圆情怀渐行渐远。</summary>
    
    
    
    <category term="生活" scheme="https://www.aimiliy.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
    <category term="中秋节" scheme="https://www.aimiliy.top/tags/%E4%B8%AD%E7%A7%8B%E8%8A%82/"/>
    
  </entry>
  
  <entry>
    <title>二次元随机壁纸API</title>
    <link href="https://www.aimiliy.top/posts/f8b6e52d.html"/>
    <id>https://www.aimiliy.top/posts/f8b6e52d.html</id>
    <published>2024-09-10T03:14:05.000Z</published>
    <updated>2024-09-11T10:39:52.933Z</updated>
    
    <content type="html"><![CDATA[<div class="note warning flat"><p>本文最后更新于2024-09-11，若资源失效，请留言反馈。所有壁纸API资源来源于网络，若不慎影响到您的利益，请联系我删除。</p></div><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近折腾网站的时候，发现之前使用的随机壁纸API大部分都挂掉了，于是又来水一期，简单的记录汇总了一下。失效的API我会给他划掉，如有忘记划掉的还请评论说明。</p><ul><li><strong>零七生活 API</strong> <a href="https://api.oick.cn/random/api.php">https://api.oick.cn/random/api.php</a></li><li><strong>如诗 API</strong> <a href="https://api.likepoems.com/img/pc">https://api.likepoems.com/img/pc</a></li><li><strong>赫萝随机图片 API</strong> <a href="https://api.horosama.com/random.php">https://api.horosama.com/random.php</a></li><li><strong>LoliAPI</strong> <a href="https://www.loliapi.com/bg/">https://www.loliapi.com/bg/</a></li><li><strong>PAULZZH 东方Project随机图片API</strong> <a href="https://img.paulzzh.com/touhou/random">https://img.paulzzh.com/touhou/random</a></li><li><strong>必应历史壁纸API</strong> <a href="https://bing.img.run/rand.php">https://bing.img.run/rand.php</a></li><li><strong>必应每日壁纸API</strong> <a href="https://bing.img.run/1920x1080.php">https://bing.img.run/1920x1080.php</a></li><li><del><strong>韩小韩API</strong> <a href="https://api.vvhan.com/api/acgimg">https://api.vvhan.com/api/acgimg</a></del></li><li><strong>樱花API</strong> <a href="http://www.dmoe.cc/random.php">http://www.dmoe.cc/random.php</a></li><li><strong>岁月小筑API</strong> <a href="https://img.xjh.me/random_img.php">https://img.xjh.me/random_img.php</a></li><li><del><strong>Lucky小站API</strong> <a href="https://www.rrll.cc/tuceng/ecy.php">https://www.rrll.cc/tuceng/ecy.php</a></del></li><li><del><strong>三秋API</strong> <a href="https://api.ghser.com/random/api.php">https://api.ghser.com/random/api.php</a></del></li><li><del><strong>汐岑API</strong> <a href="https://acg.yanwz.cn/wallpaper/api.php">https://acg.yanwz.cn/wallpaper/api.php</a></del></li><li><strong>呓喵酱API</strong> <a href="https://api.yimian.xyz/img">https://api.yimian.xyz/img</a></li><li><del><strong>晓晴API</strong> <a href="https://acg.toubiec.cn/random.php">https://acg.toubiec.cn/random.php</a></del></li><li><del><strong>BlogBIG API</strong> <a href="https://api.blogbig.cn/random/api.php">https://api.blogbig.cn/random/api.php</a></del></li><li><strong>樱道API</strong> <a href="https://api.r10086.com/樱道随机图片api接口.php?图片系列=动漫综合1">https://api.r10086.com/樱道随机图片api接口.php?图片系列=动漫综合1</a></li><li><del><strong>小歪API</strong> <a href="https://api.ixiaowai.cn/api/api.php">https://api.ixiaowai.cn/api/api.php</a></del></li><li><del><strong>MC酱API</strong> <a href="https://api.ixiaowai.cn/mcapi/mcapi.php">https://api.ixiaowai.cn/mcapi/mcapi.php</a></del></li><li><del><strong>保罗API</strong> <a href="https://api.paugram.com/wallpaper/">https://api.paugram.com/wallpaper/</a></del></li><li><strong>墨天逸API</strong> <a href="https://api.mtyqx.cn/tapi/random.php">https://api.mtyqx.cn/tapi/random.php</a></li><li><strong>EEE.DOG API</strong> <a href="https://api.yimian.xyz/img">https://api.yimian.xyz/img</a></li><li><del><strong>动漫星空 API</strong> <a href="https://api.dongmanxingkong.com/suijitupian/acg/1080p/index.php">https://api.dongmanxingkong.com/suijitupian/acg/1080p/index.php</a></del></li><li><strong>东方Project API</strong> <a href="https://img.paulzzh.tech/touhou/random">https://img.paulzzh.tech/touhou/random</a></li><li><strong>夏沫 API</strong> <a href="https://cdn.seovx.com">https://cdn.seovx.com</a></li><li><strong>搏天 API</strong> <a href="https://api.btstu.cn/doc/sjbz.php">https://api.btstu.cn/doc/sjbz.php</a></li><li><strong>TomyJan API</strong> <a href="https://api.tomys.top/doc/acgimg.html">https://api.tomys.top/doc/acgimg.html</a></li><li><del><strong>次元 API</strong> <a href="https://t.lizi.moe/pc">https://t.lizi.moe/pc</a> - PC横屏</del></li><li><strong>次元 API</strong> <a href="https://t.mwm.moe/mp">https://t.mwm.moe/mp</a> - 移动端竖屏</li><li><strong>次元 API</strong> <a href="https://t.mwm.moe/fj">https://t.mwm.moe/fj</a> - 二次元风景</li><li><strong>98缘 API</strong> <a href="http://www.98qy.com/sjbz/api.php">http://www.98qy.com/sjbz/api.php</a></li><li><strong>超级小兔的随机图 API</strong> <a href="https://imgapi.xl0408.top/index.php">https://imgapi.xl0408.top/index.php</a></li><li><strong>绅哥哥的博客 API</strong> <a href="https://api.dujin.org/pic/yuanshen/">https://api.dujin.org/pic/yuanshen/</a></li><li><del><strong>Jitsu的随机涩图API</strong> <a href="https://img.jitsu.top/r18">https://img.jitsu.top/r18</a></del></li><li><del><strong>一只小嘟嘟的萝莉捏图接口 API</strong> <a href="http://www.yzxddd.icu/api/">http://www.yzxddd.icu/api/</a></del></li><li><strong>复苏 API</strong> <a href="http://dsyai.club/api/Dmlmg.php">http://dsyai.club/api/Dmlmg.php</a></li><li><strong>随机二次元动漫图 API</strong> <a href="https://api.lolimi.cn/API/dmt/api.php?type=image">https://api.lolimi.cn/API/dmt/api.php?type=image</a></li><li><strong>随机原神壁纸 API</strong> <a href="https://api.lolimi.cn/API/yuan/?type=image">https://api.lolimi.cn/API/yuan/?type=image</a></li><li><strong>星河随机图片 API</strong> <a href="https://api.asxe.vip/random.php">https://api.asxe.vip/random.php</a></li><li><strong>二次元风景 API</strong> <a href="https://api.asxe.vip/scenery.php">https://api.asxe.vip/scenery.php</a></li><li><strong>魅影随机图片 API</strong> <a href="https://tuapi.eees.cc/api.php?category=dongman&amp;type=302">https://tuapi.eees.cc/api.php?category=dongman&amp;type=302</a></li><li><del><strong>素颜 API</strong> <a href="https://api.suyanw.cn/apij/">https://api.suyanw.cn/apij/</a></del></li><li><del><strong>新随机二次元PC横图 API</strong> <a href="https://ybapi.cn/API/dmt.php">https://ybapi.cn/API/dmt.php</a></del></li></ul>]]></content>
    
    
    <summary type="html">整理一下二次元随机壁纸API</summary>
    
    
    
    <category term="资源" scheme="https://www.aimiliy.top/categories/%E8%B5%84%E6%BA%90/"/>
    
    
    <category term="二次元" scheme="https://www.aimiliy.top/tags/%E4%BA%8C%E6%AC%A1%E5%85%83/"/>
    
    <category term="API" scheme="https://www.aimiliy.top/tags/API/"/>
    
    <category term="壁纸" scheme="https://www.aimiliy.top/tags/%E5%A3%81%E7%BA%B8/"/>
    
  </entry>
  
  <entry>
    <title>公共图床</title>
    <link href="https://www.aimiliy.top/posts/52d551c5.html"/>
    <id>https://www.aimiliy.top/posts/52d551c5.html</id>
    <published>2024-09-09T03:19:47.000Z</published>
    <updated>2024-09-11T10:39:52.933Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>各位朋友们好！今天来水一期，请放心食用。今天我向各位朋友们推荐几款免费好用的图床。</p><h2 id="路过图床-https-imgse-com"><a href="#路过图床-https-imgse-com" class="headerlink" title="路过图床(https://imgse.com)"></a>路过图床(<a href="https://imgse.com">https://imgse.com</a>)</h2><p>这款图床算是老牌的选手了，它的特点就是高速稳定的图片上传和外链服务，全球CDN加速，最大单张支持 10 MB，对于一般的日常使用来说已经够用了。而且它支持批量上传，对于需要一次性处理多张图片的用户非常方便。<br>不过，这款图床有个小缺点，就是上传后，预览和编辑的操作比较繁琐，需要二次确认才能完成上传，使用上稍微有些不够流畅。而且偶尔会出现图片访问速度慢的情况，特别是在高峰期时。</p><p><img src="https://cdn.aimiliy.top/backImg/17258527798373.webp" alt="17258527798373.webp"></p><h2 id="SKY-CHART-BED-https-tuchuang-voooe-cn"><a href="#SKY-CHART-BED-https-tuchuang-voooe-cn" class="headerlink" title="SKY CHART BED(https://tuchuang.voooe.cn)"></a>SKY CHART BED(<a href="https://tuchuang.voooe.cn">https://tuchuang.voooe.cn</a>)</h2><p>这是我目前正在使用的图床服务，最大的特点就是免费且稳定。上传单张图片最大支持 25 MB，对于大部分用户来说还是非常友好。上传后可以直接获取图片的原始链接以及嵌入代码，无需各种繁琐的操作。<br><img src="https://cdn.aimiliy.top/backImg/17258534221046.webp" alt="17258534221046.webp"></p><h2 id="SM-MS-sm-ms"><a href="#SM-MS-sm-ms" class="headerlink" title="SM.MS(sm.ms)"></a>SM.MS(<a href="https://sm.ms">sm.ms</a>)</h2><p>SM.MS使用时需要注册，不过简单注册即可使用，可以批量上传 10 张图片，单个文件最大5M，支持主流图片格式，包括gif。不过，目前国内可能有访问限制。同时该图床也是Twikoo评论配置图片上传功能的唯一的免费图床（其他两个配置的图床都是收费的）<br><img src="https://cdn.aimiliy.top/backImg/17258538789123.webp" alt="17258538789123.webp"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>免费图床虽好，但也不要过度依赖。万一厂家跑路，那岂不是哭都没得哭了。所以各位朋友在使用的同时还是即使做好备份，避免出现最坏的情况嘛！</p>]]></content>
    
    
    <summary type="html">分享几个免费实用的公共图床</summary>
    
    
    
    <category term="资源" scheme="https://www.aimiliy.top/categories/%E8%B5%84%E6%BA%90/"/>
    
    
    <category term="图床" scheme="https://www.aimiliy.top/tags/%E5%9B%BE%E5%BA%8A/"/>
    
  </entry>
  
  <entry>
    <title>拒绝精神内耗，从你我做起</title>
    <link href="https://www.aimiliy.top/posts/15def915.html"/>
    <id>https://www.aimiliy.top/posts/15def915.html</id>
    <published>2024-09-05T10:11:55.000Z</published>
    <updated>2024-09-11T10:39:52.933Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在如今这个信息爆炸、节奏飞快的时代，精神内耗似乎成了我们生活的一部分。我们常常陷入无尽的自我怀疑、焦虑和纠结中，不断为未来的不确定性担忧，为过去的错误懊悔，甚至为无关紧要的事情反复思考。这种状态不仅消耗我们的精力，也让我们在面对现实问题时，难以集中注意力，失去了行动的动力。</p><h2 id="所谓“精神内耗”"><a href="#所谓“精神内耗”" class="headerlink" title="所谓“精神内耗”"></a>所谓“精神内耗”</h2><p>精神内耗可以理解为一种自我消耗的心理状态。当你反复思考同一个问题，却无法找到明确的解决方案时，或者对自己过度苛责，担心未来的不可控因素，这时你就陷入了精神内耗。它是一种无形的情绪能量消耗，让我们在情绪上负重前行，像是被绑住了手脚，前进的每一步都变得异常困难。</p><h2 id="常见表现"><a href="#常见表现" class="headerlink" title="常见表现"></a>常见表现</h2><h3 id="1-自我怀疑"><a href="#1-自我怀疑" class="headerlink" title="1.自我怀疑"></a>1.自我怀疑</h3><p>我们常常怀疑自己的能力和决定，担心别人对自己的看法，总觉得自己不够好，不足以胜任工作或者面对生活中的挑战。</p><h3 id="2-情绪波动大"><a href="#2-情绪波动大" class="headerlink" title="2.情绪波动大"></a>2.情绪波动大</h3><p>通常我们的情绪也容易受到外界因素的影响，别人的一句话或者一个表情，都会在心中引发连锁反应，久久无法平息。</p><h3 id="3-拖延症"><a href="#3-拖延症" class="headerlink" title="3.拖延症"></a>3.拖延症</h3><p>我们有时候也会因为害怕做错或者担心结果不好，所以迟迟不敢行动。反而不断地思索各种可能性，最后却一事无成。</p><h2 id="如何拒绝？"><a href="#如何拒绝？" class="headerlink" title="如何拒绝？"></a>如何拒绝？</h2><h3 id="1-接受不完美的自己"><a href="#1-接受不完美的自己" class="headerlink" title="1.接受不完美的自己"></a>1.接受不完美的自己</h3><p>生命本就充满了不确定性，谁也不能保证自己永远不犯错。与其过度追求完美，不如接受自己有缺点的一面，允许自己偶尔犯错。在错误中总结归纳经验，从而避免下一次的错误。</p><h3 id="2-明确目标，行动为先"><a href="#2-明确目标，行动为先" class="headerlink" title="2.明确目标，行动为先"></a>2.明确目标，行动为先</h3><p>精神内耗的产生，很多时候是因为我们陷入了无尽的思考而没有付诸行动。给自己设立一个明确的目标，然后一步步去执行。只有真正开始付诸行动的同时，我们才能明白踏出第一步的重要性。</p><h3 id="3-学会管理情绪"><a href="#3-学会管理情绪" class="headerlink" title="3.学会管理情绪"></a>3.学会管理情绪</h3><p>情绪管理是拒绝精神内耗的重要一步。不要去在意别人的眼光或看法，不要让自己的情绪受到别人的影响，要学会管理自己的情绪。情绪是我们的内在反应，我们可以通过理性思考来缓解情绪的负面影响。</p><h3 id="4-建立良好的生活习惯"><a href="#4-建立良好的生活习惯" class="headerlink" title="4.建立良好的生活习惯"></a>4.建立良好的生活习惯</h3><p>健康的生活习惯，如充足的睡眠、规律的锻炼和健康的饮食，能有效提高我们的心理状态和身体健康，减少因身体疲惫而带来的情绪波动。</p><h3 id="5-减少无意义的比较"><a href="#5-减少无意义的比较" class="headerlink" title="5.减少无意义的比较"></a>5.减少无意义的比较</h3><p>每个人都有自己独特的节奏和人生轨迹，不需要通过别人的成就来定义自己的人生。过多的比较只会让我们迷失方向，忘记了自己真正想要的是什么。我们需要的是脚踏实地，把握未来。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>精神内耗的陷阱往往是无形的，但却可以通过自我觉察和行动来摆脱。生活不应被内心的焦虑所主导，而应该充满热情与活力。拒绝精神内耗，从你我做起，关注当下的行动，拥抱自己的不完美，才能过上更加轻松自如的生活。希望从今天开始，我们都能逐渐放下无谓的担忧，过上更加真实、健康的生活。</p>]]></content>
    
    
    <summary type="html">拒绝精神内耗，从你我做起</summary>
    
    
    
    <category term="生活" scheme="https://www.aimiliy.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
    <category term="自律" scheme="https://www.aimiliy.top/tags/%E8%87%AA%E5%BE%8B/"/>
    
    <category term="精神内耗" scheme="https://www.aimiliy.top/tags/%E7%B2%BE%E7%A5%9E%E5%86%85%E8%80%97/"/>
    
  </entry>
  
  <entry>
    <title>使用NatApp实现内网穿透</title>
    <link href="https://www.aimiliy.top/posts/be304d8c.html"/>
    <id>https://www.aimiliy.top/posts/be304d8c.html</id>
    <published>2024-08-30T09:44:09.000Z</published>
    <updated>2024-10-12T12:05:21.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近发现了一个比较有意思的工具-NatApp，使用该工具可以实现本地内网穿透，可以将本地的应用部署网络上提供别人访问（无需局域网）</p><h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><p>NatApp的网址：<a href="https://natapp.cn/">https://natapp.cn/</a><br>该工具提供了付费版本和免费版本，免费版本实现内网穿透后应用访问速度较慢，内网穿透后的域名是不定时强制更换且不支持Https，带宽也比较低。付费版本则没有这么多限制，本次教程我们使用的是免费版本。<br><img src="https://cdn.aimiliy.top/backImg/img.webp" alt="img.png"></p><h3 id="申请一个免费隧道"><a href="#申请一个免费隧道" class="headerlink" title="申请一个免费隧道"></a>申请一个免费隧道</h3><p>在购买隧道中，我们选择免费隧道（每个人最多只能同时拥有两条不同协议的隧道），隧道协议注意选择Web协议（应用于http(s)协议），本地端口选择你本地应用的端口，然后购买确认即可。<br><img src="https://cdn.aimiliy.top/backImg/img_1.webp" alt="img_1.png"></p><h3 id="下载Window客户端"><a href="#下载Window客户端" class="headerlink" title="下载Window客户端"></a>下载Window客户端</h3><p>配置好隧道之后，我们需要下载一个Natapp的Window客户端。该客户端是免安装版本，下载后解压即可使用。<a href="https://download.natapp.cn/assets/downloads/clients/2_4_0/natapp_windows_amd64_2_4_0.zip">点击下载</a><br><img src="https://cdn.aimiliy.top/backImg/img_2.webp" alt="img_2.png"></p><h3 id="添加本地配置文件"><a href="#添加本地配置文件" class="headerlink" title="添加本地配置文件"></a>添加本地配置文件</h3><p>客户端下载完毕后，此时我们只需要完成最后一步。在解压后的Natapp.exe客户端所在的目录中添加一个名为config.ini的配置文件，内容如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将本文件放置于natapp同级目录 程序将读取 [default] 段</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在命令行参数模式如 natapp -authtoken=xxx 等相同参数将会覆盖掉此配置</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">命令行参数 -config= 可以指定任意config.ini文件</span></span><br><span class="line">[default]</span><br><span class="line">authtoken=                      #对应一条隧道的authtoken</span><br><span class="line">clienttoken=                    #对应客户端的clienttoken,将会忽略authtoken,若无请留空,</span><br><span class="line">log=none                        #log 日志文件,可指定本地文件, none=不做记录,stdout=直接屏幕输出 ,默认为none</span><br><span class="line">loglevel=ERROR                  #日志等级 DEBUG, INFO, WARNING, ERROR 默认为 DEBUG</span><br><span class="line">httpauthtoken_proxy=                     #代理设置 如 http://10.123.10.10:3128 非代理上网用户请务必留空</span><br></pre></td></tr></table></figure><br>注意：在我的隧道可以看到我们创建的个人隧道，其中展示该条隧道的authoken信息，此时我们需要将该autotoken放入上面config.ini中。config.ini中除了authtoken必填，其他参数则的任意选择。<br><img src="https://cdn.aimiliy.top/backImg/img_3.webp" alt="img_3.png"></p><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>在上述步骤完成后，我们就可以启动本地应用的内网穿透了。首先打开cmd命令行窗口，然后转到Natapp.exe文件所在目录，输入以下命令：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">natapp -authtoken=&#123;你的authtoken&#125; </span><br></pre></td></tr></table></figure><br>之后可以看到以下启动后的界面，其中包含本地服务内网穿透后的域名，此时其他人可以通过该域名访问你的应用了。<br><img src="https://cdn.aimiliy.top/backImg/img_4.webp" alt="img_4.png"></p>]]></content>
    
    
    <summary type="html">简单记录一下使用NatApp实现内网穿透的过程</summary>
    
    
    
    <category term="技术" scheme="https://www.aimiliy.top/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="NatApp" scheme="https://www.aimiliy.top/tags/NatApp/"/>
    
    <category term="内网穿透" scheme="https://www.aimiliy.top/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    
  </entry>
  
  <entry>
    <title>阿里云服务器漏洞修复</title>
    <link href="https://www.aimiliy.top/posts/699ac122.html"/>
    <id>https://www.aimiliy.top/posts/699ac122.html</id>
    <published>2024-08-26T08:14:58.000Z</published>
    <updated>2024-10-12T12:05:21.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近收到了一份来自阿里云系统的邮件，上面说我的服务器目前有几个高危漏洞需要我修复一下。我想着多一事不如少一事，为了避免今后网站出现各种各样的问题，我还是决定登录阿里云去处理一下。</p><h2 id="修复过程"><a href="#修复过程" class="headerlink" title="修复过程"></a>修复过程</h2><p>本想着阿里云应该会有那种自动修复工具的，我也可以不用去搜那么多教程了。实际上他的确可以自动修复，无需人工操作。但该功能是收费的，一个漏洞/2RMB。我那十几个漏洞，这修复一下岂不是要了我的老命。<br><img src="https://cdn.aimiliy.top/articlePic/17246610476647.webp" alt=""><br>最后还是我自己去看了一下这个漏洞的代码，大致意思就是container-selinux 2.189.0-1.al8这个软件包的版本过低了，系统命中的软件包版本是大于2.221.0-1.al8的，所以我只需要找到一个大于该版本的软件包，重新安装即可。<br>最终我也是在网上找到了大于该版本的container-selinux软件包。下载好的软件包上传服务器，随后运行以下命令即可。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm -ivh container-selinux-2.229.0-2.module_el8.10.0+3858+6ad51f9f.noarch.rpm</span><br></pre></td></tr></table></figure><br>安装完成后运行以下命令查看一下版本号，查看是否安装成功<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep container-selinux</span><br></pre></td></tr></table></figure><br>其实关于升级该漏洞，还有一个其他的方法。在打开连接服务器的客户端时，一般的服务器都会在开头显示出该服务器目前的漏洞情况以及修复命令。<br><img src="https://cdn.aimiliy.top/articlePic/17246631223547.webp" alt=""><br>我没有采用该方法的主要原因是该方法升级漏洞不稳定，有可能会导致数据丢失、运行环境丢失等其他问题。所以使用该方法修复漏洞之前还需要给服务器添加快照，以避免出现因漏洞修复导致的数据丢失问题。</p>]]></content>
    
    
    <summary type="html">记录一下服务器的漏洞修复过程</summary>
    
    
    
    <category term="技术" scheme="https://www.aimiliy.top/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="Alibaba Cloud Linux" scheme="https://www.aimiliy.top/tags/Alibaba-Cloud-Linux/"/>
    
  </entry>
  
  <entry>
    <title>黑神话悟空</title>
    <link href="https://www.aimiliy.top/posts/a484fb3.html"/>
    <id>https://www.aimiliy.top/posts/a484fb3.html</id>
    <published>2024-08-20T09:04:55.000Z</published>
    <updated>2024-09-11T10:39:52.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>今天黑神话悟空发售了，各位“天命人”加油。<br><img src="https://cdn.aimiliy.top/cosPic/Linghua11716792604527539.webp" alt=""></p>]]></content>
    
    
    <summary type="html">黑神话悟空已全球同步发售</summary>
    
    
    
    <category term="游戏" scheme="https://www.aimiliy.top/categories/%E6%B8%B8%E6%88%8F/"/>
    
    
    <category term="黑神话悟空" scheme="https://www.aimiliy.top/tags/%E9%BB%91%E7%A5%9E%E8%AF%9D%E6%82%9F%E7%A9%BA/"/>
    
    <category term="ARPG" scheme="https://www.aimiliy.top/tags/ARPG/"/>
    
  </entry>
  
  <entry>
    <title>黑神话悟空明天正式发行</title>
    <link href="https://www.aimiliy.top/posts/14084397.html"/>
    <id>https://www.aimiliy.top/posts/14084397.html</id>
    <published>2024-08-19T07:51:57.000Z</published>
    <updated>2024-08-30T17:53:50.048Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>明天就是20号了，也就是是黑神话悟空发布时候的日子，四年前的这一天也是黑神话悟空发布首支预告的日子。虽然明天就要发布了，但我的内心却非常的平淡，应为我知道“他”迟早会归来，“他”也绝不会让我们失望。<br>就让我们“天命人”一起在8月20号-“直面天命”。<br><img src="https://cdn.aimiliy.top/backImg/da079d65176b8983c90d24e762c84dbc86e283b1.webp" alt=""></p>]]></content>
    
    
    <summary type="html">2024-08-20黑神话悟空正式发行</summary>
    
    
    
    <category term="游戏" scheme="https://www.aimiliy.top/categories/%E6%B8%B8%E6%88%8F/"/>
    
    
    <category term="黑神话悟空" scheme="https://www.aimiliy.top/tags/%E9%BB%91%E7%A5%9E%E8%AF%9D%E6%82%9F%E7%A9%BA/"/>
    
    <category term="ARPG" scheme="https://www.aimiliy.top/tags/ARPG/"/>
    
  </entry>
  
  <entry>
    <title>黑神话：悟空 性能测试工具</title>
    <link href="https://www.aimiliy.top/posts/d80497d3.html"/>
    <id>https://www.aimiliy.top/posts/d80497d3.html</id>
    <published>2024-08-15T09:56:46.000Z</published>
    <updated>2024-08-30T17:53:50.048Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>就在2024年8月14号，游戏科学官方发布了一款专为黑神话悟空定制的本地性能测试工具。可以在本地上用自己的电脑泡一下，看看实际效果如何。看到网上大家都在展示自己的测试截图，在这里我也是晒下一下我的测试截图。<br>先说一下我机器的配置：<br><strong>CPU：i7-13700KF</strong><br><strong>内存：金百达白刃16gx2</strong><br><strong>显卡：七彩虹4070ti adoc</strong><br><strong>显示器：AOC AG274UXP 4k 144hz</strong><br>下面是我的跑分截图：<br><img src="https://cdn.aimiliy.top/pic/9dd35238d8cc3d6369e2370bef7b5be8.webp" alt=""><br>可以看大在4k画质拉满、光追拉满的情况下，也有接近60的帧率。基本可以流畅的游玩，后续如果在实际游戏中帧率波动较大，也可以通过降低光线追踪来提高游玩体验。不得不说游戏科学的优化工作做得非常不错。<br>希望游戏真正出来后，体验能更好！</p>]]></content>
    
    
    <summary type="html">黑神话：悟空 性能测试工具</summary>
    
    
    
    <category term="游戏" scheme="https://www.aimiliy.top/categories/%E6%B8%B8%E6%88%8F/"/>
    
    
    <category term="黑神话悟空" scheme="https://www.aimiliy.top/tags/%E9%BB%91%E7%A5%9E%E8%AF%9D%E6%82%9F%E7%A9%BA/"/>
    
    <category term="ARPG" scheme="https://www.aimiliy.top/tags/ARPG/"/>
    
  </entry>
  
  <entry>
    <title>Window下MySQL免安装版本的卸载</title>
    <link href="https://www.aimiliy.top/posts/9509fa6a.html"/>
    <id>https://www.aimiliy.top/posts/9509fa6a.html</id>
    <published>2024-08-06T02:10:01.000Z</published>
    <updated>2024-09-11T10:39:52.931Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前是因为需要在本地测试一个功能，所在我才在本机上安装了一个MySQL（免安装版本的），后面就忘记卸载了。今天我在使用SSH通道连接远程MySQL的服务器映射到本地的3306端口时，提示端口被占用，我才想起来我本机上有安装过MySQL。但是忘记咋卸载了，所以下面就来记录一下卸载的过程吧！</p><h1 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h1><h2 id="停止MySQL的服务"><a href="#停止MySQL的服务" class="headerlink" title="停止MySQL的服务"></a>停止MySQL的服务</h2><p>1、第一步以<strong>管理员</strong>的身份打开cmd窗口，然后cd到MySQL的安装目录的bin目录<br><img src="https://qiniu.aimiliy.top/globalResource/17229112788064.png" alt=""></p><p>2、运行以下命令停止MySQL服务<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net stop mysql</span><br></pre></td></tr></table></figure></p><h2 id="删除MySQL的注册表"><a href="#删除MySQL的注册表" class="headerlink" title="删除MySQL的注册表"></a>删除MySQL的注册表</h2><p>1、打开注册表：键盘安装Win+R，输入regedit<br><img src="https://qiniu.aimiliy.top/globalResource/17229115028066.png" alt=""></p><p>2、进入MySQL服务的目录下方删除<strong>EventMessageFile</strong>和<strong>TypesSupported</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">计算机\HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Services\EventLog\Application\MySQLD Service</span><br></pre></td></tr></table></figure><br><img src="https://qiniu.aimiliy.top/globalResource/17229116366024.png" alt=""></p><h2 id="移除MySQL服务"><a href="#移除MySQL服务" class="headerlink" title="移除MySQL服务"></a>移除MySQL服务</h2><p>1、第一步以<strong>管理员</strong>的身份打开cmd窗口，然后cd到MySQL的安装目录的bin目录<br><img src="https://qiniu.aimiliy.top/globalResource/17229112788064.png" alt=""></p><p>2、运行以下命令移除服务<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqld -remove mysql</span><br></pre></td></tr></table></figure><br>3、删除MySQL文件</p>]]></content>
    
    
    <summary type="html">本文主要是记录Window下MySQL免安装版本的卸载过程</summary>
    
    
    
    <category term="技术" scheme="https://www.aimiliy.top/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="MySQL" scheme="https://www.aimiliy.top/tags/MySQL/"/>
    
    <category term="Windows" scheme="https://www.aimiliy.top/tags/Windows/"/>
    
  </entry>
  
  <entry>
    <title>黑神话悟空</title>
    <link href="https://www.aimiliy.top/posts/a639fb3.html"/>
    <id>https://www.aimiliy.top/posts/a639fb3.html</id>
    <published>2024-07-31T06:54:51.000Z</published>
    <updated>2024-08-30T17:53:50.047Z</updated>
    
    <content type="html"><![CDATA[<h2 id="黑神话悟空即将发售"><a href="#黑神话悟空即将发售" class="headerlink" title="黑神话悟空即将发售"></a>黑神话悟空即将发售</h2><p>今天是2024年7月31号，距离黑神话发售仅仅只有三周了。但我想说的是，都快发售了啊😣！游科你倒是发视频啊！真是急死我了😫！<br>贴吧里面真是乌烟瘴气，啥货色都有，气死俺了。游科你一定要争点气啊！8.20一定要狠狠打一下那些臭**的脸。<br><img src="https://qiniu.aimiliy.top/globalResource/blackMythWukong.webp" alt=""></p>]]></content>
    
    
    <summary type="html">黑神话悟空即将发售</summary>
    
    
    
    <category term="游戏" scheme="https://www.aimiliy.top/categories/%E6%B8%B8%E6%88%8F/"/>
    
    
    <category term="黑神话悟空" scheme="https://www.aimiliy.top/tags/%E9%BB%91%E7%A5%9E%E8%AF%9D%E6%82%9F%E7%A9%BA/"/>
    
    <category term="ARPG" scheme="https://www.aimiliy.top/tags/ARPG/"/>
    
  </entry>
  
  <entry>
    <title>绝区零</title>
    <link href="https://www.aimiliy.top/posts/eb09f29a.html"/>
    <id>https://www.aimiliy.top/posts/eb09f29a.html</id>
    <published>2024-07-12T07:39:17.000Z</published>
    <updated>2024-09-11T10:39:52.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>昨天运气非常不错，一个双倍的648加上之前赞的80抽总共抽了一个命的艾莲加一个专武。我当时人都惊呆了，竟然一次都没有歪。主要是我玩抽卡游戏运气就没这么好过，大部分情况都是大保底。</p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>绝区零刚出的时候，的确让人眼前一亮。游戏的复古风和现代科技融合的设计感独特，再加上宣传中对战斗元素的重视，确实让人充满期待。但在实际体验中，还是感觉有些地方需要打磨和改进。</p><p>目前我的游戏等级是45级，游戏整体的美术风格和音乐相当不错，战斗节奏很快，需要把握时机。前期还好，到了后期怪物的血量和伤害就起来了，这个时候就需要一点操作和手法。所以对于大多数人来说上手还是挺容易的，多练一练基本都没问题。目前游戏主要的争议点就是那个走格子的玩法，大部分的探索委托都会在频繁的战斗中穿插几次走格子的玩法，有时候打着打着突然就给你卡住了，当时个人感觉太磨叽了，这个我希望能改一改。</p><p>另外一点让我不太满意的是圣遗物系统。原神和崩坏中用圣遗物系统就算了，绝区零还是沿用了这一套。毕竟刷圣遗物真的很耗时，体验很“坐牢”。作为一款新游戏，能不能在这方面做些创新？</p><p>总体来说，绝区零的部分玩法还不错，但很多内容让我感觉像是“换汤不换药”。深渊系统依然是数值压制、刷时间以及每个月刷新一次等等。还有一个让我很难受的一点是，主角的Random Play和六分街之间就隔着一扇门，每次进出都要加载黑屏一下，体验感太差了，希望后续能优化。</p>]]></content>
    
    
    <summary type="html">分享一下我玩绝区零的一些看法</summary>
    
    
    
    <category term="游戏" scheme="https://www.aimiliy.top/categories/%E6%B8%B8%E6%88%8F/"/>
    
    
    <category term="绝区零" scheme="https://www.aimiliy.top/tags/%E7%BB%9D%E5%8C%BA%E9%9B%B6/"/>
    
    <category term="二次元" scheme="https://www.aimiliy.top/tags/%E4%BA%8C%E6%AC%A1%E5%85%83/"/>
    
  </entry>
  
  <entry>
    <title>内存开启XMP</title>
    <link href="https://www.aimiliy.top/posts/f6122879.html"/>
    <id>https://www.aimiliy.top/posts/f6122879.html</id>
    <published>2024-07-10T07:09:35.000Z</published>
    <updated>2024-10-12T12:05:21.837Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>上周我在玩《绝区零》时，屏幕突然卡死，紧接着游戏闪退。重启后，每次尝试进入游戏都卡在登录界面，随后又闪退。无论我怎么尝试，都是这个情况。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><p>我的电脑配置是 i7-13700KF 加上七彩虹的 4070 Ti adoc，系统盘是三星的 980 Pro 1TB。由于游戏都放在 C 盘，昨天遇到《绝区零》的问题时，我还尝试了玩《LOL》。登录 WeGame 后，点击开始游戏，还没进到 LOL 主界面就卡死闪退。其他游戏也差不多是这种情况。不仅如此，其他盘上的游戏也无法正常运行。这让我怀疑是不是系统出了问题（Win11 系统，我禁用了 Windows 更新，从未更新过）。无奈之下，只能制作系统 U 盘重装系统（下载镜像时的速度真是让人无奈）。</p><p>系统重装后，重新下载游戏进行测试，结果还是卡死闪退。折腾了一下午，心态崩溃。我担心可能是显卡问题，刚买不到一年，就得拿去修，真的很不爽。</p><p>第二天，我在主板 BIOS 里关闭了 XMP，重启后游戏居然可以正常运行一段时间（虽然还是会有卡死闪退的情况，但至少能进游戏玩）。我现在使用的是美商海盗船 8G x 4 3200MHz 的内存。之前的内存是两套批次不同的，所以可能导致了不稳定（关闭 XMP 后，内存频率降到了 2133MHz）。因此，我决定换一条新内存。由于我是 D4 主板，所以我选择了金百达白刃 16G x 2 4000MHz 的内存。现在内存降价幅度很大，这个内存的价格也降了不少。</p><p>几天后，新内存到了，我立刻动手装机。断电、拔掉旧内存、插上新内存、启动电脑、进入 BIOS，一气呵成。打开 BIOS 后，我重新开启了内存 XMP，然后重启电脑。结果电脑直接黑屏了。无奈之下，只能关闭 XMP，恢复到内存的默认频率 2666MHz。虽然后续游戏测试没有大问题，但不能超频还是有点不满意。我决定手动调整一下时序和电压。因为对这方面不太懂，只能找网上教程一步一步尝试，最终找到了一个合适的平衡点。现在的参数是：17-20-20-38，G1 模式，频率 4000MHz，电压 1.43V。使用 AIDA64 测试内存后，延迟表现还不错，内存稳定性也很高，电脑测试也没发现大问题。</p><p>好了，到此为止。再见！</p>]]></content>
    
    
    <summary type="html">本文主要记录内存开启XMP时的过程</summary>
    
    
    
    <category term="技术" scheme="https://www.aimiliy.top/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="内存" scheme="https://www.aimiliy.top/tags/%E5%86%85%E5%AD%98/"/>
    
    <category term="XMP" scheme="https://www.aimiliy.top/tags/XMP/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch基础知识（二）</title>
    <link href="https://www.aimiliy.top/posts/414a7aa.html"/>
    <id>https://www.aimiliy.top/posts/414a7aa.html</id>
    <published>2024-07-04T09:29:30.000Z</published>
    <updated>2024-09-11T10:46:55.197Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Tensor"><a href="#1-Tensor" class="headerlink" title="1.Tensor"></a>1.Tensor</h1><p><strong>torch.Tensor</strong>是一种包含单一数据类型元素的多维矩阵<br>Torch定义了10种CPU tensor类型和GPU tensor类型：</p><div class="table-container"><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td>torch.float32 or torch.float</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64-bit floating point</td><td>torch.float64 or torch.double</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr><tr><td>16-bit floating point [1]</td><td>torch.float16 or torch.half</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>16-bit floating point [2]</td><td>torch.bfloat16</td><td>torch.BFloat16Tensor</td><td>torch.cuda.BFloat16Tensor</td></tr><tr><td>32-bit complex</td><td>torch.complex32 or torch.chalf</td><td></td><td></td></tr><tr><td>64-bit complex</td><td>torch.complex64 or torch.cfloat</td><td></td><td></td></tr><tr><td>128-bit complex</td><td>torch.complex128 or torch.cdouble</td><td></td><td></td></tr><tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr><tr><td>8-bit integer (signed)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer (signed)</td><td>torch.int16 or torch.short</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32-bit integer (signed)</td><td>torch.int32 or torch.int</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64-bit integer (signed)</td><td>torch.int64 or torch.long</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr><tr><td>Boolean</td><td>torch.bool</td><td>torch.BoolTensor</td><td>torch.cuda.BoolTensor</td></tr><tr><td>quantized 8-bit integer (unsigned)</td><td>torch.quint8</td><td>torch.ByteTensor</td><td>/</td></tr><tr><td>quantized 8-bit integer (signed)</td><td>torch.qint8</td><td>torch.CharTensor</td><td>/</td></tr><tr><td>quantized 32-bit integer (signed)</td><td>torch.qint32</td><td>torch.IntTensor</td><td>/</td></tr><tr><td>quantized 4-bit integer (unsigned) [3]</td><td>torch.quint4x2</td><td>torch.ByteTensor</td><td>/</td></tr></tbody></table></div><div class="note info flat"><p>创建</p></div><p>一个张量tensor可以从Python的<strong>list</strong>或序列构建：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.FloatTensor([[1, 2, 3], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line">Out[0]: </span><br><span class="line">tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br></pre></td></tr></table></figure></p><p>根据可选择的大小和数据新建一个tensor。 如果没有提供参数，将会返回一个空的零维张量。如果提供了<strong>numpy.ndarray</strong>,<strong>torch.Tensor</strong>或<strong>torch.Storage</strong>，将会返回一个有同样参数的tensor.如果提供了python序列，将会从序列的副本创建一个tensor<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">接口 一个空张量tensor可以通过规定其大小来构建</span></span><br><span class="line">class torch.Tensor</span><br><span class="line">class torch.Tensor(*sizes)</span><br><span class="line">class torch.Tensor(size)</span><br><span class="line">class torch.Tensor(sequence)</span><br><span class="line">class torch.Tensor(ndarray)</span><br><span class="line">class torch.Tensor(tensor)</span><br><span class="line">class torch.Tensor(storage)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">实例化</span></span><br><span class="line">torch.IntTensor(2, 4).zero_()</span><br></pre></td></tr></table></figure></p><p>可以用python的索引和切片来获取和修改一个张量tensor中的内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">x[1][2]</span><br><span class="line">Out[0]: tensor(6.)</span><br><span class="line"></span><br><span class="line">x[0][1] = 8</span><br><span class="line">x</span><br><span class="line">Out[1]: </span><br><span class="line">tensor([[1., 8., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br></pre></td></tr></table></figure><p>每一个张量tensor都有一个相应的<strong>torch.Storage</strong>用来保存其数据。类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算<strong>会改变tensor的函数操作会用一个下划线后缀来标示</strong>。比如，<strong>torch.FloatTensor.abs_()</strong>会在原地计算绝对值，并返回改变后的tensor，而<strong>tensor.FloatTensor.abs()</strong>将会在一个新的tensor中计算结果</p><h1 id="2-storage"><a href="#2-storage" class="headerlink" title="2.storage"></a>2.storage</h1><div class="note info flat"><p><a href="https://www.jianshu.com/p/ebd7f6395bf4">tensor的数据结构、storage()、stride()、storage_offset()</a></p></div><p>pytorch中一个tensor对象分为头信息区（Tensor）和存储区（Storage）两部分<br><img src="https://cdn.aimiliy.top/articlePic/img_2.png" alt="img.png"><br>头信息区主要保存tensor的形状（size）、步长（stride）、数据类型（type）等信息；而真正的data（数据)则以连续一维数组的形式放在存储区，由torch.Storage实例管理着<br><strong>注意：storage永远是一维数组，任何维度的tensor的实际数据都存储在一维的storage中</strong></p><div class="note info flat"><p>获取tensor的storage</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[1.0, 4.0],[2.0, 1.0],[3.0, 5.0]])</span><br><span class="line">a.storage()</span><br><span class="line">Out[0]: </span><br><span class="line"> 1.0</span><br><span class="line"> 4.0</span><br><span class="line"> 2.0</span><br><span class="line"> 1.0</span><br><span class="line"> 3.0</span><br><span class="line"> 5.0</span><br><span class="line">[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]</span><br><span class="line"></span><br><span class="line">a.storage()[2] = 9</span><br><span class="line"></span><br><span class="line">id(a.storage())</span><br><span class="line">Out[1]: 1343354913168</span><br></pre></td></tr></table></figure><h1 id="3-Pytorch加载数据"><a href="#3-Pytorch加载数据" class="headerlink" title="3.Pytorch加载数据"></a>3.Pytorch加载数据</h1><p>Pytorch中加载数据需要Dataset、Dataloader。</p><ul><li>Dataset提供一种方式去获取每个数据及其对应的label，告诉我们总共有多少个数据。</li><li>Dataloader为后面的网络提供不同的数据形式，它将一批一批数据进行一个打包。</li></ul><h1 id="4-Tensorboard"><a href="#4-Tensorboard" class="headerlink" title="4.Tensorboard"></a>4.Tensorboard</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor())               </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回</span>      </span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=64,shuffle=True,num_workers=0,drop_last=False)      </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">用<span class="keyword">for</span>循环取出DataLoader打包好的四个数据</span></span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line">step = 0</span><br><span class="line">for data in test_loader:</span><br><span class="line">    imgs, targets = data # 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成             </span><br><span class="line">    writer.add_images(&quot;test_data&quot;,imgs,step)</span><br><span class="line">    step = step + 1</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="4-transforms"><a href="#4-transforms" class="headerlink" title="4.transforms"></a>4.transforms</h1><p>① Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。<br>② Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from torchvision import transforms</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">img_path = &quot;Data/FirstTypeData/val/bees/10870992_eebeeb3a12.jpg&quot;</span><br><span class="line">img = Image.open(img_path)  </span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()  # 创建 transforms.ToTensor类 的实例化对象</span><br><span class="line">tensor_img = tensor_trans(img)  # 调用 transforms.ToTensor类 的__call__的魔术方法   </span><br><span class="line">print(tensor_img)</span><br></pre></td></tr></table></figure></p><h1 id="5-torchvision数据集"><a href="#5-torchvision数据集" class="headerlink" title="5.torchvision数据集"></a>5.torchvision数据集</h1><p>① torchvision中有很多数据集，当我们写代码时指定相应的数据集指定一些参数，它就可以自行下载。<br>② CIFAR-10数据集包含60000张32×32的彩色图片，一共10个类别，其中50000张训练图片，10000张测试图片。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=True,download=True) # root为存放数据集的相对路线</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=False,download=True) # train=True是训练集，train=False是测试集  </span><br><span class="line"></span><br><span class="line">print(test_set[0])       # 输出的3是target </span><br><span class="line">print(test_set.classes)  # 测试数据集中有多少种</span><br><span class="line"></span><br><span class="line">img, target = test_set[0] # 分别获得图片、target</span><br><span class="line">print(img)</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line">print(test_set.classes[target]) # 3号target对应的种类</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure></p><h1 id="6-损失函数"><a href="#6-损失函数" class="headerlink" title="6.损失函数"></a>6.损失函数</h1><p>① Loss损失函数一方面计算实际输出和目标之间的差距。<br>② Loss损失函数另一方面为我们更新输出提供一定的依据<br><div class="note info flat"><p>L1loss损失函数</p></div><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">inputs = torch.tensor([1,2,3],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([1,2,5],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(1,1,1,3))</span><br><span class="line">targets = torch.reshape(targets,(1,1,1,3))</span><br><span class="line">loss = L1Loss()  # 默认为 maen</span><br><span class="line">result = loss(inputs,targets)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><br><div class="note info flat"><p>MSE损失函数</p></div><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">from torch import nn</span><br><span class="line">inputs = torch.tensor([1,2,3],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([1,2,5],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(1,1,1,3))</span><br><span class="line">targets = torch.reshape(targets,(1,1,1,3))</span><br><span class="line">loss_mse = nn.MSELoss()</span><br><span class="line">result_mse = loss_mse(inputs,targets)</span><br><span class="line">print(result_mse)</span><br></pre></td></tr></table></figure><br><div class="note info flat"><p>交叉熵损失函数</p></div><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">x = torch.tensor([0.1,0.2,0.3])</span><br><span class="line">y = torch.tensor([1])</span><br><span class="line">x = torch.reshape(x,(1,3)) # 1的 batch_size，有三类</span><br><span class="line">loss_cross = nn.CrossEntropyLoss()</span><br><span class="line">result_cross = loss_cross(x,y)</span><br><span class="line">print(result_cross)</span><br></pre></td></tr></table></figure></p><h1 id="7-优化器"><a href="#7-优化器" class="headerlink" title="7.优化器"></a>7.优化器</h1><p>① 损失函数调用backward方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。<br>② 梯度要清零，如果梯度不清零会导致梯度累加<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss() # 交叉熵    </span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器</span><br><span class="line">for data in dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = tudui(imgs)</span><br><span class="line">    result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距</span><br><span class="line">    optim.zero_grad()  # 梯度清零</span><br><span class="line">    result_loss.backward() # 反向传播，计算损失函数的梯度</span><br><span class="line">    optim.step()   # 根据梯度，对网络的参数进行调优</span><br><span class="line">    print(result_loss) # 对数据只看了一遍，只看了一轮，所以loss下降不大</span><br></pre></td></tr></table></figure><br><div class="note info flat"><p>神经网络学习率优化</p></div><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn </span><br><span class="line">from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=64,drop_last=True)</span><br><span class="line"></span><br><span class="line">class Tudui(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(3,32,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32,32,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32,64,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(1024,64),</span><br><span class="line">            Linear(64,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss() # 交叉熵    </span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1) # 每过 step_size 更新一次优化器，更新是学习率为原来的学习率的 0.1 倍    </span><br><span class="line">for epoch in range(20):</span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for data in dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距</span><br><span class="line">        optim.zero_grad()  # 梯度清零</span><br><span class="line">        result_loss.backward() # 反向传播，计算损失函数的梯度</span><br><span class="line">        optim.step()   # 根据梯度，对网络的参数进行调优</span><br><span class="line">        scheduler.step() # 学习率太小了，所以20个轮次后，相当于没走多少</span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line">    print(running_loss) # 对这一轮所有误差的总和</span><br></pre></td></tr></table></figure></p><h1 id="8-网络模型使用及修改"><a href="#8-网络模型使用及修改" class="headerlink" title="8.网络模型使用及修改"></a>8.网络模型使用及修改</h1><div class="note info flat"><p>网络模型添加</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=True) # 下载卷积层对应的参数是多少、池化层对应的参数时多少，这些参数时ImageNet训练好了的</span><br><span class="line">vgg16_true.add_module(&#x27;add_linear&#x27;,nn.Linear(1000,10)) # 在VGG16后面添加一个线性层，使得输出为适应CIFAR10的输出，CIFAR10需要输出10个种类</span><br><span class="line"></span><br><span class="line">print(vgg16_true)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>网络模型修改</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=False) # 没有预训练的参数     </span><br><span class="line">print(vgg16_false)</span><br><span class="line">vgg16_false.classifier[6] = nn.Linear(4096,10)</span><br><span class="line">print(vgg16_false)</span><br></pre></td></tr></table></figure><h1 id="9-网络模型保存与读取"><a href="#9-网络模型保存与读取" class="headerlink" title="9.网络模型保存与读取"></a>9.网络模型保存与读取</h1><div class="note info flat"><p>模型结构 + 模型参数</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=False)</span><br><span class="line">torch.save(vgg16,&quot;./model/vgg16_method1.pth&quot;) # 保存方式一：模型结构 + 模型参数      </span><br><span class="line">print(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(&quot;./model/vgg16_method1.pth&quot;) # 保存方式一对应的加载模型    </span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>模型参数（官方推荐），不保存网络模型结构</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=False)</span><br><span class="line">torch.save(vgg16.state_dict(),&quot;./model/vgg16_method2.pth&quot;) # 保存方式二：模型参数（官方推荐）,不再保存网络模型结构  </span><br><span class="line">print(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(&quot;./model/vgg16_method2.pth&quot;) # 导入模型参数   </span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><h1 id="10-固定模型参数"><a href="#10-固定模型参数" class="headerlink" title="10.固定模型参数"></a>10.固定模型参数</h1><p>在训练过程中可能需要固定一部分模型的参数，只更新另一部分参数，有两种思路实现这个目标</p><ul><li>1.一个是设置不要更新参数的<strong>网络层</strong>为false</li><li>2.另一个就是在定义优化器时只传入要更新的参数<br>当然最优的做法是，优化器中只传入requires_grad=True的参数，这样占用的内存会更小一点，效率也会更高<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义一个简单的网络</span></span><br><span class="line">class net(nn.Module):</span><br><span class="line">    def __init__(self, num_class=3):</span><br><span class="line">        super(net, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(8, 4)</span><br><span class="line">        self.fc2 = nn.Linear(4, num_class)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.fc2(self.fc1(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = net()</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">冻结fc1层的参数</span></span><br><span class="line">for name, param in model.named_parameters():</span><br><span class="line">    if &quot;fc1&quot; in name:</span><br><span class="line">        param.requires_grad = False</span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">只传入requires_grad = True的参数</span></span><br><span class="line">optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters(), lr=1e-2)</span><br><span class="line">print(&quot;model.fc1.weight&quot;, model.fc1.weight)</span><br><span class="line">print(&quot;model.fc2.weight&quot;, model.fc2.weight)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line">for epoch in range(10):</span><br><span class="line">    x = torch.randn((3, 8))</span><br><span class="line">    label = torch.randint(0, 3, [3]).long()</span><br><span class="line">    output = model(x)</span><br><span class="line"></span><br><span class="line">    loss = loss_fn(output, label)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">print(&quot;model.fc1.weight&quot;, model.fc1.weight)</span><br><span class="line">print(&quot;model.fc2.weight&quot;, model.fc2.weight)</span><br></pre></td></tr></table></figure><h1 id="11-训练流程"><a href="#11-训练流程" class="headerlink" title="11.训练流程"></a>11.训练流程</h1></li></ul><div class="note info flat"><p>DataLoader加载数据集</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">length 长度</span></span><br><span class="line">train_data_size = len(train_data)</span><br><span class="line">test_data_size = len(test_data)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line">print(&quot;训练数据集的长度：&#123;&#125;&quot;.format(train_data_size))</span><br><span class="line">print(&quot;测试数据集的长度：&#123;&#125;&quot;.format(test_data_size))</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data_size, batch_size=64)        </span><br><span class="line">test_dataloader = DataLoader(test_data_size, batch_size=64)</span><br></pre></td></tr></table></figure><div class="note info flat"><p>测试网络正确</p></div><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">搭建神经网络</span></span><br><span class="line">class Tudui(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,32,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,64,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Flatten(),  # 展平后变成 64*4*4 了</span><br><span class="line">            nn.Linear(64*4*4,64),</span><br><span class="line">            nn.Linear(64,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    tudui = Tudui()</span><br><span class="line">    input = torch.ones((64,3,32,32))</span><br><span class="line">    output = tudui(input)</span><br><span class="line">    print(output.shape)  # 测试输出的尺寸是不是我们想要的</span><br></pre></td></tr></table></figure><div class="note info flat"><p>网络训练数据</p></div><p>① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。<br>② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。<br>③ 不启用 Batch Normalization 和 Dropout。 如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。<br>④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的性质。<br>⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里</span></span><br><span class="line">class Tudui(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,32,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(32,64,5,1,2),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Flatten(),  # 展平后变成 64*4*4 了</span><br><span class="line">            nn.Linear(64*4*4,64),</span><br><span class="line">            nn.Linear(64,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        return x</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">length 长度</span></span><br><span class="line">train_data_size = len(train_data)</span><br><span class="line">test_data_size = len(test_data)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line">print(&quot;训练数据集的长度：&#123;&#125;&quot;.format(train_data_size))</span><br><span class="line">print(&quot;测试数据集的长度：&#123;&#125;&quot;.format(test_data_size))</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=64)        </span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建网络模型</span></span><br><span class="line">tudui = Tudui() </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">优化器</span></span><br><span class="line">learning = 0.01  # 1e-2 就是 0.01 的意思</span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置网络的一些参数</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记录训练的次数</span></span><br><span class="line">total_train_step = 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记录测试的次数</span></span><br><span class="line">total_test_step = 0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">训练的轮次</span></span><br><span class="line">epoch = 10</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加 tensorboard</span></span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line"></span><br><span class="line">for i in range(epoch):</span><br><span class="line">    print(&quot;-----第 &#123;&#125; 轮训练开始-----&quot;.format(i+1))</span><br><span class="line"></span><br><span class="line">    # 训练步骤开始</span><br><span class="line">    tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用</span><br><span class="line">    for data in train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距</span><br><span class="line"></span><br><span class="line">        # 优化器对模型调优</span><br><span class="line">        optimizer.zero_grad()  # 梯度清零</span><br><span class="line">        loss.backward() # 反向传播，计算损失函数的梯度</span><br><span class="line">        optimizer.step()   # 根据梯度，对网络的参数进行调优</span><br><span class="line"></span><br><span class="line">        total_train_step = total_train_step + 1</span><br><span class="line">        if total_train_step % 100 == 0:</span><br><span class="line">            print(&quot;训练次数：&#123;&#125;，Loss：&#123;&#125;&quot;.format(total_train_step,loss.item()))  # 方式二：获得loss值</span><br><span class="line">            writer.add_scalar(&quot;train_loss&quot;,loss.item(),total_train_step)</span><br><span class="line"></span><br><span class="line">    # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）</span><br><span class="line">    tudui.eval()  # 当网络中有dropout层、batchnorm层时，这些层不能起作用</span><br><span class="line">    total_test_loss = 0</span><br><span class="line">    total_accuracy = 0</span><br><span class="line">    with torch.no_grad():  # 没有梯度了</span><br><span class="line">        for data in test_dataloader: # 测试数据集提取数据</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            outputs = tudui(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失</span><br><span class="line">            total_test_loss = total_test_loss + loss.item() # 所有loss</span><br><span class="line">            accuracy = (outputs.argmax(1) == targets).sum()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line"></span><br><span class="line">    print(&quot;整体测试集上的Loss：&#123;&#125;&quot;.format(total_test_loss))</span><br><span class="line">    print(&quot;整体测试集上的正确率：&#123;&#125;&quot;.format(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(&quot;test_loss&quot;,total_test_loss,total_test_step)</span><br><span class="line">    writer.add_scalar(&quot;test_accuracy&quot;,total_accuracy/test_data_size,total_test_step)  </span><br><span class="line">    total_test_step = total_test_step + 1</span><br><span class="line"></span><br><span class="line">    torch.save(tudui, &quot;./model/tudui_&#123;&#125;.pth&quot;.format(i)) # 保存每一轮训练后的结果</span><br><span class="line">    #torch.save(tudui.state_dict(),&quot;tudui_&#123;&#125;.path&quot;.format(i)) # 保存方式二         </span><br><span class="line">    print(&quot;模型已保存&quot;)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></p><h1 id="12-模型-参数查看"><a href="#12-模型-参数查看" class="headerlink" title="12.模型|参数查看"></a>12.模型|参数查看</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyModel(torch.nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.layer1 = torch.nn.Sequential(</span><br><span class="line">            torch.nn.Linear(3, 4),</span><br><span class="line">            torch.nn.Linear(4, 3),</span><br><span class="line">        )</span><br><span class="line">        self.layer2 = torch.nn.Linear(3, 6)</span><br><span class="line">        self.layer3 = torch.nn.Sequential(</span><br><span class="line">            torch.nn.Linear(6, 7),</span><br><span class="line">            torch.nn.Linear(7, 5),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = MyModel()</span><br><span class="line">print(net)</span><br><span class="line">MyModel(</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): Linear(in_features=3, out_features=4, bias=True)</span><br><span class="line">    (1): Linear(in_features=4, out_features=3, bias=True)</span><br><span class="line">  )</span><br><span class="line">  (layer2): Linear(in_features=3, out_features=6, bias=True)</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (0): Linear(in_features=6, out_features=7, bias=True)</span><br><span class="line">    (1): Linear(in_features=7, out_features=5, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>查看参数<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">for layer in net.modules():</span><br><span class="line">    print(type(layer))  # 查看每一层的类型</span><br><span class="line">    # print(layer)</span><br><span class="line">&lt;class &#x27;__main__.MyModel&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.container.Sequential&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.container.Sequential&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line">&lt;class &#x27;torch.nn.modules.linear.Linear&#x27;&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for param in net.parameters():</span><br><span class="line">    print(param.shape)  # 打印每一层的参数</span><br><span class="line">torch.Size([4, 3])</span><br><span class="line">torch.Size([4])</span><br><span class="line">torch.Size([3, 4])</span><br><span class="line">torch.Size([3])</span><br><span class="line">torch.Size([6, 3])</span><br><span class="line">torch.Size([6])</span><br><span class="line">torch.Size([7, 6])</span><br><span class="line">torch.Size([7])</span><br><span class="line">torch.Size([5, 7])</span><br><span class="line">torch.Size([5])</span><br><span class="line"></span><br><span class="line">for name, param in net.named_parameters():</span><br><span class="line">    print(name, param.shape)  # 看的更细</span><br><span class="line">layer1.0.weight torch.Size([4, 3])</span><br><span class="line">layer1.0.bias torch.Size([4])</span><br><span class="line">layer1.1.weight torch.Size([3, 4])</span><br><span class="line">layer1.1.bias torch.Size([3])</span><br><span class="line">layer2.weight torch.Size([6, 3])</span><br><span class="line">layer2.bias torch.Size([6])</span><br><span class="line">layer3.0.weight torch.Size([7, 6])</span><br><span class="line">layer3.0.bias torch.Size([7])</span><br><span class="line">layer3.1.weight torch.Size([5, 7])</span><br><span class="line">layer3.1.bias torch.Size([5])</span><br><span class="line"></span><br><span class="line">for key, value in net.state_dict().items():  # 参数名以及参数</span><br><span class="line">    print(key, value.shape)</span><br><span class="line">layer1.0.weight torch.Size([4, 3])</span><br><span class="line">layer1.0.bias torch.Size([4])</span><br><span class="line">layer1.1.weight torch.Size([3, 4])</span><br><span class="line">layer1.1.bias torch.Size([3])</span><br><span class="line">layer2.weight torch.Size([6, 3])</span><br><span class="line">layer2.bias torch.Size([6])</span><br><span class="line">layer3.0.weight torch.Size([7, 6])</span><br><span class="line">layer3.0.bias torch.Size([7])</span><br><span class="line">layer3.1.weight torch.Size([5, 7])</span><br><span class="line">layer3.1.bias torch.Size([5])</span><br></pre></td></tr></table></figure></p><h1 id="13-模型保存-加载"><a href="#13-模型保存-加载" class="headerlink" title="13.模型保存|加载"></a>13.模型保存|加载</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1、加载模型+参数</span></span><br><span class="line">net = torch.load(&quot;resnet50.pth&quot;)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2、已有模型,加载预训练参数</span></span><br><span class="line">resnet50 = models.resnet50(weights=None)  </span><br><span class="line">resnet50.load_state_dict(torch.load(&quot;resnet58_weight.pth&quot;))</span><br></pre></td></tr></table></figure><h1 id="14-网络的修改"><a href="#14-网络的修改" class="headerlink" title="14.网络的修改"></a>14.网络的修改</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">from torchvision import models</span><br><span class="line"></span><br><span class="line">alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (4): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">    (5): ReLU(inplace=True)</span><br><span class="line">    (6): Linear(in_features=4096, out_features=1000, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>修改网络结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1、-----删除网络的最后一层-----</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">del alexnet.classifier</span></span><br><span class="line">del alexnet.classifier[6]</span><br><span class="line">print(alexnet)</span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (4): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">    (5): ReLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2、-----删除网络的最后多层-----</span></span><br><span class="line">alexnet.classifier = alexnet.classifier[:-2]</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3、-----修改网络的某一层-----</span></span><br><span class="line">alexnet.classifier[6] = nn.Linear(in_features=4096, out_features=1024)</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Linear(in_features=4096, out_features=1024, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">4、-----网络添加层，每次添加一层-----</span></span><br><span class="line">alexnet.classifier.add_module(&#x27;7&#x27;, nn.ReLU(inplace=True))</span><br><span class="line">alexnet.classifier.add_module(&#x27;8&#x27;, nn.Linear(in_features=1024, out_features=20))</span><br><span class="line">print(alexnet)</span><br><span class="line"></span><br><span class="line">AlexNet(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (7): ReLU(inplace=True)</span><br><span class="line">    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (9): ReLU(inplace=True)</span><br><span class="line">    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (1): Linear(in_features=9216, out_features=4096, bias=True)</span><br><span class="line">    (2): ReLU(inplace=True)</span><br><span class="line">    (3): Linear(in_features=4096, out_features=1024, bias=True)</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): Linear(in_features=1024, out_features=20, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="15-参数冻结"><a href="#15-参数冻结" class="headerlink" title="15.参数冻结"></a>15.参数冻结</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">任务一∶</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1、将模型A作为backbone，修改为模型B</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2、模型A的预训练参数加载到模型B上</span></span><br><span class="line"></span><br><span class="line">resnet_modified = resnet50()</span><br><span class="line">new_weights_dict = resnet_modified.state_dict()</span><br><span class="line"></span><br><span class="line">resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)</span><br><span class="line">weights_dict = resnet.state_dict()</span><br><span class="line"></span><br><span class="line">for k in weights_dict.keys():</span><br><span class="line">    if k in new_weights_dict.keys() and not k.startswith(&#x27;fc&#x27;):</span><br><span class="line">        new_weights_dict[k] = weights_dict[k]</span><br><span class="line">resnet_modified.load_state_dict(new_weights_dict)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">resnet_modified.load_state_dict(new_weights_dict,strict=False)</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">任务二:</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">冻结与训练好的参数</span></span><br><span class="line">params = []</span><br><span class="line">train_layer = [&#x27;layer5&#x27;, &#x27;conv_end&#x27;, &#x27;bn_end&#x27;]</span><br><span class="line">for name, param in resnet_modified.named_parameters():</span><br><span class="line">    if any(name.startswith(prefix) for prefix in train_layer):</span><br><span class="line">        print(name)</span><br><span class="line">        params.append(param)</span><br><span class="line">    else:</span><br><span class="line">        param.requires_grad = False</span><br><span class="line">optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=5e-4)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本文主要介绍有关Pytorch的基础知识（二）</summary>
    
    
    
    <category term="教程" scheme="https://www.aimiliy.top/categories/%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Pytorch" scheme="https://www.aimiliy.top/tags/Pytorch/"/>
    
    <category term="Machine Learning" scheme="https://www.aimiliy.top/tags/Machine-Learning/"/>
    
    <category term="Deep Learning" scheme="https://www.aimiliy.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch基础知识（一）</title>
    <link href="https://www.aimiliy.top/posts/5a986f0.html"/>
    <id>https://www.aimiliy.top/posts/5a986f0.html</id>
    <published>2024-07-02T08:39:08.000Z</published>
    <updated>2024-09-11T10:39:52.931Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、pytorch环境部署"><a href="#一、pytorch环境部署" class="headerlink" title="一、pytorch环境部署"></a>一、pytorch环境部署</h1><h2 id="1-安装conda"><a href="#1-安装conda" class="headerlink" title="1. 安装conda"></a>1. 安装conda</h2><p>conda 是一个开源的软件包管理系统和环境管理软件，用于安装多个版本的软件包及其依赖关系，并在它们之间轻松切换。conda 是为Python程序创建的，类似于 Linux、MacOS、Windows，也可以打包和分发其他软件。<br><strong>注意：必须在 cmd 里面才可以，在 powershell 里面输入命令有些是无效的</strong></p><h2 id="2-创建Python环境"><a href="#2-创建Python环境" class="headerlink" title="2. 创建Python环境"></a>2. 创建Python环境</h2><p>创建环境<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n name python=3.8  ##示例，实际根据需求选择Python版本以及设置环境名称</span><br></pre></td></tr></table></figure><br>激活环境<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate name</span><br></pre></td></tr></table></figure><br>退出环境<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deavtivate</span><br></pre></td></tr></table></figure><br>查看环境下包的信息<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip list # 列出pip环境里的所有包</span><br><span class="line">conda list # 列出conda环境里的所有包</span><br></pre></td></tr></table></figure></p><h2 id="3-安装NVIDIA-CUDA以及CUDNN（若没有GPU可以跳过本步骤）"><a href="#3-安装NVIDIA-CUDA以及CUDNN（若没有GPU可以跳过本步骤）" class="headerlink" title="3. 安装NVIDIA CUDA以及CUDNN（若没有GPU可以跳过本步骤）"></a>3. 安装NVIDIA CUDA以及CUDNN（若没有GPU可以跳过本步骤）</h2><h3 id="3-1-查看本机GPU信息"><a href="#3-1-查看本机GPU信息" class="headerlink" title="3.1 查看本机GPU信息"></a>3.1 查看本机GPU信息</h3><p>在CMD控制台中输入以下命令就可以查看本机GPU型号以及其适配CUDA型号信息<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi # 查看本机GPU型号以及其适配CUDA的信息</span><br></pre></td></tr></table></figure></p><h3 id="3-2-安装CUDA"><a href="#3-2-安装CUDA" class="headerlink" title="3.2 安装CUDA"></a>3.2 安装CUDA</h3><ol><li>下载CUDA安装包，下载地址：<a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a><ul><li>选择Windows或者Linux，选择对应版本的CUDA安装包</li></ul></li><li>安装CUDA安装包<ul><li>默认安装即可，如果出现错误，可以参考：<a href="https://blog.csdn.net/weixin_53762670/article/details/131845364">https://blog.csdn.net/weixin_53762670/article/details/131845364</a></li></ul></li><li><p>验证CUDA安装是否成功</p><ul><li>打开cmd，输入以下命令<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>如果出现以下信息，则说明安装成功<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2021 NVIDIA Corporation</span><br><span class="line">Built on Wed_Jun__2_19:15:15_Pacific_Daylight_Time_2021</span><br><span class="line">Cuda compilation tools, release 11.5, V11.5.119</span><br></pre></td></tr></table></figure></li></ul></li><li><p>安装CUDNN</p><ul><li>下载CUDNN安装包，下载地址：<a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a></li><li>选择对应CUDA版本的CUDNN安装包</li><li>将下载后的压缩包解压到CUDA安装的根目录下，例如：C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.5</li></ul></li><li>验证CUDNN安装是否成功<ul><li>进入到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7\extras\demo_suite 目录下，在上方文件路径中输入CMD，<br>然后回车，进入到该目录命令窗口下，首先输入bandwidth.exe，如果出现以下信息，则说明安装成功</li><li><img src="https://cdn.aimiliy.top/articlePic/img.png" alt="img.png"></li><li>然后在输入deviceQuery.exe，如果出现以下信息，则说明安装成功</li><li><img src="https://cdn.aimiliy.top/articlePic/img_1.png" alt="img_1.png"></li></ul></li></ol><h2 id="4-安装Pytorch"><a href="#4-安装Pytorch" class="headerlink" title="4. 安装Pytorch"></a>4. 安装Pytorch</h2><h3 id="1-安装Pytorch相关包"><a href="#1-安装Pytorch相关包" class="headerlink" title="1. 安装Pytorch相关包"></a>1. 安装Pytorch相关包</h3><p>输入以下命令安装Pytorch<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia ## 注意pytorch-cuda版本替换为自己实际的CUDA版本</span><br></pre></td></tr></table></figure></p><h3 id="2-验证Pytorch安装是否成功"><a href="#2-验证Pytorch安装是否成功" class="headerlink" title="2. 验证Pytorch安装是否成功"></a>2. 验证Pytorch安装是否成功</h3><ul><li>打开cmd，输入以下命令<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda activate name # 激活对应环境</span><br><span class="line">python</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import torch</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; torch.cuda.is_available()</span></span><br></pre></td></tr></table></figure></li><li>如果出现以下信息，则说明安装成功，可以使用GPU进行训练<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import torch</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; torch.cuda.is_available()</span></span><br><span class="line">True</span><br></pre></td></tr></table></figure><h1 id="二、处理数据"><a href="#二、处理数据" class="headerlink" title="二、处理数据"></a>二、处理数据</h1><h2 id="1-数据集的加载"><a href="#1-数据集的加载" class="headerlink" title="1. 数据集的加载"></a>1. 数据集的加载</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision import transforms</span><br><span class="line">trans = transforms.Compose([</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=True,transform=trans,download=True)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=False,transform=trans,download=True)</span><br><span class="line">img,target = test_set[0]</span><br><span class="line">print(img) </span><br><span class="line">print(target)</span><br><span class="line">print(test_set.classes[target])</span><br></pre></td></tr></table></figure>Dataset介绍：<ul><li>Dataset 是一个抽象类，不能直接使用，需要继承 Dataset 类，然后重写 <strong>getitem</strong> 和 <strong>len</strong> 方法，这两个方法都是抽象方法，必须实现。</li><li><strong>getitem</strong> 方法用于获取数据集的某一个样本，返回一个样本。</li><li><strong>len</strong> 方法用于获取数据集的长度，返回一个整数。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import Dataset</span><br><span class="line">import cv2</span><br><span class="line">from PIL import Image</span><br><span class="line">import os</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">help</span>(Dataset)</span></span><br><span class="line">class mydata(Dataset):</span><br><span class="line">def __init__(self,root_dir,label_dir):</span><br><span class="line">self.root_dir = root_dir</span><br><span class="line">self.label_dir = label_dir</span><br><span class="line">self.path = os.path.join(self.root_dir,self.label_dir)</span><br><span class="line">self.img_path = os.listdir(self.path)</span><br><span class="line">   def __getitem__(self, idx):</span><br><span class="line">       img_name = self.img_path[idx]</span><br><span class="line">       img_item_path = os.path.join(self.root_dir,self.label_dir,self.img_path)</span><br><span class="line">       img = Image.open(img_item_path)</span><br><span class="line">       label = self.label_dir</span><br><span class="line">       return img,label</span><br><span class="line">   def __len__(self):</span><br><span class="line">       return len(self.img_path)</span><br><span class="line">ants_dataset = mydata(root_dir,label_dir)</span><br><span class="line">img, label = ants_dataset[0]</span><br><span class="line">img.show()</span><br><span class="line">bees_dataset = mydata(root_dir,label_dir)</span><br><span class="line">all_train = ants_dataset+bees_dataset //拼接数据集</span><br><span class="line">len(all_train)</span><br></pre></td></tr></table></figure>DataLoader介绍：</li><li>DataLoader 是一个迭代器，用于加载数据集。 可以通过 DataLoader 的参数设置，如 batch_size、shuffle、num_workers 等，来控制加载数据的方式。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision import transforms</span><br><span class="line">from torch.utils.data.dataloader import DataLoader</span><br><span class="line">trans = transforms.Compose([</span><br><span class="line">transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=True,transform=trans,download=True)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=False,transform=trans,download=True)</span><br><span class="line">print(type(test_set))</span><br><span class="line">loader = DataLoader(dataset=test_set,batch_size=64,shuffle=True,num_workers=0,drop_last=False)</span><br><span class="line"></span><br><span class="line">img,target = test_set[0]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">torch.Size([3, 32, 32])</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3</span></span><br><span class="line">print(img.shape)</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line">for data in loader:</span><br><span class="line">imgs,targets =data</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">torch.Size([64, 3, 32, 32])</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">torch.Size([64])</span></span><br><span class="line">print(imgs.shape)</span><br><span class="line">print(targets.shape)</span><br><span class="line">break</span><br></pre></td></tr></table></figure><h1 id="三、-搭建网络"><a href="#三、-搭建网络" class="headerlink" title="三、 搭建网络"></a>三、 搭建网络</h1>nn.Module介绍：</li><li>nn.Module 是一个抽象类，不能直接使用，需要继承 nn.Module 类，然后重写 forward 方法，这个方法就是网络结构。</li><li>forward 方法用于定义网络结构，返回一个张量。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn </span><br><span class="line">import torch</span><br><span class="line">class mymodel(nn.Module):</span><br><span class="line">def __init__(self) :</span><br><span class="line">super(mymodel,self).__init__()</span><br><span class="line"></span><br><span class="line"> def forward(self,x):</span><br><span class="line">     y = x+1</span><br><span class="line">     return y</span><br><span class="line"></span><br><span class="line">a = mymodel()</span><br><span class="line">x = torch.tensor([10,20,30])</span><br><span class="line">print(a(x))</span><br></pre></td></tr></table></figure>搭建网络：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">import torchvision</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.nn import Conv2d</span><br><span class="line">from torch.nn import MaxPool2d</span><br><span class="line">from torch.nn import ReLU</span><br><span class="line">from torch.nn import Flatten</span><br><span class="line">from torch.nn import Linear</span><br><span class="line">from torch.nn import Sigmoid</span><br><span class="line">from torch.nn import Sequential</span><br><span class="line">from torch.utils.data.dataloader import DataLoader</span><br><span class="line">from tensorboardX import SummaryWriter</span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),</span><br><span class="line">download=True)</span><br><span class="line">dataloader  = DataLoader(dataset,batch_size=64)</span><br><span class="line"></span><br><span class="line">class mod(nn.Module):</span><br><span class="line">def __init__(self):</span><br><span class="line">super(mod,self).__init__()</span><br><span class="line">self.conv1 = Conv2d(3,32,5,stride=1,padding=2)</span><br><span class="line">self.max_pool2d = MaxPool2d(2)</span><br><span class="line">self.conv2 = Conv2d(32,32,5,padding=2)</span><br><span class="line">self.max_pool2d2 = MaxPool2d(2)</span><br><span class="line">self.conv3 = Conv2d(32,64,5,padding=2)</span><br><span class="line">self.max_pool2d3 = MaxPool2d(2)</span><br><span class="line">self.flatten = Flatten()</span><br><span class="line">self.Linear = Linear(1024,64)</span><br><span class="line">self.Linear2 = Linear(64,10)</span><br><span class="line"></span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(3,32,5,stride=1,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32,32,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32,64,5,padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(1024,64),</span><br><span class="line">            Linear(64,10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self,x):</span><br><span class="line">        # x=self.conv1(x)</span><br><span class="line">        # x=self.max_pool2d(x)</span><br><span class="line">        # x=self.conv2(x)</span><br><span class="line">        # x=self.max_pool2d2(x)</span><br><span class="line">        # x=self.conv3(x)</span><br><span class="line">        # x=self.max_pool2d3(x)</span><br><span class="line">        # x=self.flatten(x)</span><br><span class="line">        # x=self.Linear(x)</span><br><span class="line">        # x=self.Linear2(x)</span><br><span class="line">        x = self.model1(x)</span><br><span class="line"></span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">modshi = mod()</span><br><span class="line">print(modshi)</span><br><span class="line"></span><br><span class="line">x = torch.ones((64,3,32,32))</span><br><span class="line">out = modshi(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for data in dataloader:</span><br><span class="line">imgs,targers = data</span><br><span class="line">out = modshi(imgs)</span><br><span class="line">print(out.shape)</span><br><span class="line">break</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可视化网络</span></span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line">writer.add_graph(modshi,x)</span><br><span class="line">writer.close()</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">mod(</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (max_pool2d2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (max_pool2d3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (flatten): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (Linear): Linear(in_features=1024, out_features=64, bias=True)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (Linear2): Linear(in_features=64, out_features=10, bias=True)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  (model1): Sequential(</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">    (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">    (6): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">    (7): Linear(in_features=1024, out_features=64, bias=True)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">    (8): Linear(in_features=64, out_features=10, bias=True)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">  )</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">torch.Size([64, 10])</span> </span><br></pre></td></tr></table></figure>损失函数：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.nn import L1Loss</span><br><span class="line">from torch.nn import MSELoss</span><br><span class="line">from torch.nn import CrossEntropyLoss</span><br><span class="line">x = torch.tensor([1,2,3],dtype=torch.float32)</span><br><span class="line">y = torch.tensor([1,2,5],dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">x = torch.reshape(x,(1,1,1,3))</span><br><span class="line">y = torch.reshape(y,(1,1,1,3))</span><br><span class="line"></span><br><span class="line">loss = L1Loss(reduce=&#x27;sum&#x27;)</span><br><span class="line">result = loss(x,y)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">loss = MSELoss()</span><br><span class="line">result = loss(x,y)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">x = torch.tensor([0.1,0.2,0.3])</span><br><span class="line">y = torch.tensor([1])</span><br><span class="line">x = torch.reshape(x,(1,3))</span><br><span class="line">loss = CrossEntropyLoss()</span><br><span class="line">result = loss(x,y)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>优化器：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">import torchvision</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.nn import Conv2d</span><br><span class="line">from torch.nn import MaxPool2d</span><br><span class="line">from torch.nn import ReLU</span><br><span class="line">from torch.nn import Flatten</span><br><span class="line">from torch.nn import Linear</span><br><span class="line">from torch.nn import Sigmoid</span><br><span class="line">from torch.nn import Sequential</span><br><span class="line">from torch.utils.data.dataloader import DataLoader</span><br><span class="line">from torch.nn import CrossEntropyLoss</span><br><span class="line">from torch.optim import SGD</span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),</span><br><span class="line">download=True)</span><br><span class="line">dataloader  = DataLoader(dataset,batch_size=64)</span><br><span class="line"></span><br><span class="line">class mod(nn.Module):</span><br><span class="line">def __init__(self):</span><br><span class="line">super(mod,self).__init__()</span><br><span class="line">self.model1 = Sequential(</span><br><span class="line">Conv2d(3,32,5,stride=1,padding=2),</span><br><span class="line">MaxPool2d(2),</span><br><span class="line">Conv2d(32,32,5,padding=2),</span><br><span class="line">MaxPool2d(2),</span><br><span class="line">Conv2d(32,64,5,padding=2),</span><br><span class="line">MaxPool2d(2),</span><br><span class="line">Flatten(),</span><br><span class="line">Linear(1024,64),</span><br><span class="line">Linear(64,10),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line"></span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">modshi = mod()</span><br><span class="line">loss = CrossEntropyLoss()</span><br><span class="line">optim = SGD(modshi.parameters(),lr=0.01)</span><br><span class="line"></span><br><span class="line">for epoch in range(20):</span><br><span class="line">runing_loss = 0.0</span><br><span class="line">for data in dataloader:</span><br><span class="line">imgs,targers = data</span><br><span class="line">out = modshi(imgs)</span><br><span class="line">loss1 = loss(out,targers)</span><br><span class="line">optim.zero_grad()</span><br><span class="line">loss1.backward()</span><br><span class="line">optim.step()</span><br><span class="line">runing_loss = runing_loss+loss1</span><br><span class="line">print(runing_loss)</span><br></pre></td></tr></table></figure>GPU的使用：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fff = model.cuda()</span><br><span class="line">loss_fn = loss.cuda()</span><br><span class="line">imgs = imgs.cuda()</span><br><span class="line">targets = targets.cuda()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda&quot;)</span><br><span class="line">model = model.to(device)</span><br><span class="line">imgs = imgs.to(device)</span><br></pre></td></tr></table></figure>现有模型使用：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=True)</span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=False)</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),</span><br><span class="line">download=True)</span><br><span class="line">dataloader  = DataLoader(dataset,batch_size=64)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在最后一层加先行层从1000-》10</span></span><br><span class="line">vgg16_true.add_module(&#x27;add&#x27;,nn.Linear(1000,10))</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改vgg最后一层</span></span><br><span class="line">vgg16_false.classifier[6] = nn.Linear(4096,10)</span><br></pre></td></tr></table></figure>保存读取网络：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line"></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=True)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保存1,模型结构和参数,需要模型定义的代码</span></span><br><span class="line">torch.save(vgg16,&#x27;vgg16.pth&#x27;)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">加载1</span></span><br><span class="line">model = torch.load(&#x27;vgg16.pth&#x27;)</span><br><span class="line">print(model)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保存2，保存参数字典形式</span></span><br><span class="line">torch.save(vgg16.state_dict(),&quot;vgg.pth&quot;)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">加载2</span></span><br><span class="line">model = torchvision.models.vgg16()</span><br><span class="line">model.load_state_dict(torch.load(&#x27;vgg.pth&#x27;))</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="四、完整的训练验证流程"><a href="#四、完整的训练验证流程" class="headerlink" title="四、完整的训练验证流程"></a>四、完整的训练验证流程</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">model.py</span></span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class fff(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(fff, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(3, 32, 5, stride=1, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32, 32, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32, 64, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(1024, 64),</span><br><span class="line">            Linear(64, 10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line"></span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    model = fff()</span><br><span class="line">    x = torch.ones(64,3,32,32)</span><br><span class="line">    y = model(x)</span><br><span class="line">    print(y.shape)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">main.py</span></span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import torchvision</span><br><span class="line">from model import fff</span><br><span class="line">from tensorboardX import SummaryWriter</span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=&#x27;dataset&#x27;, train=True, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=True)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=&#x27;dataset&#x27;, train=False, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=True)</span><br><span class="line"></span><br><span class="line">print(&quot;训练数据集长度为&#123;&#125;&quot;.format(len(train_data)))</span><br><span class="line">print(&quot;测试数据集长度为&#123;&#125;&quot;.format(len(test_data)))</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=64)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64)</span><br><span class="line"></span><br><span class="line">model = fff()</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">learn = 1e-2</span><br><span class="line">optim = torch.optim.SGD(model.parameters(),lr=learn)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记录训练次数</span></span><br><span class="line">total_train_step = 0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记录测试次数</span></span><br><span class="line">total_test_step = 0</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">训练轮数</span></span><br><span class="line">epoch = 10</span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line"></span><br><span class="line">for i in range(epoch):</span><br><span class="line">    model.train()</span><br><span class="line">    print(&#x27;-------------------------&#123;&#125;轮开始-------------------&#x27;.format(i))</span><br><span class="line">    for data in train_dataloader:</span><br><span class="line">        imgs,targets = data</span><br><span class="line">        y = model(imgs)</span><br><span class="line">        loss1 = loss(y, targets)</span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        loss1.backward()</span><br><span class="line">        optim.step()</span><br><span class="line">        total_train_step+=1</span><br><span class="line">        if total_train_step % 300 == 0:</span><br><span class="line">            print(&quot;第&#123;&#125;次,损失为&#123;&#125;&quot;.format(total_train_step, loss1.item()))</span><br><span class="line">            writer.add_scalar(&quot;train_loss&quot;,loss1.item(),total_train_step)</span><br><span class="line">    model.eval()</span><br><span class="line">    total_test_loss = 0</span><br><span class="line">    total_accuracy = 0</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for data in test_dataloader:</span><br><span class="line">            imgs ,targets = data</span><br><span class="line">            y = model(imgs)</span><br><span class="line">            loss1 = loss(y,targets)</span><br><span class="line">            total_test_loss +=loss1</span><br><span class="line">            accuracy = (y.argmax(1)==targets).sum()</span><br><span class="line">            total_accuracy+=accuracy</span><br><span class="line">    print(&quot;整体测试集上loss&#123;&#125;&quot;.format(total_test_loss))</span><br><span class="line">    print(&quot;整体测试集上accuracy&#123;&#125;&quot;.format(total_accuracy/len(test_data)))</span><br><span class="line">    writer.add_scalar(&quot;test_loss&quot;, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(&quot;test_accuracy&quot;, total_accuracy/len(test_data), total_test_step)</span><br><span class="line">    total_test_step += 1</span><br><span class="line"></span><br><span class="line">    torch.save(model,&quot;cif&#123;&#125;.pth&quot;.format(i))</span><br><span class="line">    print(&quot;save over&quot;)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">test.py</span></span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line">import torchvision</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">img_path = &quot;image/下载.jpeg&quot;</span><br><span class="line">img = Image.open(img_path)</span><br><span class="line">trans = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.Resize((32, 32)),</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line">img = trans(img)</span><br><span class="line">img = torch.reshape(img, (1, 3, 32, 32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class fff(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(fff, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(3, 32, 5, stride=1, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32, 32, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32, 64, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(64*4*4, 64),</span><br><span class="line">            Linear(64, 10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line"></span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">device = torch.device(&quot;mps&quot;)</span><br><span class="line">model = torch.load(&quot;./cif9.pth&quot;).to(device)</span><br><span class="line">model.eval()</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    img = img.to(device)</span><br><span class="line">    y = model(img)</span><br><span class="line">    y = y.argmax(1)</span><br><span class="line">    print(y)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本文主要是介绍有关Pytorch的基础知识（一）</summary>
    
    
    
    <category term="教程" scheme="https://www.aimiliy.top/categories/%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Pytorch" scheme="https://www.aimiliy.top/tags/Pytorch/"/>
    
    <category term="Machine Learning" scheme="https://www.aimiliy.top/tags/Machine-Learning/"/>
    
    <category term="Deep Learning" scheme="https://www.aimiliy.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>艾尔登法环DLC - 黄金树之影</title>
    <link href="https://www.aimiliy.top/posts/bc5092e1.html"/>
    <id>https://www.aimiliy.top/posts/bc5092e1.html</id>
    <published>2024-06-18T03:16:14.000Z</published>
    <updated>2024-09-11T10:46:55.197Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>《艾尔登法环》DLC「黄金树幽影」终于确定2024年6月21日发售，本文将介绍「黄金树幽影」背景、各平台和各版本的内容及售价信息、PC版配置需求容量大小等，一起来看下。</p></blockquote><h2 id="《艾尔登法环：黄金树幽影》简介："><a href="#《艾尔登法环：黄金树幽影》简介：" class="headerlink" title="《艾尔登法环：黄金树幽影》简介："></a>《艾尔登法环：黄金树幽影》简介：</h2><p>「黄金树幽影」是《艾尔登法环》的首个DLC，也是FS社迄今为止制作过的规模最大的扩展包。在「黄金树幽影」中玩家将会踏入＂幽影之地＂，展开既神秘又充斥危险的崭新冒险，并使用全新的武器及盔甲对抗极具威胁性的头目。追随米凯拉的脚步，揭开《艾尔登法环》故事中的黑暗秘密。</p><p>幽影之地──</p><p>受黄金树幽影藏匿的土地；神祇玛莉卡降生之处；</p><p>决不受赞扬的战争与屠杀，发生的地点；以及受梅瑟莫之火洗礼的地方。</p><p>因此，米凯拉前往幽影之地，他舍弃了与黄金相连的身体、力量，还有命运──</p><p>舍弃了一切。</p><p>而米凯拉在等待， 等着与他约定的王归来。</p><h2 id="PC平台（Steam）国区售价："><a href="#PC平台（Steam）国区售价：" class="headerlink" title="PC平台（Steam）国区售价："></a>PC平台（Steam）国区售价：</h2><p><strong>「黄金树幽影」单DLC售价198元</strong><br><strong>「黄金树幽影」数字典藏包售价248元，含黄金树幽影+数字美术书＆数字原声带</strong></p><p><img src="https://qiniu.aimiliy.top/assets/Linghua11718682729368234.png" alt="img.png"></p><h2 id="《艾尔登法环：黄金树幽影》PC版配置需求："><a href="#《艾尔登法环：黄金树幽影》PC版配置需求：" class="headerlink" title="《艾尔登法环：黄金树幽影》PC版配置需求："></a>《艾尔登法环：黄金树幽影》PC版配置需求：</h2><p><img src="https://qiniu.aimiliy.top/assets/Linghua11718682778804392.png" alt="img.png"></p><h2 id="主机平台（含PS4-5、XSX-S、XboxOne）港区售价："><a href="#主机平台（含PS4-5、XSX-S、XboxOne）港区售价：" class="headerlink" title="主机平台（含PS4|5、XSX|S、XboxOne）港区售价："></a>主机平台（含PS4|5、XSX|S、XboxOne）港区售价：</h2><p><strong>「黄金树幽影」单DLC售价299港元</strong><br><strong>「黄金树幽影」数字典藏包售价399港元，含黄金树幽影＋数字美术书＆数字原声带</strong><br><strong>《艾尔登法环：黄金树幽影版》售价599港元，含艾尔登法环+黄金树幽影</strong><br><strong>《艾尔登法环：黄金树幽影》数字豪华版售价699港元，含艾尔登法环+黄金树幽影+数字美术书＆数字原声带</strong></p><p><img src="https://qiniu.aimiliy.top/assets/Linghua11718682852562493.png" alt="img.png"><br><img src="https://qiniu.aimiliy.top/assets/Linghua11718682878165579.png" alt="img_1.png"></p><h2 id="《艾尔登法环：黄金树幽影》实体限定版（数量有限）："><a href="#《艾尔登法环：黄金树幽影》实体限定版（数量有限）：" class="headerlink" title="《艾尔登法环：黄金树幽影》实体限定版（数量有限）："></a>《艾尔登法环：黄金树幽影》实体限定版（数量有限）：</h2><p><strong>国外地区已开启预购，售价249.99美元/欧元</strong><br><strong>“穿刺者”梅瑟莫角色模型（46公分）</strong><br><strong>数字原声带</strong><br><strong>40页精装美术书</strong><br><strong>「黄金树幽影」下载码（不含艾尔登法环本体）</strong></p><p><img src="https://qiniu.aimiliy.top/assets/Linghua11718682959144957.png" alt="img.png"></p><p>以上所有平台预购均有预购特典——肢体动作“米凯拉法环”，推进游戏进度，也能获得此肢体动作。</p><p><img src="https://qiniu.aimiliy.top/assets/Linghua11718682995133698.png" alt="img_1.png"></p>]]></content>
    
    
    <summary type="html">DLC - 黄金树之影</summary>
    
    
    
    <category term="游戏" scheme="https://www.aimiliy.top/categories/%E6%B8%B8%E6%88%8F/"/>
    
    
    <category term="艾尔登法环" scheme="https://www.aimiliy.top/tags/%E8%89%BE%E5%B0%94%E7%99%BB%E6%B3%95%E7%8E%AF/"/>
    
    <category term="魂类" scheme="https://www.aimiliy.top/tags/%E9%AD%82%E7%B1%BB/"/>
    
    <category term="DLC" scheme="https://www.aimiliy.top/tags/DLC/"/>
    
  </entry>
  
  <entry>
    <title>Linux下MySQL安装教程</title>
    <link href="https://www.aimiliy.top/posts/46df6e3a.html"/>
    <id>https://www.aimiliy.top/posts/46df6e3a.html</id>
    <published>2023-06-12T01:24:00.000Z</published>
    <updated>2023-06-12T01:24:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、MySQL介绍"><a href="#一、MySQL介绍" class="headerlink" title="一、MySQL介绍"></a>一、MySQL介绍</h1><h2 id="1-1-MySQL简介"><a href="#1-1-MySQL简介" class="headerlink" title="1.1 MySQL简介"></a>1.1 MySQL简介</h2><blockquote><p>MySQL 8.0 是最新版本的 MySQL 数据库管理系统，是一种关系型数据库管理系统，由 Oracle 公司开发和维护。MySQL 8.0 带来了一系列新特性，包括多个性能提升，更好的安全性和扩展性，以及新的管理功能。</p><h2 id="1-2-MySQL特点"><a href="#1-2-MySQL特点" class="headerlink" title="1.2 MySQL特点"></a>1.2 MySQL特点</h2><p><strong>开源</strong>：MySQL 是一个开源的关系型数据库管理系统，遵循 GPL 许可协议。<br><strong>快速</strong>：MySQL 是一个快速、可扩展的数据库管理系统，具有高性能和高吞吐量。<br><strong>安全</strong>：MySQL 提供了 strong authentication 和 strong authorization，确保数据的安全性。<br><strong>兼容性</strong>：MySQL 支持多种数据库语言，包括 SQL、PL/SQL、T-SQL、MySQL、Oracle、DB2 等。<br><strong>社区</strong>：MySQL 社区提供了大量资源，包括文档、教程、论坛和开源代码库。</p><h1 id="二、本次实践介绍"><a href="#二、本次实践介绍" class="headerlink" title="二、本次实践介绍"></a>二、本次实践介绍</h1><h2 id="2-1-环境准备"><a href="#2-1-环境准备" class="headerlink" title="2.1 环境准备"></a>2.1 环境准备</h2><div class="table-container"><table><thead><tr><th>hostname</th><th>IP地址</th><th>系统版本</th><th>内核版本</th><th>mysql版本</th></tr></thead><tbody><tr><td>localhost</td><td>192.168.1.100</td><td>CentOS 7.9.2009</td><td>3.10.0-1160.el7.x86_64</td><td>8.0.28</td></tr></tbody></table></div><h2 id="2-2-实践目的"><a href="#2-2-实践目的" class="headerlink" title="2.2 实践目的"></a>2.2 实践目的</h2><ol><li>在centos7.6系统下安装MySQL8.0版本。</li><li>可以远程登录mysql数据库。<h1 id="三、卸载MySQL数据库"><a href="#三、卸载MySQL数据库" class="headerlink" title="三、卸载MySQL数据库"></a>三、卸载MySQL数据库</h1><h2 id="3-1-卸载MySQL"><a href="#3-1-卸载MySQL" class="headerlink" title="3.1 卸载MySQL"></a>3.1 卸载MySQL</h2>如果系统已安装有其他版本的mysql，需提前卸载清空环境。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -e mysql　　// 普通删除模式</span><br><span class="line">rpm -e --nodeps mysql　　// 强力删除模式</span><br></pre></td></tr></table></figure></li></ol></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /etc/my.cnf      // 替换自己实际目录</span><br><span class="line">rm -rf /var/lib/mysql/  // 替换自己实际目录</span><br></pre></td></tr></table></figure><h1 id="四、安装MySQL数据库"><a href="#四、安装MySQL数据库" class="headerlink" title="四、安装MySQL数据库"></a>四、安装MySQL数据库</h1><h2 id="4-1-下载安装配置"><a href="#4-1-下载安装配置" class="headerlink" title="4.1 下载安装配置"></a>4.1 下载安装配置</h2><ol><li>切换到 /usr/local/<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br></pre></td></tr></table></figure></li><li>创建mysql文件夹<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir mysql</span><br></pre></td></tr></table></figure></li><li>切换到mysql文件夹下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd mysql</span><br></pre></td></tr></table></figure></li><li>下载mysql8.0安装包<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.28-linux-glibc2.12-x86_64.tar.xz</span><br></pre></td></tr></table></figure></li><li>解压mysql安装包<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar xvJf mysql-8.0.28-linux-glibc2.12-x86_64.tar.xz </span><br></pre></td></tr></table></figure></li><li>重命名解压出来的文件夹，这里改成mysql-8.0<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv mysql-8.0.28-linux-glibc2.12-x86_64 mysql-8.0</span><br></pre></td></tr></table></figure></li><li>进入MySQL根目录，创建data文件夹<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd mysql-8.0</span><br><span class="line">mkdir data</span><br></pre></td></tr></table></figure><h1 id="五、MySQL初始化"><a href="#五、MySQL初始化" class="headerlink" title="五、MySQL初始化"></a>五、MySQL初始化</h1><h2 id="5-1-初始化"><a href="#5-1-初始化" class="headerlink" title="5.1 初始化"></a>5.1 初始化</h2><blockquote><p>在初始化之前提一嘴，Linux中的MySQL默认是区分表明大小写的，如果你是安装的也是MySQL8，那么不区分大小需要在数据库初始化时设置，不然初始化后，在配置文件在设置重启时是会报错的。 进入MySQL的bin目录进行初始化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./mysqld --user=mysql --basedir=/opt/mysql/mysql-8.0 --datadir=/opt/mysql/mysql-8.0/data --lower-case-table-names=1 --initialize</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang.voooe.cn/images/2024/06/12/img.png" alt="img.png"><br>–lower-case-table-names=1 即设置不区分表明大小写，最后面的 dwW3&lt;/EIewj 为初始化生产的临时密码。</p><h2 id="5-2-初始化失败"><a href="#5-2-初始化失败" class="headerlink" title="5.2 初始化失败"></a>5.2 初始化失败</h2><p>进入bin目录报如下错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang.voooe.cn/images/2024/06/12/img_1.png" alt="img_1.png"><br>原因：<br>这是一个与运行MySQL数据库相关的问题。出现这个错误的原因是系统缺少了名为libaio.so.1的共享库文件。<br>先安装numactl软件包，命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install numactl.x86_64</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang.voooe.cn/images/2024/06/12/image.png" alt="image.png"><br>然后安装libaio开发包，命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install  libaio-devel.x86_64</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang.voooe.cn/images/2024/06/12/image75e000e8b8f3a061.png" alt="image75e000e8b8f3a061.png"><br>安装成功后就可以再次初始化了。</p><h1 id="六、修改MySQL配置文件"><a href="#六、修改MySQL配置文件" class="headerlink" title="六、修改MySQL配置文件"></a>六、修改MySQL配置文件</h1><p>初次进入时配置是空的，进入命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/my.cnf</span><br></pre></td></tr></table></figure><p>复制下面文件保存退出。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">port=3306</span><br><span class="line">basedir=/usr/MYSQL/mysql-8.0.31/  # MySQL根目录</span><br><span class="line">datadir=/usr/MYSQL/mysql-8.0.31/data/  # MySQL的data目录</span><br><span class="line">socket=/tmp/mysql.sock</span><br><span class="line">character-set-server=UTF8MB4</span><br><span class="line">symbolic-links=0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">解决“java.sql.SQLException: Expression <span class="comment">#1 of ORDER BY clause is not in SELECT list,references column”</span></span></span><br><span class="line">sql_mode=&#x27;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION&#x27;</span><br></pre></td></tr></table></figure><h1 id="七、添加服务到系统"><a href="#七、添加服务到系统" class="headerlink" title="七、添加服务到系统"></a>七、添加服务到系统</h1><p>进入MySQL根目录，直接复制粘贴命令即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -a ./support-files/mysql.server /etc/init.d/mysql</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang.voooe.cn/images/2024/06/12/image3400e2bfb280f11e.png" alt="img.png"></p><h1 id="八、授权以及添加服务"><a href="#八、授权以及添加服务" class="headerlink" title="八、授权以及添加服务"></a>八、授权以及添加服务</h1><p>这里授权和添加服务都是在MySQL的根目录中操作。</p><h2 id="8-1-授权"><a href="#8-1-授权" class="headerlink" title="8.1 授权"></a>8.1 授权</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x /etc/init.d/mysql</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang.voooe.cn/images/2024/06/12/imagec257c3fe2dd04358.png" alt="img_1.png"></p><h1 id="九、启动MySQL"><a href="#九、启动MySQL" class="headerlink" title="九、启动MySQL"></a>九、启动MySQL</h1><h2 id="9-1-启动命令"><a href="#9-1-启动命令" class="headerlink" title="9.1 启动命令"></a>9.1 启动命令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start</span><br></pre></td></tr></table></figure><h2 id="9-2-查看MySQL状态"><a href="#9-2-查看MySQL状态" class="headerlink" title="9.2 查看MySQL状态"></a>9.2 查看MySQL状态</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql status</span><br></pre></td></tr></table></figure><h2 id="9-3-踩坑"><a href="#9-3-踩坑" class="headerlink" title="9.3 踩坑"></a>9.3 踩坑</h2><p>如果启动时报如下错<br><img src="https://tuchuang.voooe.cn/images/2024/06/12/imagedb44548a38c5ba57.png" alt="img_2.png"><br>一个原因是上面新建的data目录没有权限，使用如下命令加上：  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod -R 755 /usr/local/mysql/mysql-8.0.32/data</span><br></pre></td></tr></table></figure><p>将data目录改为你的data目录，然后再使用<br><img src="https://tuchuang.voooe.cn/images/2024/06/12/image306f4da7ad1d2979.png" alt="img_3.png"><br>然后再启动mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start</span><br></pre></td></tr></table></figure><h1 id="十、将MySQL命令添加到服务"><a href="#十、将MySQL命令添加到服务" class="headerlink" title="十、将MySQL命令添加到服务"></a>十、将MySQL命令添加到服务</h1><h2 id="10-1-添加命令"><a href="#10-1-添加命令" class="headerlink" title="10.1 添加命令"></a>10.1 添加命令</h2><p>命令： ln -s “你的MySQL根目录/bin/mysql” /usr/bin ，这一步是为了能在任何地方通过用户名和密码登录MySQL，这是我的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/MYSQL/mysql-8.0.31/bin/mysql /usr/bin</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang.voooe.cn/images/2024/06/12/image813678273626d0b1.png" alt="img_4.png"></p><h2 id="10-2-踩坑"><a href="#10-2-踩坑" class="headerlink" title="10.2 踩坑"></a>10.2 踩坑</h2><p>直到上一步都没有出错，在登录mysql时报如下错：<br><img src="https://tuchuang.voooe.cn/images/2024/06/12/imageb6877e37e47580bf.png" alt="img_5.png"><br>可能是挂载的路径错了，到挂载目录查看，命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/bin</span><br></pre></td></tr></table></figure><p>可以看到mysql服务是红色的，并且一直在闪，正常的应该是绿色的。<br><img src="https://tuchuang.voooe.cn/images/2024/06/12/image6f45afc64e99c3c8.png" alt="img_6.png"><br>我当前的mysql根目录是8.0.32，添加的时候写成了8.0.31，怪不得会报错。<br>解决办法：删除当前mysql的服务，重新添加。<br>先删除，命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /usr/bin/mysql</span><br></pre></td></tr></table></figure><p>重新添加：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/local/mysql/mysql-8.0.32/bin/mysql /usr/bin</span><br></pre></td></tr></table></figure><p>可以看到mysql服务添加成功，可以登录mysql了。<br><img src="https://tuchuang.voooe.cn/images/2024/06/12/image317af20da433bf50.png" alt="img_7.png"></p><h1 id="十一、修改密码"><a href="#十一、修改密码" class="headerlink" title="十一、修改密码"></a>十一、修改密码</h1><h2 id="11-1-先使用前面初始化拿到的临时密码登录"><a href="#11-1-先使用前面初始化拿到的临时密码登录" class="headerlink" title="11.1 先使用前面初始化拿到的临时密码登录"></a>11.1 先使用前面初始化拿到的临时密码登录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang.voooe.cn/images/2024/06/12/image2a46602335e036f2.png" alt="img_8.png"></p><h2 id="11-2-切换到mysql数据库"><a href="#11-2-切换到mysql数据库" class="headerlink" title="11.2 切换到mysql数据库"></a>11.2 切换到mysql数据库</h2><p>切换mysql命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">use mysql</span><br></pre></td></tr></table></figure><p>如果切换mysql数据库时出现这样的错就直接修改密码：<br><img src="https://tuchuang.voooe.cn/images/2024/06/12/image3f7e34c16a61536c.png" alt="img_9.png"><br>查询用户表命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select Host, User from user;</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang.voooe.cn/images/2024/06/12/image0a05976c46494fc4.png" alt="img_10.png"><br>可以看到允许访问root的host有哪些，%表示所有，这里只有localhost，修改密码不成功可以看一下root对应的host的值。</p><h2 id="11-3-修改密码"><a href="#11-3-修改密码" class="headerlink" title="11.3 修改密码"></a>11.3 修改密码</h2><p>修改密码的命令为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;;</span><br></pre></td></tr></table></figure><p>修改之后需要刷新，使密码生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h2 id="11-4-踩坑，修改成功可以不看"><a href="#11-4-踩坑，修改成功可以不看" class="headerlink" title="11.4 踩坑，修改成功可以不看"></a>11.4 踩坑，修改成功可以不看</h2><h3 id="11-4-1-设置简单密码报错"><a href="#11-4-1-设置简单密码报错" class="headerlink" title="11.4.1 设置简单密码报错"></a>11.4.1 设置简单密码报错</h3><p>初次修改密码时，如果设置过于简单的密码可能会报错，可以使用命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show variables like &#x27;validate_password%&#x27;;</span><br></pre></td></tr></table></figure><p>validate_password_policy 的默认值为 1 或者 MEDIUM，表示密码必须符合长度，且必须含有数字，小写或大写字母，特殊字符。 validate_password_length 的默认值为 8，表示密码长度为8。</p></blockquote></li></ol><p>注意查看密码安全策略的这步，只有初始密码登录MySQL才能查到结果，修改密码再次使用此命令查询没有结果。<br>设置简单密码策略<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set global validate_password_policy=0;</span><br></pre></td></tr></table></figure><br>设置密码长度<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set global validate_password_length=6；</span><br></pre></td></tr></table></figure><br>这样设置之后就可以将密码修改为“123456”了。</p><h3 id="11-4-2-修改密码报错"><a href="#11-4-2-修改密码报错" class="headerlink" title="11.4.2 修改密码报错"></a>11.4.2 修改密码报错</h3><p>有些朋友修改密码时会报错，原因可能就在这里，假如你修改密码的命令为：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;;</span><br></pre></td></tr></table></figure><br>那么是没有问题的，如果你修改密码的命令为:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;;</span><br></pre></td></tr></table></figure><br>那么可能就会报错。 解决办法，使用如下命令：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update user set Host=&#x27;%&#x27; where User=&#x27;root&#x27;;</span><br></pre></td></tr></table></figure><br>将root用户对应的Host改为“%”再重新修改密码就行了。 记得修改密码后用命令:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flush privileges;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
    <summary type="html">Linux下安装MySQL的详细步骤和操作</summary>
    
    
    
    <category term="技术" scheme="https://www.aimiliy.top/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="MySQL" scheme="https://www.aimiliy.top/tags/MySQL/"/>
    
    <category term="Linux" scheme="https://www.aimiliy.top/tags/Linux/"/>
    
  </entry>
  
</feed>
